{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Location Classification in Hurricane Harvey\n",
    "The goal of this analysis is to evaluate methods by which users Tweeting about Hurricane Harvey may be classified as in the area or otherwise.\n",
    "\n",
    "Data was collected ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding Dimension: 'Local'\n",
      "Subject: user\n",
      "Classes: ['Unsure', 'Non-Witness', 'Witness']\n",
      "\n",
      "1500 Accounts coded by primary coder, 151 by secondary coder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get account-based codings:\n",
    "account_codings = Coding.objects.filter(coding_id=1).filter(user__isnull=False).filter(data_code__data_code_id__gt=0)\n",
    "account_codings_secondary = Coding.objects.filter(coding_id=2).filter(user__isnull=False)\n",
    "\n",
    "# Check available coding schema:\n",
    "dimensions = DataCodeDimension.objects.all()[1:]\n",
    "for d in dimensions:\n",
    "    print('Coding Dimension: \\'{}\\'\\nSubject: {}\\nClasses: {}\\n'.format(d.name, d.coding_subject, list(d.datacode.values_list('name', flat=True))))\n",
    "# Note these totals combine all user or Tweet codes, so can be misleading if more than one dimension is used.\n",
    "print(\"{} Accounts coded by primary coder, {} by secondary coder.\".format(account_codings.count(), account_codings_secondary.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "added_at                             1500\n",
       "betweenness_centrality                878\n",
       "closeness_centrality                  878\n",
       "created_at                           1500\n",
       "data_source                          1500\n",
       "default_profile                      1500\n",
       "default_profile_image                1500\n",
       "degree_centrality                     878\n",
       "description                          1500\n",
       "eigenvector_centrality                878\n",
       "favourites_count                     1500\n",
       "followers_count                      1500\n",
       "friends_count                        1500\n",
       "geo_enabled                          1500\n",
       "has_extended_profile                 1500\n",
       "id                                   1500\n",
       "in_degree                            1500\n",
       "is_deleted                              0\n",
       "is_deleted_observed                     0\n",
       "is_translation_enabled               1500\n",
       "katz_centrality                         0\n",
       "lang                                 1500\n",
       "listed_count                         1500\n",
       "load_centrality                       878\n",
       "location                             1500\n",
       "name                                 1500\n",
       "needs_phone_verification                0\n",
       "old_screen_name                        96\n",
       "out_degree                           1500\n",
       "protected                            1500\n",
       "ratio_detected                       1479\n",
       "ratio_media                          1479\n",
       "ratio_original                       1479\n",
       "screen_name                          1500\n",
       "statuses_count                       1500\n",
       "suspended                               0\n",
       "time_zone                             921\n",
       "translator_type                      1500\n",
       "tweets_per_hour                      1479\n",
       "undirected_eigenvector_centrality     878\n",
       "url                                   814\n",
       "user_class                           1500\n",
       "user_followers                          0\n",
       "user_followers_update                   0\n",
       "user_following                          0\n",
       "user_following_update                   0\n",
       "user_id                              1500\n",
       "user_network_update_observed_at         0\n",
       "utc_offset                            921\n",
       "verified                             1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all Users coded by primary coder:\n",
    "# (exclude data_code_id=0 as this is the temporary 'to be coded' class)\n",
    "users = User.objects.filter(coding_for_user__coding_id=1, \n",
    "                            coding_for_user__data_code__data_code_id__gt=0)\n",
    "users_df = pd.DataFrame(list(users.values()))\n",
    "users_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all authors from coded Tweets:\n",
    "#coded_tweets = Tweet.objects.filter(coding_for_tweet__coding_id=1, \n",
    "#                            coding_for_tweet__data_code__data_code_id__gt=0)\n",
    "#authors = User.objects.filter(tweet__in=coded_tweets).distinct()\n",
    "#author_df = pd.DataFrame(list(authors.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Location Data\n",
    "There are a number of options which can represent the ground truth location of the user.\n",
    "* Location listed on a user's profile\n",
    "* User Timezone (deprecated)\n",
    "* Manual Coding\n",
    "* Location data derived from Tweet stream\n",
    "    * GPS tagged Tweets\n",
    "    * Mention of location in Tweet body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing user-set location in profile field\n",
    "The location the user sets as a string is evaluated and a locality decision made. In this instance, a location is considered 'local' if its coordinates (supplied by the Google geolocation API or parsed directly from the location string) fall within the bounding box used for geographic Twitter data collection, or if it contains the string 'houston' or 'christi' (representing the town Corpus Christi). Both of these locations fall within the bounding box, and are used here as a time-saving operation.\n",
    "\n",
    "Note that as this field can be set manually, it is unverifiable and therefore not a perfect representation of location, even where it exists. Users may neglect to update their location after moving, and some observations were made of users setting their location to that of a disaster event as a 'show of solidarity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block supports manual coding of locations as local or non-local.\n",
    "## It has been superceded by the next block which uses the Googlemaps API\n",
    "\n",
    "#location_list = users_df.location.unique()\n",
    "#with open('data/harvey_user_location/all_profile_locations.txt', 'w') as f:\n",
    "#    for item in location_list:\n",
    "#        f.write(\"%s\\n\" % item)\n",
    "\n",
    "################\n",
    "## This list then manually sorted and non-local locations removed.\n",
    "## List of local locations then re-imported.\n",
    "## Note this list excludes any locations containing 'Christi' or 'Houston'\n",
    "## Note: if more users are coded, this list needs to be re-examined. Raise alert:\n",
    "#if users_df.shape[0] != 931:\n",
    "#    print('ALERT: New codings detected. Consider updating manual locality selection')\n",
    "################\n",
    "\n",
    "#with open('data/harvey_user_location/local_profile_locations_manual_check.txt', 'r') as f:\n",
    "#    local_locations_list = f.read().splitlines()\n",
    "    \n",
    "## Create column for users with local location listed in profile\n",
    "#users_df['local_profile_location_manual'] = \\\n",
    "#    (users_df.location.str.contains('houston|christi', case=False, regex=True) |\n",
    "#    users_df.location.isin(local_locations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_coordinates(string):\n",
    "    '''Parse a string for coordinates'''\n",
    "    reg = '[nsewNSEW]?\\s?-?\\d+[\\.°]\\s?\\d+°?\\s?[nsewNSEW]?'\n",
    "    result = re.findall(reg, string)\n",
    "    if len(result) == 2: # Coordinates detected\n",
    "        for i in range(len(result)):\n",
    "            # Replace middle degree symbol with decimal:\n",
    "            reg_middle_degree = '(\\d+)°\\s?(\\d+)'\n",
    "            result[i] = re.sub(reg_middle_degree, r'\\1.\\2', result[i])\n",
    "            # Remove trailing degree symbol, N and E marks:\n",
    "            reg_strip = '[°neNE\\s]'\n",
    "            result[i] = re.sub(reg_strip, '', result[i])\n",
    "            # Replace south/west with negative sign:\n",
    "            reg_replace_sw = '[swSW](\\d+\\.\\d+)|(\\d+\\.\\d+)[swSW]'\n",
    "            result[i] = re.sub(reg_replace_sw, r'-\\1\\2', result[i])\n",
    "            # Remove double negative (where string contained eg. '-99.10w')\n",
    "            result[i] = re.sub('--', '-', result[i])\n",
    "        return (float(result[0]), float(result[1]))\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import googlemaps\n",
    "\n",
    "\n",
    "def is_in_bounding_box(coords, boxes):\n",
    "    '''\n",
    "    Check whether coordinates fall within defined bounding box:\n",
    "    Boxes are defined as their NW and SE points.\n",
    "    '''\n",
    "    for box in boxes:\n",
    "        if coords[0] < box[0][0] and coords[0] > box[1][0]:\n",
    "            if coords[1] > box[0][1] and coords[1] < box[1][1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_local(location, boxes, known_localities=[]):\n",
    "    '''\n",
    "    Check whether a location string falls within a set of \n",
    "    bounding boxes using Googlemaps API.\n",
    "    '''\n",
    "    if not location:\n",
    "        return False\n",
    "    # Check known localities first to save on API requests:\n",
    "    for x in known_localities:\n",
    "        if x in location:\n",
    "            return True\n",
    "    # Try and parse coordinates from string rather than API query:\n",
    "    coords = parse_coordinates(location)\n",
    "    # Get coords from API:\n",
    "    if not coords:\n",
    "        # Get API key from file:\n",
    "        with open(\"auth.yml\", 'r') as ymlfile:\n",
    "            auth = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "        gmaps = googlemaps.Client(key=auth['apikeys']['googlemaps'])\n",
    "        #########################################################\n",
    "        ####### OVERRIDE API OBJECT TO PREVENT API CALLS: #######\n",
    "        geocode_result = gmaps.geocode(location)\n",
    "        #geocode_result = False\n",
    "        #print('WARNING -- API DISABLED')\n",
    "        #########################################################\n",
    "        #########################################################\n",
    "        if geocode_result:\n",
    "            lat = geocode_result[0]['geometry']['location']['lat']\n",
    "            lon = geocode_result[0]['geometry']['location']['lng']\n",
    "            coords = (lat, lon)\n",
    "    if coords:\n",
    "        return(is_in_bounding_box(coords, boxes))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding boxes used for Hurricane Harvey dataset:\n",
    "boxes = [[(29.1197,-99.9590682),(26.5486063,-97.5021)],\n",
    "        [(30.3893434,-97.5021),(26.5486063,-93.9790001)]]\n",
    "# Don't need to look these up (save on API requests)\n",
    "known_localities = ['houston', 'christi']\n",
    "\n",
    "# Get list of locations in profiles:\n",
    "location_list = users_df.location.dropna().str.lower().unique()\n",
    "\n",
    "# Create sublist of local locations\n",
    "local_location_list = [loc for loc in location_list if is_local(loc, boxes, known_localities)]\n",
    "# Create sublist of non-local locations (for manual verification)\n",
    "non_local_location_list = [loc for loc in location_list if loc not in local_location_list]\n",
    "\n",
    "# Create column for users with local location listed in profile\n",
    "users_df['local_profile_location'] = users_df.location.str.lower().isin(local_location_list)\n",
    "\n",
    "# Write lists to file to save calling API on kernel restart:\n",
    "with open('data/harvey_user_location/local_locations_list_from_api.txt', 'w') as f:\n",
    "    for item in local_location_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('data/harvey_user_location/non_local_locations_list_from_api.txt', 'w') as f:\n",
    "    for item in non_local_location_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timezone Field\n",
    "Timezone data provided by Twitter when capturing the user objects is less specific than other methods, but may be useful as a supplementary source.\n",
    "As this data field has been deprecated by Twitter, it will not be available in new data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for profiles in relevant time zone:\n",
    "timezone = 'Central Time (US & Canada)'\n",
    "\n",
    "users_df['local_timezone'] = users_df.time_zone == timezone\n",
    "users_df = users_df.drop(['time_zone', 'utc_offset'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Coding\n",
    "Accounts were manually coded as 'local' or 'non-local'.\n",
    "\n",
    "Coders were shown the user account details as well as the Twitter stream of the user. The coders were instructed to determine whether the user account was in an area affected by the hurricane at any point during the data collection period. Therefore, the term 'local' may be misleading to the reader, as the definition given to the coders will include anyone visiting the area as, for example, a responder or aid worker. This larger set of 'on the ground' users is a more useful target for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent manual coding:\n",
    "users_df['coded_as'] = \\\n",
    "    users_df['screen_name'].apply(lambda x: User.objects.get(screen_name=x).coding_for_user.get(coding_id=1).data_code.name)\n",
    "users_df['coded_as_witness'] = users_df['coded_as'] == 'Witness'\n",
    "# Remove original column:\n",
    "users_df = users_df.drop(['coded_as'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPS from Tweet stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether any of a user's Tweets fall within the bounding box and update column:\n",
    "users_df['tweet_from_locality'] = False\n",
    "users = users_df.screen_name.tolist()\n",
    "for u in users:\n",
    "    try:\n",
    "        geo_tweets = User.objects.get(screen_name=u).tweet.filter(coordinates_lat__isnull=False)\n",
    "    except:\n",
    "        print('Error with user: ', u)\n",
    "        continue\n",
    "    for tweet in geo_tweets:\n",
    "        coords = (tweet.coordinates_lat, tweet.coordinates_lon)\n",
    "        if is_in_bounding_box(coords, boxes):\n",
    "            users_df.loc[users_df['screen_name'] == u, 'tweet_from_locality'] = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combination Columns\n",
    "Combining data from columns may improve accuracy (at the cost of recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['three_local_metrics'] = users_df[['tweet_from_locality', \n",
    "                    'local_timezone', 'local_profile_location']].all(axis='columns')\n",
    "users_df['local_tw_and_local_profile'] = users_df[['tweet_from_locality', \n",
    "                    'local_profile_location']].all(axis='columns')\n",
    "users_df['local_tw_and_local_tz'] = users_df[['tweet_from_locality', \n",
    "                    'local_timezone']].all(axis='columns')\n",
    "users_df['local_tz_and_local_profile'] = users_df[['local_profile_location', \n",
    "                    'local_timezone']].all(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "## Write/read dataframe to temp file ###\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "path = 'data/harvey_user_location/temp_users_df.csv'\n",
    "\n",
    "#users_df.to_csv(path)\n",
    "\n",
    "#users_df = pd.read_csv(path, index_col=0) \n",
    "#users_df = users_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "# Reading currently splits a row, delete rogue row:\n",
    "#if users_df.shape[0] == 1500:\n",
    "#    print('Dropping row 222, to verify')\n",
    "#    users_df = users_df.drop(users_df.index[222])\n",
    "#print(users_df.shape)\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 56)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Stand-In Metrics\n",
    "Now we can compare the metrics against the hand-coded classifications to decide whether they are suitable as stand-in values. Note that while the combination columns do increase precision (as expected), they drastically impact recall. \n",
    "\n",
    "This suggests that the columns have little correlation.\n",
    "\n",
    "local_tw_and_local_profile has the lowest drop in pos_cases (i.e. the highest correlation) but we do not see an increase in precision, and do suffer a drop in recall. Therefore these combination columns are not considered useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive cases:  378\n",
      "Total negative cases:  1122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>pos_precision</th>\n",
       "      <th>pos_recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet_from_locality</td>\n",
       "      <td>462</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local_timezone</td>\n",
       "      <td>347</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>local_profile_location</td>\n",
       "      <td>394</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>three_local_metrics</td>\n",
       "      <td>105</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>local_tw_and_local_profile</td>\n",
       "      <td>277</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>local_tw_and_local_tz</td>\n",
       "      <td>150</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>local_tz_and_local_profile</td>\n",
       "      <td>154</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  column_name pos_count  pos_precision  pos_recall  accuracy  \\\n",
       "0         tweet_from_locality       462           0.53        0.65      0.77   \n",
       "1              local_timezone       347           0.40        0.37      0.70   \n",
       "2      local_profile_location       394           0.64        0.67      0.82   \n",
       "3         three_local_metrics       105           0.69        0.19      0.77   \n",
       "4  local_tw_and_local_profile       277           0.64        0.47      0.80   \n",
       "5       local_tw_and_local_tz       150           0.63        0.25      0.77   \n",
       "6  local_tz_and_local_profile       154           0.66        0.27      0.78   \n",
       "\n",
       "   weighted_precision  weighted_recall  \n",
       "0                0.79             0.77  \n",
       "1                0.69             0.70  \n",
       "2                0.82             0.82  \n",
       "3                0.76             0.77  \n",
       "4                0.79             0.80  \n",
       "5                0.75             0.77  \n",
       "6                0.76             0.78  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Columns to compare:\n",
    "columns = ['tweet_from_locality', 'local_timezone', 'local_profile_location', \n",
    "            #'local_profile_location_manual', \n",
    "           'three_local_metrics', 'local_tw_and_local_profile', 'local_tw_and_local_tz', \n",
    "           'local_tz_and_local_profile']\n",
    "\n",
    "# Create reporting dataframe:\n",
    "results = pd.DataFrame(columns=['column_name', 'pos_count', 'pos_precision', 'pos_recall', \n",
    "                                'accuracy', 'weighted_precision', 'weighted_recall'])\n",
    "\n",
    "# Fill NA values\n",
    "users_df.coded_as_witness.fillna(False, inplace=True)\n",
    "\n",
    "for col in columns:\n",
    "    users_df[col].fillna(False, inplace=True)\n",
    "    report = classification_report(users_df['coded_as_witness'], users_df[col], \n",
    "                                    output_dict = True)\n",
    "    row = pd.Series({'column_name': col, 'pos_count': users_df[col].sum(), \n",
    "                    'pos_precision': round(report['True']['precision'], 2),\n",
    "                    'pos_recall': round(report['True']['recall'], 2),\n",
    "                    'accuracy': round(report['accuracy'], 2),\n",
    "                    'weighted_precision': round(report['weighted avg']['precision'], 2),\n",
    "                    'weighted_recall': round(report['weighted avg']['recall'], 2)})\n",
    "    results = results.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "print('Total positive cases: ', users_df['coded_as_witness'].sum())\n",
    "print('Total negative cases: ', users_df['coded_as_witness'].count() - users_df['coded_as_witness'].sum())\n",
    "# Print table:\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc. Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>added_at</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>created_at</th>\n",
       "      <th>data_source</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>description</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>...</th>\n",
       "      <th>user_network_update_observed_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>local_profile_location</th>\n",
       "      <th>local_timezone</th>\n",
       "      <th>coded_as_witness</th>\n",
       "      <th>tweet_from_locality</th>\n",
       "      <th>three_local_metrics</th>\n",
       "      <th>local_tw_and_local_profile</th>\n",
       "      <th>local_tw_and_local_tz</th>\n",
       "      <th>local_tz_and_local_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-28 20:42:59.273657+00:00</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.135798</td>\n",
       "      <td>2013-03-01 19:23:11+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>If You Want To Live A Happy Life ❇ change your...</td>\n",
       "      <td>3.905631e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-30 13:58:20.296918+00:00</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.122066</td>\n",
       "      <td>2014-01-20 00:34:57+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>Employee Giving PM @Microsoft.A daydreamer w/ ...</td>\n",
       "      <td>1.785776e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-26 19:51:45.107222+00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077120</td>\n",
       "      <td>2012-07-24 13:47:47+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>Making an impact isn’t something reserved for ...</td>\n",
       "      <td>8.518251e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-26 11:13:05.769123+00:00</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.167070</td>\n",
       "      <td>2010-12-16 17:30:04+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>Eyeing global entropy through a timeline windo...</td>\n",
       "      <td>4.315565e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-26 14:19:23.604361+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-04-24 12:08:14+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Producer. Show Control Designer. Project Coord...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          added_at  betweenness_centrality  \\\n",
       "0 2017-08-28 20:42:59.273657+00:00                0.000043   \n",
       "1 2017-08-30 13:58:20.296918+00:00                0.000015   \n",
       "2 2017-08-26 19:51:45.107222+00:00                0.000000   \n",
       "3 2017-08-26 11:13:05.769123+00:00                0.000383   \n",
       "4 2017-08-26 14:19:23.604361+00:00                     NaN   \n",
       "\n",
       "   closeness_centrality                created_at  data_source  \\\n",
       "0              0.135798 2013-03-01 19:23:11+00:00            1   \n",
       "1              0.122066 2014-01-20 00:34:57+00:00            1   \n",
       "2              0.077120 2012-07-24 13:47:47+00:00            3   \n",
       "3              0.167070 2010-12-16 17:30:04+00:00            1   \n",
       "4                   NaN 2009-04-24 12:08:14+00:00            1   \n",
       "\n",
       "   default_profile  default_profile_image  degree_centrality  \\\n",
       "0            False                  False           0.000304   \n",
       "1             True                  False           0.000243   \n",
       "2            False                  False           0.000061   \n",
       "3            False                  False           0.000668   \n",
       "4            False                  False                NaN   \n",
       "\n",
       "                                         description  eigenvector_centrality  \\\n",
       "0  If You Want To Live A Happy Life ❇ change your...            3.905631e-07   \n",
       "1  Employee Giving PM @Microsoft.A daydreamer w/ ...            1.785776e-07   \n",
       "2  Making an impact isn’t something reserved for ...            8.518251e-14   \n",
       "3  Eyeing global entropy through a timeline windo...            4.315565e-05   \n",
       "4  Producer. Show Control Designer. Project Coord...                     NaN   \n",
       "\n",
       "   ...  user_network_update_observed_at  verified  local_profile_location  \\\n",
       "0  ...                             None     False                   False   \n",
       "1  ...                             None     False                   False   \n",
       "2  ...                             None     False                   False   \n",
       "3  ...                             None     False                   False   \n",
       "4  ...                             None     False                   False   \n",
       "\n",
       "   local_timezone  coded_as_witness  tweet_from_locality  three_local_metrics  \\\n",
       "0            True             False                False                False   \n",
       "1           False             False                False                False   \n",
       "2           False             False                 True                False   \n",
       "3           False             False                False                False   \n",
       "4           False             False                False                False   \n",
       "\n",
       "  local_tw_and_local_profile local_tw_and_local_tz  local_tz_and_local_profile  \n",
       "0                      False                 False                       False  \n",
       "1                      False                 False                       False  \n",
       "2                      False                 False                       False  \n",
       "3                      False                 False                       False  \n",
       "4                      False                 False                       False  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent length of profile description:\n",
    "users_df['description_length'] = users_df.description.str.len()\n",
    "#users_df = users_df.drop(['description_length'], axis=1)\n",
    "\n",
    "# Profile language is English:\n",
    "users_df['lang_is_en'] = users_df['lang'] == 'en'\n",
    "users_df = users_df.drop(['lang'], axis=1)\n",
    "\n",
    "# translator_type exists:\n",
    "users_df['has_translator_type'] = users_df['translator_type'] != 'none'\n",
    "users_df = users_df.drop(['translator_type'], axis=1)\n",
    "\n",
    "# Url in profile:\n",
    "users_df['has_url'] = users_df['url'].notnull()\n",
    "\n",
    "# User has changed screen_name during collection period:\n",
    "users_df['changed_screen_name'] = users_df['old_screen_name'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns:  is_deleted\n",
      "Dropping columns:  is_deleted_observed\n",
      "Dropping columns:  katz_centrality\n",
      "Dropping columns:  needs_phone_verification\n",
      "Dropping columns:  protected\n",
      "Dropping columns:  ratio_media\n",
      "Dropping columns:  suspended\n",
      "Dropping columns:  user_followers\n",
      "Dropping columns:  user_followers_update\n",
      "Dropping columns:  user_following\n",
      "Dropping columns:  user_following_update\n",
      "Dropping columns:  user_network_update_observed_at\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with only one unique value:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 1:\n",
    "        print('Dropping columns: ', col)\n",
    "        users_df = users_df.drop([col], axis=1)\n",
    "        \n",
    "# TODO: consider dropping where value_counts() == 1, or is the alternative NaN value useful?:\n",
    "    # protected and ratio_media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to represent age of account at time of detection, and how soon\n",
    "# after the beginning of the event that the account was first detected.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate whole days between two dates:\n",
    "def get_age_in_days(date_str, anchor_date):\n",
    "    date_str = str(date_str) # In case date_str is already a datetime obj\n",
    "    if date_str[-3:-2] == \":\":\n",
    "        date_str = date_str[:-3] + date_str[-2:]\n",
    "    try:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    except:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S.%f%z')\n",
    "    return abs((anchor_date - datetime_object).days)\n",
    "\n",
    "    \n",
    "# Get dates of event:\n",
    "e = Event.objects.all()[0]\n",
    "end = max(e.time_end, e.kw_stream_end, e.gps_stream_end)\n",
    "start = min(e.time_start, e.kw_stream_start, e.gps_stream_start)\n",
    "\n",
    "\n",
    "# Create column for age of account at end of data collection period:\n",
    "users_df['account_age'] = users_df['created_at'].apply(get_age_in_days, args=(end,))\n",
    "\n",
    "# Create column for how early from beginning of event account was first detected:\n",
    "users_df['day_of_detection'] = users_df['added_at'].apply(get_age_in_days, args=(start,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "########## Temporary handling of row with nan for ########################################\n",
    "########## geo_enabled, has_extended_profile, is_translation_enabled, verified: ##########\n",
    "##########################################################################################\n",
    "#users_df = users_df.drop(users_df.index[221])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_source\n",
      "[1 3 0] \n",
      "\n",
      "user_class\n",
      "[2 1] \n",
      "\n",
      "day_of_detection\n",
      "[3 5 1 2 6 4 7 8] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check columns for categorical candidates:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 20:\n",
    "        if len(users_df[col].unique()) == 2 and 0 in users_df[col].unique() and 1 in users_df[col].unique():\n",
    "            continue # Already encoded as True/False\n",
    "        print(col)\n",
    "        print(users_df[col].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns as one-hot\n",
    "\n",
    "# Only two user_class categories, so binarise:\n",
    "users_df['is_user_class_2'] = users_df['user_class'] == 1.\n",
    "users_df = users_df.drop(['user_class'], axis=1)\n",
    "\n",
    "# Encode data_source categories (data_source=2 excluded as not extant in this dataset)\n",
    "# data_source=0 is not encoded as we only need n-1 columns to represent n categories.\n",
    "users_df['is_data_source_1'] = users_df['data_source'] == 1.\n",
    "#users_df['is_data_source_2'] = users_df['data_source'] == 2.\n",
    "users_df['is_data_source_3'] = users_df['data_source'] == 3.\n",
    "users_df = users_df.drop(['data_source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_profile\n",
      "[False  True]\n",
      "default_profile_image\n",
      "[False  True]\n",
      "geo_enabled\n",
      "[ True False]\n",
      "has_extended_profile\n",
      "[ True False]\n",
      "is_translation_enabled\n",
      "[False  True]\n",
      "verified\n",
      "[False  True]\n",
      "local_profile_location\n",
      "[False  True]\n",
      "local_timezone\n",
      "[ True False]\n",
      "coded_as_witness\n",
      "[False  True]\n",
      "tweet_from_locality\n",
      "[False  True]\n",
      "three_local_metrics\n",
      "[False  True]\n",
      "local_tw_and_local_profile\n",
      "[False  True]\n",
      "local_tw_and_local_tz\n",
      "[False  True]\n",
      "local_tz_and_local_profile\n",
      "[False  True]\n",
      "lang_is_en\n",
      "[ True False]\n",
      "has_translator_type\n",
      "[ True False]\n",
      "has_url\n",
      "[False  True]\n",
      "changed_screen_name\n",
      "[False  True]\n",
      "is_user_class_2\n",
      "[False  True]\n",
      "is_data_source_1\n",
      "[ True False]\n",
      "is_data_source_3\n",
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "# Convert True/False columns to 0/1\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) == 2:\n",
    "        if True in users_df[col].values and False in users_df[col].values:\n",
    "            print(col)\n",
    "            print(users_df[col].unique())\n",
    "            users_df[col] = users_df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix negative values in in_degree and out_degree: an error from data collection:\n",
    "users_df.loc[users_df['in_degree'] < 0, 'in_degree'] = 0\n",
    "users_df.loc[users_df['out_degree'] < 0, 'out_degree'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499, 50)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########################################\n",
    "## Write/read dataframe to temp file ###\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "import pandas as pd\n",
    "path = 'data/harvey_user_location/cleaned_users_df.csv'\n",
    "\n",
    "users_df.to_csv(path)\n",
    "\n",
    "#users_df = pd.read_csv(path, index_col=0) \n",
    "#users_df = users_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "print(users_df.shape)\n",
    "\n",
    "########################################\n",
    "########################################\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The data is now ready for classification. Random forests will be used initially and will suit the relatively small dataset. These will be compared to XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['added_at', 'betweenness_centrality', 'closeness_centrality',\n",
       "       'created_at', 'default_profile', 'default_profile_image',\n",
       "       'degree_centrality', 'description', 'eigenvector_centrality',\n",
       "       'favourites_count', 'followers_count', 'friends_count', 'geo_enabled',\n",
       "       'has_extended_profile', 'id', 'in_degree', 'is_translation_enabled',\n",
       "       'listed_count', 'load_centrality', 'location', 'name',\n",
       "       'old_screen_name', 'out_degree', 'ratio_detected', 'ratio_original',\n",
       "       'screen_name', 'statuses_count', 'tweets_per_hour',\n",
       "       'undirected_eigenvector_centrality', 'url', 'user_id', 'verified',\n",
       "       'local_profile_location', 'local_timezone', 'coded_as_witness',\n",
       "       'tweet_from_locality', 'three_local_metrics',\n",
       "       'local_tw_and_local_profile', 'local_tw_and_local_tz',\n",
       "       'local_tz_and_local_profile', 'description_length', 'lang_is_en',\n",
       "       'has_translator_type', 'has_url', 'changed_screen_name', 'account_age',\n",
       "       'day_of_detection', 'is_user_class_2', 'is_data_source_1',\n",
       "       'is_data_source_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display all columns of DF in cell:\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Check available columns\n",
    "users_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns which are not relevant to the classification problem are dropped, and the test and train partitions are created. \n",
    "\n",
    "The column 'local_profile_location_manual' is removed as 'local_profile_location' is preferred for scalability.\n",
    "\n",
    "Any remaining NAN values are also filled with zeroes. \n",
    "\n",
    "The dataframe is also checked for any columns that were not cleaned appropriately (for example, still True/False rather than 1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1199, 39) (1199,)\n",
      "(300, 39) (300,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>has_extended_profile</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>is_translation_enabled</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>load_centrality</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>ratio_detected</th>\n",
       "      <th>ratio_original</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>tweets_per_hour</th>\n",
       "      <th>undirected_eigenvector_centrality</th>\n",
       "      <th>verified</th>\n",
       "      <th>local_profile_location</th>\n",
       "      <th>local_timezone</th>\n",
       "      <th>tweet_from_locality</th>\n",
       "      <th>three_local_metrics</th>\n",
       "      <th>local_tw_and_local_profile</th>\n",
       "      <th>local_tw_and_local_tz</th>\n",
       "      <th>local_tz_and_local_profile</th>\n",
       "      <th>description_length</th>\n",
       "      <th>lang_is_en</th>\n",
       "      <th>has_translator_type</th>\n",
       "      <th>has_url</th>\n",
       "      <th>changed_screen_name</th>\n",
       "      <th>account_age</th>\n",
       "      <th>day_of_detection</th>\n",
       "      <th>is_user_class_2</th>\n",
       "      <th>is_data_source_1</th>\n",
       "      <th>is_data_source_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.135798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>3.905631e-07</td>\n",
       "      <td>2030</td>\n",
       "      <td>519</td>\n",
       "      <td>1859</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>6042</td>\n",
       "      <td>0.830619</td>\n",
       "      <td>5.377061e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1645</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.122066</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>1.785776e-07</td>\n",
       "      <td>1015</td>\n",
       "      <td>446</td>\n",
       "      <td>661</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>531</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>2.210768e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>8.518251e-14</td>\n",
       "      <td>12</td>\n",
       "      <td>277</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>774</td>\n",
       "      <td>0.152563</td>\n",
       "      <td>7.589479e-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.167070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>4.315565e-05</td>\n",
       "      <td>347</td>\n",
       "      <td>608</td>\n",
       "      <td>496</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>6383</td>\n",
       "      <td>0.333378</td>\n",
       "      <td>3.327919e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2451</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>25</td>\n",
       "      <td>321</td>\n",
       "      <td>1687</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>8360</td>\n",
       "      <td>10.142596</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3052</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   betweenness_centrality  closeness_centrality  default_profile  \\\n",
       "0                0.000043              0.135798                0   \n",
       "1                0.000015              0.122066                1   \n",
       "2                0.000000              0.077120                0   \n",
       "3                0.000383              0.167070                0   \n",
       "4                0.000000              0.000000                0   \n",
       "\n",
       "   default_profile_image  degree_centrality  eigenvector_centrality  \\\n",
       "0                      0           0.000304            3.905631e-07   \n",
       "1                      0           0.000243            1.785776e-07   \n",
       "2                      0           0.000061            8.518251e-14   \n",
       "3                      0           0.000668            4.315565e-05   \n",
       "4                      0           0.000000            0.000000e+00   \n",
       "\n",
       "   favourites_count  followers_count  friends_count  geo_enabled  \\\n",
       "0              2030              519           1859            1   \n",
       "1              1015              446            661            1   \n",
       "2                12              277             48            1   \n",
       "3               347              608            496            1   \n",
       "4                25              321           1687            1   \n",
       "\n",
       "   has_extended_profile  in_degree  is_translation_enabled  listed_count  \\\n",
       "0                     1          1                       0            36   \n",
       "1                     0          0                       0             6   \n",
       "2                     0          0                       0           492   \n",
       "3                     0          0                       0           138   \n",
       "4                     0          0                       0            15   \n",
       "\n",
       "   load_centrality  out_degree  ratio_detected  ratio_original  \\\n",
       "0         0.000048           3        0.006803        0.476190   \n",
       "1         0.000019           0        0.200000        0.400000   \n",
       "2         0.000000           0        0.851852        1.000000   \n",
       "3         0.000388           0        0.033898        0.220339   \n",
       "4         0.000000           0        0.027855        0.006685   \n",
       "\n",
       "   statuses_count  tweets_per_hour  undirected_eigenvector_centrality  \\\n",
       "0            6042         0.830619                       5.377061e-05   \n",
       "1             531         0.028252                       2.210768e-06   \n",
       "2             774         0.152563                       7.589479e-11   \n",
       "3            6383         0.333378                       3.327919e-04   \n",
       "4            8360        10.142596                       0.000000e+00   \n",
       "\n",
       "   verified  local_profile_location  local_timezone  tweet_from_locality  \\\n",
       "0         0                       0               1                    0   \n",
       "1         0                       0               0                    0   \n",
       "2         0                       0               0                    1   \n",
       "3         0                       0               0                    0   \n",
       "4         0                       0               0                    0   \n",
       "\n",
       "   three_local_metrics  local_tw_and_local_profile  local_tw_and_local_tz  \\\n",
       "0                    0                           0                      0   \n",
       "1                    0                           0                      0   \n",
       "2                    0                           0                      0   \n",
       "3                    0                           0                      0   \n",
       "4                    0                           0                      0   \n",
       "\n",
       "   local_tz_and_local_profile  description_length  lang_is_en  \\\n",
       "0                           0                  96           1   \n",
       "1                           0                 124           1   \n",
       "2                           0                 134           1   \n",
       "3                           0                 128           1   \n",
       "4                           0                 136           1   \n",
       "\n",
       "   has_translator_type  has_url  changed_screen_name  account_age  \\\n",
       "0                    1        0                    0         1645   \n",
       "1                    0        1                    0         1321   \n",
       "2                    0        1                    0         1865   \n",
       "3                    0        1                    0         2451   \n",
       "4                    0        0                    0         3052   \n",
       "\n",
       "   day_of_detection  is_user_class_2  is_data_source_1  is_data_source_3  \n",
       "0                 3                0                 1                 0  \n",
       "1                 5                0                 1                 0  \n",
       "2                 1                0                 0                 1  \n",
       "3                 1                0                 1                 0  \n",
       "4                 1                0                 1                 0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excluded features:\n",
    "dropped_cols = ['added_at', 'description', 'created_at', 'id',  'location', 'name', 'url', \n",
    "                'screen_name', 'user_id', 'coded_as_witness', 'old_screen_name']\n",
    "\n",
    "xVar = users_df.drop(dropped_cols, axis=1)\n",
    "\n",
    "yVar = users_df['coded_as_witness']\n",
    "\n",
    "# Partition data sets:\n",
    "xVar = xVar.fillna(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xVar, yVar, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "xVar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline RandomForest classifier is created for comparison to parameter-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def get_results(clf):\n",
    "    # Evaluate Training Data\n",
    "    pred_train = clf.predict(X_train)\n",
    "    errors_train = sum(abs(pred_train - y_train))\n",
    "    train_acc = round((1 - errors_train/len(pred_train)) * 100, 2)\n",
    "    print('Accuracy on Training Data:', train_acc, '%.')\n",
    "    # Evaluate Test Data\n",
    "    pred = clf.predict(X_test)\n",
    "    errors = sum(abs(pred - y_test))\n",
    "    test_acc = round((1 - errors/len(pred)) * 100, 2)\n",
    "    print('Accuracy on Test Data:', test_acc, '%.')\n",
    "    # Evaluate AUC Score\n",
    "    auc_score = round(roc_auc_score(y_test, pred), 2)\n",
    "    print('AUC score: ', auc_score)\n",
    "    # Confusion Matrix:\n",
    "    print(pd.crosstab(y_test, pred, rownames=['Actual Result'], colnames=['Predicted Result']))\n",
    "    # Evaluate Precision & Recall:\n",
    "    prec = round(precision_score(y_test, pred), 2)\n",
    "    recall = round(recall_score(y_test, pred), 2)\n",
    "    return(train_acc, test_acc, auc_score, prec, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 99.92 %.\n",
      "Accuracy on Test Data: 83.67 %.\n",
      "AUC score:  0.78\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 195  18\n",
      "1                  31  56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  train_acc  test_acc   auc  precision  recall\n",
       "0  rfc_baseline      99.92     83.67  0.78       0.76    0.64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "results_df = pd.DataFrame(columns=['model', 'train_acc', 'test_acc', 'auc', 'precision', 'recall'])\n",
    "results_df.loc[len(results_df)] = ['rfc_baseline'] + list(get_results(clf))\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosles/projects/crisis-data/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "                       criterion='gini', max_depth=10, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=1,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       n_estimators=600, n_jobs=-1, oob_score=False,\n",
      "                       random_state=None, verbose=0, warm_start=False)\n",
      "Accuracy on Training Data: 97.16 %.\n",
      "Accuracy on Test Data: 84.33 %.\n",
      "AUC score:  0.79\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 196  17\n",
      "1                  30  57\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  train_acc  test_acc   auc  precision  recall\n",
       "0  rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1        rfc_gs      97.16     84.33  0.79       0.77    0.66"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_jobs': [-1],\n",
    "    'class_weight': ['balanced'],\n",
    "    'bootstrap': [True],\n",
    "#    'max_depth': [2, 10, 20, 50, 100],\n",
    "    'max_depth': [10, 50, 100],\n",
    "#    'max_features': [0.1, 0.2, 0.3, 0.4, 'sqrt', 'log2'],\n",
    "    'max_features': [0.4, 'sqrt', 'log2'],\n",
    "#    'min_samples_leaf': [1, 2, 3],\n",
    "#    'min_samples_split': [2, 3, 5],\n",
    "#    'n_estimators': [10, 15, 20, 40, 60, 100, 200, 300, 400, 500, 600, 700, 800],\n",
    "    'n_estimators': [300, 400, 600],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier() \n",
    "\n",
    "clf = GridSearchCV(rfc, param_grid, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "best_clf_rf = clf.best_estimator_\n",
    "best_params_rf = clf.best_params_\n",
    "\n",
    "print(best_clf_rf)\n",
    "\n",
    "results_df.loc[len(results_df)] = ['rfc_gs'] + list(get_results(best_clf_rf))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Pruning\n",
    "We can check the importance of each feature as per the model and use these values to inform the choice of features in future iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('local_profile_location', 0.14),\n",
       " ('tweet_from_locality', 0.06),\n",
       " ('listed_count', 0.05),\n",
       " ('ratio_detected', 0.05),\n",
       " ('is_data_source_3', 0.05),\n",
       " ('favourites_count', 0.04),\n",
       " ('followers_count', 0.04),\n",
       " ('statuses_count', 0.04),\n",
       " ('tweets_per_hour', 0.04),\n",
       " ('undirected_eigenvector_centrality', 0.04),\n",
       " ('local_tw_and_local_profile', 0.04),\n",
       " ('account_age', 0.04),\n",
       " ('is_data_source_1', 0.04),\n",
       " ('closeness_centrality', 0.03),\n",
       " ('degree_centrality', 0.03),\n",
       " ('friends_count', 0.03),\n",
       " ('ratio_original', 0.03),\n",
       " ('description_length', 0.03),\n",
       " ('day_of_detection', 0.03),\n",
       " ('betweenness_centrality', 0.02),\n",
       " ('eigenvector_centrality', 0.02),\n",
       " ('load_centrality', 0.02),\n",
       " ('out_degree', 0.02),\n",
       " ('local_tz_and_local_profile', 0.02),\n",
       " ('default_profile', 0.01),\n",
       " ('geo_enabled', 0.01),\n",
       " ('has_extended_profile', 0.01),\n",
       " ('in_degree', 0.01),\n",
       " ('local_tw_and_local_tz', 0.01),\n",
       " ('has_url', 0.01),\n",
       " ('default_profile_image', 0.0),\n",
       " ('is_translation_enabled', 0.0),\n",
       " ('verified', 0.0),\n",
       " ('local_timezone', 0.0),\n",
       " ('three_local_metrics', 0.0),\n",
       " ('lang_is_en', 0.0),\n",
       " ('has_translator_type', 0.0),\n",
       " ('changed_screen_name', 0.0),\n",
       " ('is_user_class_2', 0.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importance = sorted(list(zip(X_train, best_clf_rf.feature_importances_.round(decimals=2))), key=lambda x: x[1], reverse=True)\n",
    "feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with lowest importance:\n",
    "features_to_remove = [x[0] for x in feat_importance if x[1] < 0.01]\n",
    "# We can also remove features which are duplicated:\n",
    "duplicated_features = ['local_tz_and_local_profile']\n",
    "features_to_remove += duplicated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 97.66 %.\n",
      "Accuracy on Test Data: 84.33 %.\n",
      "AUC score:  0.79\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 195  18\n",
      "1                  29  58\n",
      "Accuracy on Training Data: 97.58 %.\n",
      "Accuracy on Test Data: 82.0 %.\n",
      "AUC score:  0.75\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 195  18\n",
      "1                  36  51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfc_pruned_0.01</td>\n",
       "      <td>97.66</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfc_pruned_0.02</td>\n",
       "      <td>97.58</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  train_acc  test_acc   auc  precision  recall\n",
       "0     rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1           rfc_gs      97.16     84.33  0.79       0.77    0.66\n",
       "2  rfc_pruned_0.01      97.66     84.33  0.79       0.76    0.67\n",
       "3  rfc_pruned_0.02      97.58     82.00  0.75       0.74    0.59"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary storage\n",
    "X_train_unpruned = X_train\n",
    "X_test_unpruned = X_test\n",
    "\n",
    "# Prune\n",
    "X_train = X_train.drop(features_to_remove, axis=1)\n",
    "X_test = X_test.drop(features_to_remove, axis=1)\n",
    "\n",
    "# Use params from previous GridSearch (may not be optimal given pruned set)\n",
    "clf = RandomForestClassifier().set_params(**best_params_rf)\n",
    "clf.fit(X_train, y_train)\n",
    "results_df.loc[len(results_df)] = ['rfc_pruned_0.01'] + list(get_results(clf))\n",
    "\n",
    "\n",
    "############ Second round of pruning ############\n",
    "\n",
    "# Remove features with lowest importance:\n",
    "features_to_remove = [x[0] for x in feat_importance if x[1] < 0.02 and x[1] >= 0.01 and x[0] not in duplicated_features]\n",
    "X_train = X_train.drop(features_to_remove, axis=1)\n",
    "X_test = X_test.drop(features_to_remove, axis=1)\n",
    "\n",
    "# Use params from previous GridSearch (may not be optimal given pruned set)\n",
    "clf = RandomForestClassifier().set_params(**best_params_rf)\n",
    "clf.fit(X_train, y_train)\n",
    "results_df.loc[len(results_df)] = ['rfc_pruned_0.02'] + list(get_results(clf))\n",
    "\n",
    "\n",
    "# Unprune datasets\n",
    "X_train = X_train_unpruned\n",
    "X_test = X_test_unpruned \n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 99.83 %.\n",
      "Accuracy on Test Data: 82.0 %.\n",
      "AUC score:  0.74\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 197  16\n",
      "1                  38  49\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Baseline model\n",
    "clf = XGBClassifier(max_depth=6, eval_metric='auc')\n",
    "clf.fit(X_train, y_train)\n",
    "results_df.loc[len(results_df)] = ['xgb_baseline'] + list(get_results(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 95.75 %.\n",
      "Accuracy on Test Data: 82.33 %.\n",
      "AUC score:  0.8\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 181  32\n",
      "1                  21  66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfc_pruned_0.01</td>\n",
       "      <td>97.66</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfc_pruned_0.02</td>\n",
       "      <td>97.58</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_baseline</td>\n",
       "      <td>99.83</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb_weighted</td>\n",
       "      <td>95.75</td>\n",
       "      <td>82.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  train_acc  test_acc   auc  precision  recall\n",
       "0     rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1           rfc_gs      97.16     84.33  0.79       0.77    0.66\n",
       "2  rfc_pruned_0.01      97.66     84.33  0.79       0.76    0.67\n",
       "3  rfc_pruned_0.02      97.58     82.00  0.75       0.74    0.59\n",
       "4     xgb_baseline      99.83     82.00  0.74       0.75    0.56\n",
       "5     xgb_weighted      95.75     82.33  0.80       0.67    0.76"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set weights for unbalanced classes as their proportion:\n",
    "total = sum(y_train.value_counts())\n",
    "y_weights = y_train.apply(lambda x: (total-y_train.value_counts()[x]) /\n",
    "                          y_train.value_counts()[x])\n",
    "clf = XGBClassifier(max_depth=6, eval_metric='auc')\n",
    "clf.fit(X_train, y_train, sample_weight=y_weights)\n",
    "results_df.loc[len(results_df)] = ['xgb_weighted'] + list(get_results(clf))\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 90.24 %.\n",
      "Accuracy on Test Data: 81.67 %.\n",
      "AUC score:  0.82\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 172  41\n",
      "1                  14  73\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
      "              gamma=1, learning_rate=0.05, max_delta_step=0, max_depth=8,\n",
      "              min_child_weight=5, missing=None, n_estimators=200, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfc_pruned_0.01</td>\n",
       "      <td>97.66</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfc_pruned_0.02</td>\n",
       "      <td>97.58</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_baseline</td>\n",
       "      <td>99.83</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb_weighted</td>\n",
       "      <td>95.75</td>\n",
       "      <td>82.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb_weighted_gs</td>\n",
       "      <td>90.24</td>\n",
       "      <td>81.67</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  train_acc  test_acc   auc  precision  recall\n",
       "0     rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1           rfc_gs      97.16     84.33  0.79       0.77    0.66\n",
       "2  rfc_pruned_0.01      97.66     84.33  0.79       0.76    0.67\n",
       "3  rfc_pruned_0.02      97.58     82.00  0.75       0.74    0.59\n",
       "4     xgb_baseline      99.83     82.00  0.74       0.75    0.56\n",
       "5     xgb_weighted      95.75     82.33  0.80       0.67    0.76\n",
       "6  xgb_weighted_gs      90.24     81.67  0.82       0.64    0.84"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# negative_instances / positive_instances (for scale_pos_weight) = \n",
    "neg_proportion = (len(y_train)-sum(y_train)) / sum(y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8, 10],\n",
    "#    'max_depth': [6, 10, 30],\n",
    "    'eval_metric' : ['auc'],\n",
    "#    'gamma' : [0, 0.5, 1, 2, 5],\n",
    "    'gamma' : [1],\n",
    "#    'learning_rate' : [0.05, 0.1, 0.3, 0.5],\n",
    "    'learning_rate' : [0.05, 0.1],\n",
    "    'max_delta_step' : [0, 1],\n",
    "#    'n_estimators' : [5, 10, 50, 100, 200, 300],\n",
    "    'n_estimators' : [200, 300],\n",
    "    'n_jobs' : [-1],\n",
    "    'scale_pos_weight' : [1],\n",
    "    'min_child_weight' : [0, 1, 5]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "clf = GridSearchCV(xgb, param_grid, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train, sample_weight=y_weights)\n",
    "best_clf_xgb = clf.best_estimator_\n",
    "best_params_xgb = clf.best_params_\n",
    "\n",
    "results_df.loc[len(results_df)] = ['xgb_weighted_gs'] + list(get_results(best_clf_xgb))\n",
    "print(best_clf_xgb)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('local_profile_location', 0.25),\n",
       " ('tweet_from_locality', 0.13),\n",
       " ('listed_count', 0.05),\n",
       " ('betweenness_centrality', 0.04),\n",
       " ('default_profile', 0.03),\n",
       " ('degree_centrality', 0.03),\n",
       " ('load_centrality', 0.03),\n",
       " ('undirected_eigenvector_centrality', 0.03),\n",
       " ('day_of_detection', 0.03),\n",
       " ('closeness_centrality', 0.02),\n",
       " ('eigenvector_centrality', 0.02),\n",
       " ('favourites_count', 0.02),\n",
       " ('followers_count', 0.02),\n",
       " ('friends_count', 0.02),\n",
       " ('geo_enabled', 0.02),\n",
       " ('ratio_detected', 0.02),\n",
       " ('ratio_original', 0.02),\n",
       " ('statuses_count', 0.02),\n",
       " ('tweets_per_hour', 0.02),\n",
       " ('local_timezone', 0.02),\n",
       " ('local_tw_and_local_tz', 0.02),\n",
       " ('description_length', 0.02),\n",
       " ('has_url', 0.02),\n",
       " ('account_age', 0.02),\n",
       " ('has_extended_profile', 0.01),\n",
       " ('in_degree', 0.01),\n",
       " ('out_degree', 0.01),\n",
       " ('local_tw_and_local_profile', 0.01),\n",
       " ('default_profile_image', 0.0),\n",
       " ('is_translation_enabled', 0.0),\n",
       " ('verified', 0.0),\n",
       " ('three_local_metrics', 0.0),\n",
       " ('local_tz_and_local_profile', 0.0),\n",
       " ('lang_is_en', 0.0),\n",
       " ('has_translator_type', 0.0),\n",
       " ('changed_screen_name', 0.0),\n",
       " ('is_user_class_2', 0.0),\n",
       " ('is_data_source_1', 0.0),\n",
       " ('is_data_source_3', 0.0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importance = sorted(list(zip(X_train, best_clf_xgb.feature_importances_.round(decimals=2))), key=lambda x: x[1], reverse=True)\n",
    "feat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from feature importance:\n",
    "* Geographic features are expectedly the top two features\n",
    "* The four local combination metrics are not important (three_local_metrics, local_tw_and_local_profile, local_tw_and_local_tz, local_tz_and_local_profile)\n",
    "* data_source and user_class are unimportant\n",
    "* betweeness_centrality and undirected_eigenvector_centrality are the most important network metrics\n",
    "* day_of_detection is more important than expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Model Without Network Metrics\n",
    "Network metrics are expensive to obtain from the Twitter API, yet they appear to be important to the above models.\n",
    "\n",
    "We can test equivalent models excluding these features to measure their true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 96.0 %.\n",
      "Accuracy on Test Data: 84.67 %.\n",
      "AUC score:  0.8\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 195  18\n",
      "1                  28  59\n",
      "Accuracy on Training Data: 97.91 %.\n",
      "Accuracy on Test Data: 81.33 %.\n",
      "AUC score:  0.73\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 197  16\n",
      "1                  40  47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfc_pruned_0.01</td>\n",
       "      <td>97.66</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfc_pruned_0.02</td>\n",
       "      <td>97.58</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_baseline</td>\n",
       "      <td>99.83</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb_weighted</td>\n",
       "      <td>95.75</td>\n",
       "      <td>82.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb_weighted_gs</td>\n",
       "      <td>90.24</td>\n",
       "      <td>81.67</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rfc_no_network</td>\n",
       "      <td>96.00</td>\n",
       "      <td>84.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb_no_network</td>\n",
       "      <td>97.91</td>\n",
       "      <td>81.33</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  train_acc  test_acc   auc  precision  recall\n",
       "0     rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1           rfc_gs      97.16     84.33  0.79       0.77    0.66\n",
       "2  rfc_pruned_0.01      97.66     84.33  0.79       0.76    0.67\n",
       "3  rfc_pruned_0.02      97.58     82.00  0.75       0.74    0.59\n",
       "4     xgb_baseline      99.83     82.00  0.74       0.75    0.56\n",
       "5     xgb_weighted      95.75     82.33  0.80       0.67    0.76\n",
       "6  xgb_weighted_gs      90.24     81.67  0.82       0.64    0.84\n",
       "7   rfc_no_network      96.00     84.67  0.80       0.77    0.68\n",
       "8   xgb_no_network      97.91     81.33  0.73       0.75    0.54"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary storage of unpruned dataset\n",
    "X_train_unpruned = X_train\n",
    "X_test_unpruned = X_test\n",
    "\n",
    "\n",
    "features_to_remove = ['undirected_eigenvector_centrality', 'degree_centrality', 'closeness_centrality', \n",
    "                      'eigenvector_centrality', 'load_centrality', 'betweenness_centrality']\n",
    "X_train = X_train.drop(features_to_remove, axis=1)\n",
    "X_test = X_test.drop(features_to_remove, axis=1)\n",
    "\n",
    "\n",
    "# Use params from previous GridSearch (may not be optimal given pruned set)\n",
    "clf = RandomForestClassifier().set_params(**best_params_rf)\n",
    "clf.fit(X_train, y_train)\n",
    "results_df.loc[len(results_df)] = ['rfc_no_network'] + list(get_results(clf))\n",
    "\n",
    "xgb = XGBClassifier().set_params(**best_params_xgb)\n",
    "xgb.fit(X_train, y_train)\n",
    "results_df.loc[len(results_df)] = ['xgb_no_network'] + list(get_results(xgb))\n",
    "\n",
    "\n",
    "# Unprune datasets\n",
    "X_train = X_train_unpruned\n",
    "X_test = X_test_unpruned \n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, removing network features from the XGB model causes a significant drop in AUC, precision and recall. The biggest change is in recall, and as this is the most important feature in this application, we must consider collecting these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['betweenness_centrality', 'closeness_centrality', 'default_profile',\n",
       "       'default_profile_image', 'degree_centrality', 'eigenvector_centrality',\n",
       "       'favourites_count', 'followers_count', 'friends_count', 'geo_enabled',\n",
       "       'has_extended_profile', 'in_degree', 'is_translation_enabled',\n",
       "       'listed_count', 'load_centrality', 'out_degree', 'ratio_detected',\n",
       "       'ratio_original', 'statuses_count', 'tweets_per_hour',\n",
       "       'undirected_eigenvector_centrality', 'verified',\n",
       "       'local_profile_location', 'local_timezone', 'tweet_from_locality',\n",
       "       'three_local_metrics', 'local_tw_and_local_profile',\n",
       "       'local_tw_and_local_tz', 'local_tz_and_local_profile',\n",
       "       'description_length', 'lang_is_en', 'has_translator_type', 'has_url',\n",
       "       'changed_screen_name', 'account_age', 'day_of_detection',\n",
       "       'is_user_class_2', 'is_data_source_1', 'is_data_source_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xVar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are not available at the time of Tw detection or update throughout event?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0, score=0.886, total=   0.1s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0, score=0.868, total=   0.1s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0, score=0.870, total=   0.1s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0, score=0.886, total=   0.1s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=5, max_depth=2, learning_rate=0.1, gamma=1.5, colsample_bytree=1.0, score=0.883, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8, score=0.866, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8, score=0.834, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8, score=0.864, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8, score=0.865, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.4, reg_lambda=4.5, reg_alpha=1, n_estimators=100, min_child_weight=1, max_depth=2, learning_rate=0.01, gamma=2, colsample_bytree=0.8, score=0.865, total=   0.1s\n",
      "[CV] subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0, score=0.858, total=   0.2s\n",
      "[CV] subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0, score=0.830, total=   0.2s\n",
      "[CV] subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0, score=0.865, total=   0.2s\n",
      "[CV] subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0, score=0.862, total=   0.2s\n",
      "[CV] subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.2, reg_lambda=4.5, reg_alpha=0.5, n_estimators=250, min_child_weight=1, max_depth=2, learning_rate=0.001, gamma=0.5, colsample_bytree=1.0, score=0.859, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0, score=0.884, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0, score=0.866, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0, score=0.872, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0, score=0.875, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=0.1, colsample_bytree=1.0, score=0.883, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0, score=0.873, total=   1.0s\n",
      "[CV] subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0, score=0.848, total=   1.0s\n",
      "[CV] subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0, score=0.858, total=   1.0s\n",
      "[CV] subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0, score=0.862, total=   1.0s\n",
      "[CV] subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.5, reg_lambda=4.5, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=1.0, score=0.860, total=   1.0s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6, score=0.859, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6, score=0.857, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6, score=0.860, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6, score=0.865, total=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=3, max_depth=10, learning_rate=0.001, gamma=0.01, colsample_bytree=0.6, score=0.856, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.872, total=   0.4s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.855, total=   0.4s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.857, total=   0.4s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.860, total=   0.4s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.869, total=   0.4s\n",
      "[CV] subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.888, total=   1.8s\n",
      "[CV] subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.866, total=   1.8s\n",
      "[CV] subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.864, total=   1.8s\n",
      "[CV] subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.894, total=   1.8s\n",
      "[CV] subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=1, max_depth=7, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.878, total=   1.8s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0, score=0.883, total=   0.6s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0, score=0.844, total=   0.6s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0, score=0.865, total=   0.6s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0, score=0.880, total=   0.6s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=250, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=1.0, score=0.882, total=   0.6s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6, score=0.867, total=   0.3s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6, score=0.846, total=   0.3s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6, score=0.857, total=   0.3s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6, score=0.853, total=   0.3s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0.5, n_estimators=250, min_child_weight=7, max_depth=7, learning_rate=0.001, gamma=2, colsample_bytree=0.6, score=0.857, total=   0.3s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.875, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.855, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.866, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.859, total=   0.1s\n",
      "[CV] subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.4, reg_lambda=2, reg_alpha=0, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.01, gamma=0.3, colsample_bytree=0.6, score=0.868, total=   0.1s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8, score=0.871, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8, score=0.851, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8, score=0.853, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8, score=0.863, total=   0.2s\n",
      "[CV] subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.7, reg_lambda=1, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=10, learning_rate=0.001, gamma=0.3, colsample_bytree=0.8, score=0.866, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.865, total=   0.1s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.847, total=   0.1s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.854, total=   0.1s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.851, total=   0.1s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=7, max_depth=4, learning_rate=0.001, gamma=0.3, colsample_bytree=0.6, score=0.846, total=   0.1s\n",
      "[CV] subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8, score=0.876, total=   0.4s\n",
      "[CV] subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8, score=0.847, total=   0.4s\n",
      "[CV] subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8, score=0.860, total=   0.4s\n",
      "[CV] subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8, score=0.856, total=   0.4s\n",
      "[CV] subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.2, reg_lambda=1.5, reg_alpha=0, n_estimators=500, min_child_weight=5, max_depth=7, learning_rate=0.1, gamma=0.01, colsample_bytree=0.8, score=0.861, total=   0.4s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6, score=0.868, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6, score=0.855, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6, score=0.863, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6, score=0.859, total=   0.2s\n",
      "[CV] subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=3, reg_alpha=1, n_estimators=100, min_child_weight=5, max_depth=7, learning_rate=0.01, gamma=1.5, colsample_bytree=0.6, score=0.860, total=   0.2s\n",
      "[CV] subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0, score=0.883, total=   0.5s\n",
      "[CV] subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0, score=0.863, total=   0.5s\n",
      "[CV] subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0, score=0.876, total=   0.5s\n",
      "[CV] subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0, score=0.879, total=   0.5s\n",
      "[CV] subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.4, reg_lambda=1, reg_alpha=0, n_estimators=500, min_child_weight=7, max_depth=2, learning_rate=0.01, gamma=0.1, colsample_bytree=1.0, score=0.882, total=   0.5s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, score=0.876, total=   1.5s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, score=0.878, total=   1.5s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, score=0.857, total=   1.5s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, score=0.882, total=   1.5s\n",
      "[CV] subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0 \n",
      "[CV]  subsample=0.7, reg_lambda=3, reg_alpha=0, n_estimators=500, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, score=0.873, total=   1.5s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8, score=0.878, total=   0.6s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8, score=0.875, total=   0.6s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8, score=0.874, total=   0.6s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8, score=0.891, total=   0.6s\n",
      "[CV] subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8 \n",
      "[CV]  subsample=0.6, reg_lambda=1, reg_alpha=1, n_estimators=250, min_child_weight=1, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.8, score=0.877, total=   0.6s\n",
      "[CV] subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6, score=0.875, total=   1.6s\n",
      "[CV] subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6, score=0.873, total=   1.5s\n",
      "[CV] subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6, score=0.857, total=   1.5s\n",
      "[CV] subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6, score=0.880, total=   1.5s\n",
      "[CV] subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.6, reg_lambda=1.5, reg_alpha=0, n_estimators=1000, min_child_weight=3, max_depth=7, learning_rate=0.1, gamma=2, colsample_bytree=0.6, score=0.869, total=   1.5s\n",
      "[CV] subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6, score=0.889, total=   0.5s\n",
      "[CV] subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6, score=0.859, total=   0.5s\n",
      "[CV] subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6, score=0.871, total=   0.5s\n",
      "[CV] subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6, score=0.870, total=   0.5s\n",
      "[CV] subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.5, reg_lambda=2, reg_alpha=1, n_estimators=500, min_child_weight=7, max_depth=4, learning_rate=0.1, gamma=1.5, colsample_bytree=0.6, score=0.875, total=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   53.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.8, gamma=2,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
      "              min_child_weight=1, missing=None, n_estimators=250, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=0.6, verbosity=1)\n",
      "Accuracy on Training Data: 88.41 %.\n",
      "Accuracy on Test Data: 81.0 %.\n",
      "AUC score:  0.82\n",
      "Predicted Result    0   1\n",
      "Actual Result            \n",
      "0                 171  42\n",
      "1                  15  72\n",
      "(88.41, 81.0, 0.82, 0.63, 0.83)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# negative_instances / positive_instances (for scale_pos_weight) = \n",
    "neg_proportion = (len(y_train)-sum(y_train)) / sum(y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8, 10],\n",
    "#    'max_depth': [6, 10, 30],\n",
    "    'eval_metric' : ['auc'],\n",
    "#    'gamma' : [0, 0.5, 1, 2, 5],\n",
    "    'gamma' : [1],\n",
    "#    'learning_rate' : [0.05, 0.1, 0.3, 0.5],\n",
    "    'learning_rate' : [0.05, 0.1],\n",
    "    'max_delta_step' : [0, 1],\n",
    "#    'n_estimators' : [5, 10, 50, 100, 200, 300],\n",
    "    'n_estimators' : [200, 300],\n",
    "    'n_jobs' : [-1],\n",
    "    'scale_pos_weight' : [1],\n",
    "    'min_child_weight' : [0, 1, 5]\n",
    "}\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "               \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n",
    "               \"max_depth\": [2, 4, 7, 10],\n",
    "               \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n",
    "               \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "               \"reg_alpha\": [0, 0.5, 1],\n",
    "               \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "               \"min_child_weight\": [1, 3, 5, 7],\n",
    "               \"n_estimators\": [100, 250, 500, 1000]}\n",
    "\n",
    "xgb = XGBClassifier(verbosity = True, objective = 'binary:logistic', eval_metric = 'roc_auc')\n",
    "xgb = XGBClassifier()\n",
    "#xgb_clf = xgb.XGBClassifier(tree_method = \"gpu_exact\", predictor = \"gpu_predictor\", verbosity = True\n",
    "#                           eval_metric = [\"merror\", \"map\", \"auc\"], objective = \"multi:softmax\")\n",
    "\n",
    "#clf = GridSearchCV(xgb, param_grid, scoring='roc_auc')\n",
    "clf = RandomizedSearchCV(xgb, param_distributions = param_grid, scoring = \"roc_auc\",\n",
    "                            cv = 5, verbose = 3, random_state = 40,\n",
    "                            n_iter = 20)\n",
    "clf.fit(X_train, y_train, sample_weight=y_weights)\n",
    "best_clf_xgb = clf.best_estimator_\n",
    "best_params_xgb = clf.best_params_\n",
    "\n",
    "results_df.loc[len(results_df)] = ['xgb_weighted_gs'] + list(get_results(best_clf_xgb))\n",
    "print(best_clf_xgb)\n",
    "print(get_results(best_clf_xgb))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfc_baseline</td>\n",
       "      <td>99.92</td>\n",
       "      <td>83.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc_gs</td>\n",
       "      <td>97.16</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfc_pruned_0.01</td>\n",
       "      <td>97.66</td>\n",
       "      <td>84.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfc_pruned_0.02</td>\n",
       "      <td>97.58</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_baseline</td>\n",
       "      <td>99.83</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgb_weighted</td>\n",
       "      <td>95.75</td>\n",
       "      <td>82.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb_weighted_gs</td>\n",
       "      <td>90.24</td>\n",
       "      <td>81.67</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rfc_no_network</td>\n",
       "      <td>96.00</td>\n",
       "      <td>84.67</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb_no_network</td>\n",
       "      <td>97.91</td>\n",
       "      <td>81.33</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb_weighted_rs</td>\n",
       "      <td>88.41</td>\n",
       "      <td>81.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  train_acc  test_acc   auc  precision  recall\n",
       "0     rfc_baseline      99.92     83.67  0.78       0.76    0.64\n",
       "1           rfc_gs      97.16     84.33  0.79       0.77    0.66\n",
       "2  rfc_pruned_0.01      97.66     84.33  0.79       0.76    0.67\n",
       "3  rfc_pruned_0.02      97.58     82.00  0.75       0.74    0.59\n",
       "4     xgb_baseline      99.83     82.00  0.74       0.75    0.56\n",
       "5     xgb_weighted      95.75     82.33  0.80       0.67    0.76\n",
       "6  xgb_weighted_gs      90.24     81.67  0.82       0.64    0.84\n",
       "7   rfc_no_network      96.00     84.67  0.80       0.77    0.68\n",
       "8   xgb_no_network      97.91     81.33  0.73       0.75    0.54\n",
       "9  xgb_weighted_rs      88.41     81.00  0.82       0.63    0.83"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
