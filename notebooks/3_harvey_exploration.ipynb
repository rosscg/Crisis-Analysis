{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General data questions and exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO:\n",
    "# Try ML with and without network metrics\n",
    "# Test at different time periods\n",
    "# Test other event datasets\n",
    "# Check steps from book - change to best practice sklearn\n",
    "\n",
    "\n",
    "# What correlates with witness label\n",
    "# GPS accounts tend to be spam/businesses?\n",
    "# Compare GPS from stream / profile / hand coding\n",
    "# Explore age of account\n",
    "# Is detecting co-occuring tags viable?\n",
    "# What kind of data/user is likely to be deleted?\n",
    "# Is user name change / user deletion/protection a useful predictor\n",
    "# Compare Change in network, whether it's useful to collect.\n",
    "# Check gps count, location in profile\n",
    "# Check timezone distribution\n",
    "# 'Ordinary person' vs bot/celeb/business/news -- using source field, tweet rate, timezone\n",
    "\n",
    "# prop of gps -- users and tweets. Automated, instagram sourced?\n",
    "# prop of sources\n",
    "# prop of media/urls\n",
    "# users with location on profile? Some set 'in solidarity'?\n",
    "# Cycadian posting rythym - can identify real people vs bots?\n",
    "# location via friend network?\n",
    "# language\n",
    "\n",
    "# \\item Tweets which were automatically generated from Instagram posts were much more likely to include GPS coordinates, and as media, more likely to represent a ground truth. Therefore this content may be worth focusing on.\n",
    "# \\item Aid requests were very rare. Those that were identified were often reposts rather than originals, and are often referring to the same original message which begins to trend.\n",
    "# \\item Info for affected class should differentiate between immediate and non-immediate content. E.g. a call to mobilise a clean-up or rescue crew vs. a link to an insurance claim form.\n",
    "# \\item For `unrelated' messages, those which matched the keyword stream were highly represented by automated messages coming from a particular set of sources which presumably uses trending tags to gain exposure. This is easy to pre-filter.\n",
    "# \\item Geographically-tagged Tweets are predominantly either: Instagram cross-posts, or automatically generated job listings from a small set of sources (and therefore easy to pre-filter).\n",
    "        \n",
    "\n",
    "# Sum of network edge reciprocity\n",
    "# k-cohesiveness -- Structural cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 46)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initialisation ###\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "\n",
    "EVENT_NAME = Event.objects.all()[0].name.replace(' ', '')\n",
    "DIR = './data/harvey_user_location/'\n",
    "DF_FILENAME = 'df_users.csv'\n",
    "\n",
    "# Confirm correct database is set in Django settings.py\n",
    "if 'Harvey' not in EVENT_NAME:\n",
    "    raise Exception('Event name mismatch -- check database set in Django')\n",
    "\n",
    "# Open original Dataframe\n",
    "users_df = pd.read_csv(DIR + DF_FILENAME, index_col=0)\n",
    "users_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, given we are treating the data as binary-coded, we need to be aware of the third code of 'unsure'. In most comparisons, these will automatically be considered as the opposite code from what is being tested. We can instead manually set it to either value, or remove it entirely, given that it comprises a small proportion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 cases coded as 'unsure'\n"
     ]
    }
   ],
   "source": [
    "unsure_code = (users_df.is_coded_as_witness == 0) & (users_df.is_coded_as_non_witness == 0)\n",
    "print(sum(unsure_code), 'cases coded as \\'unsure\\'')\n",
    "\n",
    "# Remove 'unsure' rows from data:\n",
    "#users_df = users_df.loc[unsure_code==False]\n",
    "\n",
    "# Assign 'unsure' rows to positive coded case:\n",
    "#users_df.is_coded_as_witness = (users_df.is_coded_as_non_witness == False).astype(int)\n",
    "\n",
    "# Assign 'unsure' rows to negative coded case:\n",
    "#users_df.is_coded_as_non_witness = (users_df.is_coded_as_witness == False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Metadata and Manual Coding\n",
    "Manual coding of users targetted the perceived locality of the user to the event. We can compare the geographic metadata provided by Twitter to these codes to determine their usefulness as a predictor for this value.\n",
    "\n",
    "### Profile Location Field\n",
    "The first value to check is the location of a user as set in their profile. This is a user-set string. In an earlier notebook, this string was geocoded using Google Maps api and evaluated for whether it fell within the bounding box defined for this event. We can therefore check whether this test correlates with the coded value.\n",
    "\n",
    "First, we check what proportion of users provide a value in the field. We can then generate a confusion matrix showing the agreement between the profile locality where provided, and the coded value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users:  31932\n",
      "Total users with location filled:  25619\n",
      "Proportion: 80.23%\n",
      "\n",
      "Proportion of coded users with location filled: 79.4%\n",
      "\n",
      "Proportion of coded users with parseable location filled: 77.47%\n"
     ]
    }
   ],
   "source": [
    "us = User.objects.filter(user_class__gt=0)\n",
    "#us = us.filter(coding_for_user__coding_id=1, coding_for_user__data_code__data_code_id__gt=0)\n",
    "tot = us.count()\n",
    "tot_loc = us.count() - (us.filter(location=\"\") | us.filter(location__isnull=True)).count()\n",
    "\n",
    "print('Total users: ', tot)\n",
    "print('Total users with location filled: ', tot_loc)\n",
    "print('Proportion: {:.4}%'.format((tot_loc/tot)*100))\n",
    "\n",
    "print('\\nProportion of coded users with location filled: {:.4}%'.format((sum(users_df.location.notna())/users_df.shape[0])*100))\n",
    "\n",
    "print('\\nProportion of coded users with parseable location filled: {:.4}%'.format(\n",
    "    (users_df.loc[(users_df.is_non_local_profile_location + users_df.is_local_profile_location) > 0].shape[0]/users_df.shape[0])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397 of 1500 (26.47%) users were classified as having a local profile\n",
      "386 of 1500 (25.73%) users were coded as a witness\n",
      "258 of 397 (64.99%) users with local profile locations were coded as witness\n",
      "258 of 386 (66.84%) witness codes had a local profile\n"
     ]
    }
   ],
   "source": [
    "vals = users_df.loc[users_df[\"is_local_profile_location\"] == 1][\"is_coded_as_witness\"].value_counts()\n",
    "vals2 = users_df.loc[users_df[\"is_coded_as_witness\"] == 1][\"is_local_profile_location\"].value_counts()\n",
    "\n",
    "print('{} of {} ({:.4}%) users were classified as having a local profile'.format(sum(vals), len(users_df), sum(vals)/len(users_df)*100))\n",
    "print('{} of {} ({:.4}%) users were coded as a witness'.format(sum(vals2), len(users_df), sum(vals2)/len(users_df)*100))\n",
    "print('{} of {} ({:.4}%) users with local profile locations were coded as witness'.format(vals[1], sum(vals), vals[1]/sum(vals)*100))\n",
    "print('{} of {} ({:.4}%) witness codes had a local profile'.format(vals2[1], sum(vals2), vals2[1]/sum(vals2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def confusion_matrix(df: pd.DataFrame, col1: str, col2: str):\n",
    "    \"\"\"\n",
    "    Given a dataframe with at least\n",
    "    two categorical columns, create a \n",
    "    confusion matrix of the count of the columns\n",
    "    cross-counts\n",
    "    \"\"\"\n",
    "    return (\n",
    "            df\n",
    "            .groupby([col1, col2])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            )\n",
    "\n",
    "\n",
    "def confusion_matrix_from_series(s1, s2):\n",
    "    \"\"\"\n",
    "    Returns confusion matrix for two binary\n",
    "    series\n",
    "    \"\"\"\n",
    "    df = pd.concat([s1, s2], axis=1)\n",
    "    return confusion_matrix(df, s1.name, s2.name)\n",
    "\n",
    "\n",
    "def calc_agreement_coefs(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculates Cohen's Kappa and\n",
    "    Krippendorff's Alpha for a\n",
    "    given confusion matrix.\n",
    "    \"\"\"\n",
    "    arr = df.to_numpy()\n",
    "    n = arr.sum()\n",
    "    p_o = 0\n",
    "    for i in range(len(arr)):\n",
    "        p_o += arr[i][i]/n\n",
    "    p_e = 0\n",
    "    for i in range(len(arr)):\n",
    "        p_e += (arr.sum(axis=1)[i] *\n",
    "                arr.sum(axis=0)[i]) / (n*n)\n",
    "    kappa = (p_o-p_e)/(1-p_e)\n",
    "    \n",
    "    coin_arr = np.transpose(arr) + arr\n",
    "    exp_distribution = [sum(x) for x in coin_arr]\n",
    "    p_e_krippendorf = sum([a * (a-1) for a in exp_distribution])/(2*n*((2*n)-1))\n",
    "    alpha = (p_o - p_e_krippendorf) / (1-p_e_krippendorf)\n",
    "    \n",
    "    return p_o, kappa, alpha\n",
    "\n",
    "\n",
    "def calc_agreement_metrics(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculates various agreement metrics\n",
    "    for a given binary confusion matrix.\n",
    "    \n",
    "    Assumes true condition as ROW heading and\n",
    "    ascending integer labels.\n",
    "    \"\"\"\n",
    "    arr = df.to_numpy()\n",
    "    if len(arr) != 2:\n",
    "        return null\n",
    "    results = {}\n",
    "    results['Prevalence'] = arr.sum(axis=0)[1]/arr.sum()\n",
    "    results['Accuracy'] = (arr[0][0] + arr[1][1])/arr.sum()\n",
    "    results['Prec'] = arr[1][1]/arr.sum(axis=1)[1]\n",
    "    results['Recall'] = arr[1][1]/arr.sum(axis=0)[1]\n",
    "    results['f1Score'] = (2 * results['Prec'] * results['Recall'])/(results['Prec']+results['Recall'])\n",
    "    results['Specificity'] = arr[0][0]/arr.sum(axis=0)[0]\n",
    "    results['FalseNegRate'] = arr[0][1]/arr.sum(axis=0)[1]\n",
    "    p_o, kappa, alpha = calc_agreement_coefs(df)\n",
    "    results['Cohen\\'s Kappa'] = kappa\n",
    "    results['Krippendorff\\'s Alpha'] = alpha\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 rows with no parseable profile location value excluded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_coded_as_witness</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_local_profile_location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>695</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_coded_as_witness          0    1\n",
       "is_local_profile_location          \n",
       "0                          695   70\n",
       "1                          139  258"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We exclude rows where either no profile location field was provided, \n",
    "# or the location was not parsed by the API:\n",
    "loc_df = users_df.loc[(users_df.is_non_local_profile_location + users_df.is_local_profile_location) > 0]\n",
    "print(users_df.shape[0] - loc_df.shape[0], 'rows with no parseable profile location value excluded')\n",
    "\n",
    "conf = confusion_matrix(loc_df, 'is_local_profile_location', 'is_coded_as_witness')\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_prof_nona</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.282272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.820138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.649874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.786585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1Score</th>\n",
       "      <td>0.711724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FalseNegRate</th>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <td>0.582731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krippendorff's Alpha</th>\n",
       "      <td>0.581198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loc_prof_nona\n",
       "Prevalence                 0.282272\n",
       "Accuracy                   0.820138\n",
       "Prec                       0.649874\n",
       "Recall                     0.786585\n",
       "f1Score                    0.711724\n",
       "Specificity                0.833333\n",
       "FalseNegRate               0.213415\n",
       "Cohen's Kappa              0.582731\n",
       "Krippendorff's Alpha       0.581198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = calc_agreement_metrics(conf)\n",
    "res_df = pd.DataFrame.from_dict(results, orient='index', columns=['loc_prof_nona'])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As excluding the ~20% of values with no parseable location field provided is not an option in practice, we must decide to either discard them (i.e. by default classify as non-local) or include them (default classify as local). The first option will inevitable discard true positive cases, thus reducing recall, whereas the latter will introduce false positives, reducing precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc_prof_notna\n",
      "is_coded_as_witness          0    1\n",
      "is_local_profile_location          \n",
      "0                          975  128\n",
      "1                          139  258\n",
      "\n",
      "loc_prof_orna\n",
      "is_coded_as_witness                0    1\n",
      "is_local_profile_location_or_na          \n",
      "0                                714   80\n",
      "1                                400  306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_prof_nona</th>\n",
       "      <th>loc_prof_notna</th>\n",
       "      <th>loc_prof_orna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.282272</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.820138</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.433428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.668394</td>\n",
       "      <td>0.792746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1Score</th>\n",
       "      <td>0.711724</td>\n",
       "      <td>0.659004</td>\n",
       "      <td>0.560440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.875224</td>\n",
       "      <td>0.640934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FalseNegRate</th>\n",
       "      <td>0.213415</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>0.207254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <td>0.582731</td>\n",
       "      <td>0.538603</td>\n",
       "      <td>0.341243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krippendorff's Alpha</th>\n",
       "      <td>0.581198</td>\n",
       "      <td>0.538725</td>\n",
       "      <td>0.309098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loc_prof_nona  loc_prof_notna  loc_prof_orna\n",
       "Prevalence                 0.282272        0.257333       0.257333\n",
       "Accuracy                   0.820138        0.822000       0.680000\n",
       "Prec                       0.649874        0.649874       0.433428\n",
       "Recall                     0.786585        0.668394       0.792746\n",
       "f1Score                    0.711724        0.659004       0.560440\n",
       "Specificity                0.833333        0.875224       0.640934\n",
       "FalseNegRate               0.213415        0.331606       0.207254\n",
       "Cohen's Kappa              0.582731        0.538603       0.341243\n",
       "Krippendorff's Alpha       0.581198        0.538725       0.309098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix(users_df, 'is_local_profile_location', 'is_coded_as_witness')\n",
    "print('loc_prof_notna')\n",
    "print(conf)\n",
    "res_df['loc_prof_notna'] = calc_agreement_metrics(conf).values()\n",
    "\n",
    "\n",
    "conf = confusion_matrix_from_series(\n",
    "    pd.Series(\n",
    "                (users_df.is_local_profile_location) | (users_df.location.isna()).astype(int), \n",
    "                name='is_local_profile_location_or_na'\n",
    "             ),\n",
    "    users_df.is_coded_as_witness)\n",
    "print('\\nloc_prof_orna')\n",
    "print(conf)\n",
    "res_df['loc_prof_orna'] = calc_agreement_metrics(conf).values()\n",
    "\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are as expected. Excluding empty fields gives a precision/recall of 0.650/0.668 whereas exluding them gives 0.433/0.793. The exclusion strategy provides the highest f1 score, however given the purpose of the algorithm must be considered when choosing how to weight precision and recall metrics. For example, given the algorithm is designed to curate the feed for human consumption, a high precision is only necessary if the rate of positive cases exceeds the humans' ability to parse the incoming stream. Where the rate is low, sacrificing precision is acceptable to present the human user with more cases which they can then manually filter. \n",
    "\n",
    "This concept will be explored in more depth later in the project. For now, it is sufficient to note the values as a baseline model.\n",
    "\n",
    "### Tweet Stream Coordinates\n",
    "When posting a Tweet, a user may attach geographic coordinates. The location of the device is provided by the hardware and automatically included with the Tweet (thus the user does not influence the input). A Tweet may also include, instead of specific coordinates, a 'Place' object -- a geographic region (defined by Twitter) which typically describes a location such as a city, state or other similarly-sized region.\n",
    "\n",
    "To geolocate a user, we can therefore investigate their Twitter feed for any Tweets containing this geographic data and compare these to the bounding box of the observed event. The derived field therefore represents whether *any* of a user's Tweets were identified as 'local' during the event.\n",
    "\n",
    "For this dataset, the Twitter feed for each observed user spanning the duration of the collection period was collected at the end of the collection period. The feed is therefore made up of Tweets detected during the collection period, and any other Tweets the user made during the period, before or after the detected Tweet, provided they existed at the end of the collection period.\n",
    "\n",
    "Further work on this area should consider the following:\n",
    "* Where a local Tweet has been detected, check the proportion of other Tweets containing geographic data.\n",
    "* Where other geo-Tweets exist, check whether they are from the same point or move around -- consider recoding for where a user Tweets from within *and* without the bounding box.\n",
    "* Where all geo-Tweets come from the same point, it is likely that location has been manually set (i.e. it is a storefront/business account). This may be verifiable by checking the Tweet source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467 of 1500 (31.1%) users have Tweet from locality\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_coded_as_witness</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_tweet_from_locality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_coded_as_witness        0    1\n",
       "has_tweet_from_locality          \n",
       "0                        898  135\n",
       "1                        216  251"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('{} of {} ({:.3}%) users have Tweet from locality'.format(\n",
    "    sum(users_df.has_tweet_from_locality), users_df.shape[0], 100*sum(users_df.has_tweet_from_locality)/users_df.shape[0]))\n",
    "\n",
    "conf = confusion_matrix(users_df, 'has_tweet_from_locality', 'is_coded_as_witness')\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_prof_nona</th>\n",
       "      <th>loc_prof_notna</th>\n",
       "      <th>loc_prof_orna</th>\n",
       "      <th>local_tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.282272</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.820138</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.433428</td>\n",
       "      <td>0.537473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.668394</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.650259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1Score</th>\n",
       "      <td>0.711724</td>\n",
       "      <td>0.659004</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.588511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.875224</td>\n",
       "      <td>0.640934</td>\n",
       "      <td>0.806104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FalseNegRate</th>\n",
       "      <td>0.213415</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>0.207254</td>\n",
       "      <td>0.349741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <td>0.582731</td>\n",
       "      <td>0.538603</td>\n",
       "      <td>0.341243</td>\n",
       "      <td>0.427080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krippendorff's Alpha</th>\n",
       "      <td>0.581198</td>\n",
       "      <td>0.538725</td>\n",
       "      <td>0.309098</td>\n",
       "      <td>0.425219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loc_prof_nona  loc_prof_notna  loc_prof_orna  local_tw\n",
       "Prevalence                 0.282272        0.257333       0.257333  0.257333\n",
       "Accuracy                   0.820138        0.822000       0.680000  0.766000\n",
       "Prec                       0.649874        0.649874       0.433428  0.537473\n",
       "Recall                     0.786585        0.668394       0.792746  0.650259\n",
       "f1Score                    0.711724        0.659004       0.560440  0.588511\n",
       "Specificity                0.833333        0.875224       0.640934  0.806104\n",
       "FalseNegRate               0.213415        0.331606       0.207254  0.349741\n",
       "Cohen's Kappa              0.582731        0.538603       0.341243  0.427080\n",
       "Krippendorff's Alpha       0.581198        0.538725       0.309098  0.425219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df['local_tw'] = calc_agreement_metrics(conf).values()\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the recall of this metric is close (slightly less than) `loc_prof_notna`, there is no increase in precision and thus it is an inferior metric to predict the true condition.\n",
    "\n",
    "We can however check whether the metric is capturing a different proportion of local users, and therefore improve upon the existing measures through combination. Using an OR condition will increase recall at the cost of precision; using an AND condition will increase precision at the cost of recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_prof_nona</th>\n",
       "      <th>loc_prof_notna</th>\n",
       "      <th>loc_prof_orna</th>\n",
       "      <th>local_tw</th>\n",
       "      <th>loc_prof_notna_or_loc_tw</th>\n",
       "      <th>loc_prof_notna_and_loc_tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Prevalence</th>\n",
       "      <td>0.282272</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "      <td>0.257333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.820138</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.649874</td>\n",
       "      <td>0.433428</td>\n",
       "      <td>0.537473</td>\n",
       "      <td>0.558219</td>\n",
       "      <td>0.653571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.668394</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.650259</td>\n",
       "      <td>0.844560</td>\n",
       "      <td>0.474093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1Score</th>\n",
       "      <td>0.711724</td>\n",
       "      <td>0.659004</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.588511</td>\n",
       "      <td>0.672165</td>\n",
       "      <td>0.549550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.875224</td>\n",
       "      <td>0.640934</td>\n",
       "      <td>0.806104</td>\n",
       "      <td>0.768402</td>\n",
       "      <td>0.912926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FalseNegRate</th>\n",
       "      <td>0.213415</td>\n",
       "      <td>0.331606</td>\n",
       "      <td>0.207254</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>0.155440</td>\n",
       "      <td>0.525907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <td>0.582731</td>\n",
       "      <td>0.538603</td>\n",
       "      <td>0.341243</td>\n",
       "      <td>0.427080</td>\n",
       "      <td>0.524972</td>\n",
       "      <td>0.425170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Krippendorff's Alpha</th>\n",
       "      <td>0.581198</td>\n",
       "      <td>0.538725</td>\n",
       "      <td>0.309098</td>\n",
       "      <td>0.425219</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>0.421208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      loc_prof_nona  loc_prof_notna  loc_prof_orna  local_tw  \\\n",
       "Prevalence                 0.282272        0.257333       0.257333  0.257333   \n",
       "Accuracy                   0.820138        0.822000       0.680000  0.766000   \n",
       "Prec                       0.649874        0.649874       0.433428  0.537473   \n",
       "Recall                     0.786585        0.668394       0.792746  0.650259   \n",
       "f1Score                    0.711724        0.659004       0.560440  0.588511   \n",
       "Specificity                0.833333        0.875224       0.640934  0.806104   \n",
       "FalseNegRate               0.213415        0.331606       0.207254  0.349741   \n",
       "Cohen's Kappa              0.582731        0.538603       0.341243  0.427080   \n",
       "Krippendorff's Alpha       0.581198        0.538725       0.309098  0.425219   \n",
       "\n",
       "                      loc_prof_notna_or_loc_tw  loc_prof_notna_and_loc_tw  \n",
       "Prevalence                            0.257333                   0.257333  \n",
       "Accuracy                              0.788000                   0.800000  \n",
       "Prec                                  0.558219                   0.653571  \n",
       "Recall                                0.844560                   0.474093  \n",
       "f1Score                               0.672165                   0.549550  \n",
       "Specificity                           0.768402                   0.912926  \n",
       "FalseNegRate                          0.155440                   0.525907  \n",
       "Cohen's Kappa                         0.524972                   0.425170  \n",
       "Krippendorff's Alpha                  0.515676                   0.421208  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = confusion_matrix_from_series(\n",
    "    pd.Series(\n",
    "        ((users_df.is_local_profile_location) | (users_df.has_tweet_from_locality)),\n",
    "        name = 'is_local_profile_location_or_local_tw'\n",
    "        ),\n",
    "    users_df.is_coded_as_witness)\n",
    "# conf\n",
    "\n",
    "res_df['loc_prof_notna_or_loc_tw'] = calc_agreement_metrics(conf).values()\n",
    "\n",
    "conf = confusion_matrix_from_series(\n",
    "    pd.Series(\n",
    "        ((users_df.is_local_profile_location) & (users_df.has_tweet_from_locality)),\n",
    "        name = 'is_local_profile_location_and_local_tw'\n",
    "        ),\n",
    "    users_df.is_coded_as_witness)\n",
    "# conf\n",
    "\n",
    "res_df['loc_prof_notna_and_loc_tw'] = calc_agreement_metrics(conf).values()\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table above, the highest precision is observed from the classifier `loc_prof_notna_and_loc_tw` at 0.656 (though at a great cost to recall). It should be noted that selecting for cases which satisfy both conditions may inadvertently select for a particular (as-yet unidentified) sub-category of user type and exclude other categories of value. The highest fscore comes from `loc_prof_notna_or_loc_tw` at 0.672, which has a precision of 0.558.\n",
    "\n",
    "While the low scores of these metrics show that they cannot provide meaningful proxies for the manually assigned code, as classifiers they provide a suitable baseline from which to measure more sophisticated models.\n",
    "\n",
    "## Tweet Source\n",
    "An important distinguishing metadatum of a Tweet is the 'source' field, which represents the platform from which the Tweet was published. When creating a third-party application which can interact with the Twitter API, a developer must provide a descriptor string which populates this field. Because many third-party applications are designed for specific use-cases, this field provides useful information which characterises the motivations for conditions under which the Tweet was created. For example, the source `TweetMyJOBS` refers to a recruitment platform and thus is attached to Tweets advertising job listings.\n",
    "\n",
    "We can look at the list of most common source from within the entire dataset and compare this to the sources detected during the collection period. (Note that the complete dataset will still contain a selection bias and does not necessarily characterise regular Twitter use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets: 1727438 \n",
      "\n",
      "31.0% Twitter for iPhone: 535558\n",
      "20.2% Twitter for Android: 348743\n",
      "19.9% Twitter Web Client: 342908\n",
      "5.5% IFTTT: 95360\n",
      "2.7% Twitter for iPad: 45830\n",
      "2.5% Twitter Lite: 43808\n",
      "2.4% Instagram: 42045\n",
      "1.9% TweetDeck: 32273\n",
      "1.7% Facebook: 28536\n",
      "1.6% Hootsuite: 27105\n",
      "1.1% Paper.li: 18490\n",
      "1.1% Botize: 18260\n",
      "0.6% TweetMyJOBS: 10867\n",
      "0.4% SafeTweet by TweetMyJOBS: 7650\n",
      "0.4% Buffer: 6819\n",
      "0.4% Google: 6469\n",
      "0.3% WordPress.com: 5461\n",
      "0.3% SocialNewsDesk: 4472\n",
      "0.2% Mobile Web (M2): 3407\n",
      "0.2% dlvr.it: 2972\n"
     ]
    }
   ],
   "source": [
    "# View most common sources from entire dataset:\n",
    "from django.db.models import Count\n",
    "from streamcollect.models import Tweet\n",
    "\n",
    "ts = Tweet.objects.all()\n",
    "print('Total Tweets:', ts.count(), '\\n')\n",
    "\n",
    "fieldname = 'source'\n",
    "counts = ts.values(fieldname).order_by(fieldname).annotate(count=Count(fieldname)).order_by('-count')\n",
    "\n",
    "for x in counts[:20]:\n",
    "    if x['count'] > 10:\n",
    "        print('{:.1f}% {}: {}'.format((x['count']/ts.count()*100), x['source'], x['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.5% of total from first party clients: 1355569\n"
     ]
    }
   ],
   "source": [
    "# Count proportion of Tweets from first-party applications\n",
    "first_party_sources = [\n",
    "    'Twitter for iPhone',\n",
    "    'Twitter for Android',\n",
    "    'Twitter Web Client',\n",
    "    'Twitter for iPad',\n",
    "    'Twitter Lite',\n",
    "    'TweetDeck',\n",
    "    'Twitter for Windows',\n",
    "    'Twitter for Mac',\n",
    "    'Twitter for Windows Phone',\n",
    "    'Twitter for BlackBerry',\n",
    "    'Twitter for Android Tablets',\n",
    "    'Twitter MMS'\n",
    "    ]\n",
    "fp_count = 0\n",
    "for x in counts:\n",
    "    if any([y in x['source'] for y in first_party_sources ]):\n",
    "        fp_count += x['count']\n",
    "print('{:.1f}% of total from first party clients: {}'.format((fp_count/ts.count()*100), fp_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total for source = 1 (keyword stream): 31303 \n",
      "\n",
      "28.6% Twitter for iPhone: 8959\n",
      "25.5% Twitter Web Client: 7989\n",
      "16.2% Twitter for Android: 5086\n",
      "5.6% Paper.li: 1754\n",
      "3.1% Hootsuite: 981\n",
      "2.3% Instagram: 733\n",
      "2.3% IFTTT: 720\n",
      "2.2% Facebook: 682\n",
      "2.1% TweetDeck: 658\n",
      "2.1% Twitter for iPad: 652\n",
      "1.9% Twitter Lite: 595\n",
      "0.9% Buffer: 274\n",
      "0.4% SocialNewsDesk: 138\n",
      "0.4% Sprout Social: 126\n",
      "0.3% Error-log: 105\n",
      "0.3% Botize: 93\n",
      "0.2% Periscope: 70\n",
      "0.2% VoiceStorm: 69\n",
      "0.2% Google: 62\n",
      "0.2% Twitter for Windows: 62\n"
     ]
    }
   ],
   "source": [
    "ts = Tweet.objects.filter(data_source=1)\n",
    "print('Total for data_source=1 (keyword stream):', ts.count(), '\\n')\n",
    "\n",
    "fieldname = 'source'\n",
    "counts = ts.values(fieldname).order_by(fieldname).annotate(count=Count(fieldname)).order_by('-count')\n",
    "\n",
    "for x in counts[:20]:\n",
    "    if x['count'] > 10:\n",
    "        print('{:.1f}% {}: {}'.format((x['count']/ts.count()*100), x['source'], x['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total for data_source=3 (geo stream) 15630 \n",
      "\n",
      "76.6% Instagram: 11971\n",
      "6.5% TweetMyJOBS: 1023\n",
      "5.9% SafeTweet by TweetMyJOBS: 928\n",
      "3.9% BubbleLife: 602\n",
      "1.7% Foursquare: 268\n",
      "1.5% Untappd: 234\n",
      "0.8% Twitter for Android: 131\n",
      "0.7% Hootsuite: 106\n",
      "0.6% Twitter for iPhone: 88\n",
      "0.4% circlepix: 55\n",
      "0.3% TownTweet: 52\n",
      "0.2% iOS: 28\n",
      "0.2% Twitter for Android Tablets: 27\n",
      "0.2% Crowdfire - Go Big: 25\n",
      "0.1% Squarespace: 18\n",
      "0.1% Twitter for Windows Phone: 15\n",
      "0.1% Tweetbot for iΟS: 12\n"
     ]
    }
   ],
   "source": [
    "ts = Tweet.objects.filter(data_source__gte=1, coordinates_lat__isnull=False)\n",
    "print('Total for data_source=3 (geo stream)', ts.count(), '\\n')\n",
    "\n",
    "fieldname = 'source'\n",
    "counts = ts.values(fieldname).order_by(fieldname).annotate(count=Count(fieldname)).order_by('-count')\n",
    "\n",
    "for x in counts[:20]:\n",
    "    if x['count'] > 10:\n",
    "        print('{:.1f}% {}: {}'.format((x['count']/ts.count()*100), x['source'], x['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7% of geo stream from first party clients: 262\n"
     ]
    }
   ],
   "source": [
    "fp_count = 0\n",
    "for x in counts:\n",
    "    if any([y in x['source'] for y in first_party_sources ]):\n",
    "        fp_count += x['count']\n",
    "print('{:.1f}% of geo stream from first party clients: {}'.format((fp_count/ts.count()*100), fp_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the entire dataset of 1,727,438 Tweets, those published by first-party Twitter clients comprised 78.5% (1,355,569). In contrast, of the subset of 15,630 Tweets collected based on their location within the event's bounding box, only 1.7% (262) were published from first-party apps. Tweets crossposted by Instagram comprised 76.6% of the geotagged Tweets. The high incidence of Instagram posts in the geographic stream therefore suggest that Instagram posts are much more likely than Tweets to include geographic data, which is preserved during the crossposting process. We can check the rate across the entire dataset excluding the geo stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Instagram Tweets (excl. geo stream): 30123\n",
      "Total geotagged Instagram Tweets: 9818, 32.6%\n"
     ]
    }
   ],
   "source": [
    "ts = Tweet.objects.filter(source='Instagram', data_source__lt=3)\n",
    "print('Total Instagram Tweets (excl. geo stream):', ts.count())\n",
    "ts_geo = ts.filter(coordinates_lat__isnull=False)\n",
    "print('Total geotagged Instagram Tweets: {}, {:.1f}%'.format(ts_geo.count(), ts_geo.count()/ts.count()*100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Instagram posts are traditionally based upon the publication of a recently-taken photo, the high incidence of geotagging makes this class of message highly useful in supporting the development of situational awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
