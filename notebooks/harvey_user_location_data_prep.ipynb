{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Location Classification in Hurricane Harvey\n",
    "The goal of this analysis is to evaluate methods by which users Tweeting about Hurricane Harvey may be classified as in the area or otherwise.\n",
    "\n",
    "Data was collected with custom software which observed several Twitter streams and enhanced this information by querying the Twitter REST APIs for the network data (friends and followers) of each author. Stream volume which exceeded the capacity of the REST requests was discarded. \n",
    "* The keyword stream monitored the terms: [#harvey, #harveystorm, #hurricaneharvey, #corpuschristi]\n",
    "* The GPS stream used the bounding box: [-99.9590682, 26.5486063, -93.9790001, 30.3893434]\n",
    "* The collection period ran from 2017-08-26 01:32:18 until 2017-09-02 10:30:52 \n",
    "* 55,605 Tweets by 33,585 unique authors were recorded\n",
    "\n",
    "Data was coded using an interface built into the collection software by a primary coder. A secondary coder coded a sub-set of coded users for validation of the coding schema. User instances were coded by whether they 'appeared to be in the affected area'.\n",
    "\n",
    "\n",
    "# Data Cleaning & Enrichment\n",
    "First we get all the coding instances made by the primary and secondary coders, and check the total codings of each class. There may be multiple coding dimensions (sets of coding schema), in which case the code requires adjustment to constrain to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding Dimension:  Local\n",
      "Subject:  user\n",
      "Class Totals (primary / secondary): \n",
      "\t Unsure: \t31 \t/ 5\n",
      "\t Non-Witness: \t1083 \t/ 94\n",
      "\t Witness: \t386 \t/ 52\n",
      "1500 Accounts coded by primary coder, 151 by secondary coder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get coding instances of user objects:\n",
    "account_codings = (Coding.objects\n",
    "                    .filter(coding_id=1)\n",
    "                    .filter(user__isnull=False)\n",
    "                    .filter(data_code__data_code_id__gt=0)\n",
    "                  )\n",
    "account_codings_secondary = (Coding.objects\n",
    "                                 .filter(coding_id=2)\n",
    "                                 .filter(user__isnull=False)\n",
    "                            )\n",
    "\n",
    "# Check available coding schema:\n",
    "dimensions = DataCodeDimension.objects.all()[1:]\n",
    "for d in dimensions:\n",
    "    print('Coding Dimension: ', d.name)\n",
    "    print('Subject: ', d.coding_subject)\n",
    "    print('Class Totals (primary / secondary): ')\n",
    "    for code in d.datacode.all():\n",
    "        print(\"\\t {}: \\t{} \\t/ {}\"\n",
    "                .format(code.name, \n",
    "                    account_codings.filter(data_code__id=code.id).count(), \n",
    "                    account_codings_secondary.filter(data_code__id=code.id).count())\n",
    "             )\n",
    "print(\"{} Accounts coded by primary coder, {} by secondary coder.\".format(account_codings.count(), account_codings_secondary.count()))\n",
    "if len(dimensions) > 1: \n",
    "    print('\\tNote: Totals represent sum of all codes from {} dimensions.'.format(len(dimensions)))\n",
    "    print('WARNING: Code in cells below assume one dimension -- adjust to constrain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a dataframe of all users which have been coded by the primary coder to create the initial dataset. The subjects of the secondary coder are a subset of this set by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utc_offset                           920\n",
       "time_zone                            920\n",
       "undirected_eigenvector_centrality    890\n",
       "closeness_centrality                 890\n",
       "degree_centrality                    890\n",
       "eigenvector_centrality               890\n",
       "load_centrality                      890\n",
       "betweenness_centrality               890\n",
       "url                                  808\n",
       "old_screen_name                       98\n",
       "suspended                              0\n",
       "user_network_update_observed_at        0\n",
       "needs_phone_verification               0\n",
       "user_followers                         0\n",
       "katz_centrality                        0\n",
       "is_deleted_observed                    0\n",
       "is_deleted                             0\n",
       "user_followers_update                  0\n",
       "user_following                         0\n",
       "user_following_update                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all Users coded by primary coder:\n",
    "# (exclude data_code_id=0 as this is the temporary 'to be coded' class)\n",
    "users = User.objects.filter(coding_for_user__coding_id=1, \n",
    "                            coding_for_user__data_code__data_code_id__gt=0)\n",
    "users_df = pd.DataFrame(list(users.values()))\n",
    "\n",
    "# Check for missing values by column:\n",
    "users_df.count()[users_df.count() != account_codings.count()].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The centrality measures have a common value. As these values are only calculated for the largest connected component of the graph, this consistency makes sense.\n",
    "* The fields time_zone, utc_offset, old_screen_name, and url are nullable.\n",
    "* Twitter has deprecated the field 'needs_phone_verification', so no values were returned.\n",
    "* Various 0 value fields had been added to the database schema but were not implemented at the time of collection. These can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping empty columns\n",
    "empty_cols = users_df.columns[users_df.isnull().all()]\n",
    "users_df.drop(empty_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop columns which have a single value, as they provide no differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns:  protected\n",
      "Dropping columns:  ratio_media\n",
      "Dropping columns:  user_class\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with only one unique value:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 1:\n",
    "        print('Dropping columns: ', col)\n",
    "        users_df = users_df.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Location Data\n",
    "There are a number of options which can represent the ground truth location of the user.\n",
    "* Location listed on a user's profile\n",
    "* User Timezone (deprecated)\n",
    "* Manual Coding\n",
    "* Location data derived from Tweet stream\n",
    "    * GPS tagged Tweets\n",
    "    * Mention of location in Tweet body\n",
    "    \n",
    "### Parsing user-set location in profile field\n",
    "The location the user sets in their profile as a string is evaluated and a locality decision made. In this instance, a location is considered 'local' if its coordinates (supplied by the Google geolocation API or parsed directly from the location string) fall within the bounding box used for geographic Twitter data collection, or if it contains the string 'houston' or 'christi' (representing the town Corpus Christi). Both of these locations fall within the bounding box, and are used here as a time-saving operation.\n",
    "\n",
    "Note that as this field can be set manually, it is unverifiable and therefore not a perfect representation of location, even where it exists. Users may neglect to update their location after moving, and some observations were made of users setting their location to that of a disaster event as a 'show of solidarity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block supports manual coding of locations as local or non-local.\n",
    "## It has been superceded by the next block which uses the Googlemaps API\n",
    "\n",
    "#location_list = users_df.location.unique()\n",
    "#with open('data/harvey_user_location/location_list_all_profile_locations.txt', 'w') as f:\n",
    "#    for item in location_list:\n",
    "#        f.write(\"%s\\n\" % item)\n",
    "\n",
    "################\n",
    "## This list then manually sorted and non-local locations removed.\n",
    "## List of local locations then re-imported.\n",
    "## Note this list excludes any locations containing 'Christi' or 'Houston'\n",
    "## Note: if more users are coded, this list needs to be re-examined. Raise alert:\n",
    "#if users_df.shape[0] != 931:\n",
    "#    print('ALERT: New codings detected. Consider updating manual locality selection')\n",
    "################\n",
    "\n",
    "#with open('data/harvey_user_location/local_profile_locations_manual_check.txt', 'r') as f:\n",
    "#    local_locations_list = f.read().splitlines()\n",
    "    \n",
    "## Create column for users with local location listed in profile\n",
    "#users_df['local_profile_location_manual'] = \\\n",
    "#    (users_df.location.str.contains('houston|christi', case=False, regex=True) |\n",
    "#    users_df.location.isin(local_locations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_coordinates(string):\n",
    "    '''Parse a string for coordinates'''\n",
    "    reg = '[nsewNSEW]?\\s?-?\\d+[\\.째]\\s?\\d+째?\\s?[nsewNSEW]?'\n",
    "    result = re.findall(reg, string)\n",
    "    if len(result) == 2: # Coordinates detected\n",
    "        for i in range(len(result)):\n",
    "            # Replace middle degree symbol with decimal:\n",
    "            reg_middle_degree = '(\\d+)째\\s?(\\d+)'\n",
    "            result[i] = re.sub(reg_middle_degree, r'\\1.\\2', result[i])\n",
    "            # Remove trailing degree symbol, N and E marks:\n",
    "            reg_strip = '[째neNE\\s]'\n",
    "            result[i] = re.sub(reg_strip, '', result[i])\n",
    "            # Replace south/west with negative sign:\n",
    "            reg_replace_sw = '[swSW](\\d+\\.\\d+)|(\\d+\\.\\d+)[swSW]'\n",
    "            result[i] = re.sub(reg_replace_sw, r'-\\1\\2', result[i])\n",
    "            # Remove double negative (where string contained eg. '-99.10w')\n",
    "            result[i] = re.sub('--', '-', result[i])\n",
    "        return (float(result[0]), float(result[1]))\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import googlemaps\n",
    "\n",
    "def is_in_bounding_box(coords, boxes):\n",
    "    '''\n",
    "    Check whether coordinates fall within defined bounding box:\n",
    "    Boxes are defined as their NW and SE points.\n",
    "    '''\n",
    "    for box in boxes:\n",
    "        if coords[0] < box[0][0] and coords[0] > box[1][0]:\n",
    "            if coords[1] > box[0][1] and coords[1] < box[1][1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_local(location, boxes, known_localities=[]):\n",
    "    '''\n",
    "    Check whether a location string falls within a set of \n",
    "    bounding boxes using Googlemaps API.\n",
    "    '''\n",
    "    if not location:\n",
    "        return False\n",
    "    # Check known localities first to save on API requests:\n",
    "    for x in known_localities:\n",
    "        if x in location:\n",
    "            return True\n",
    "    # Try and parse coordinates from string rather than API query:\n",
    "    coords = parse_coordinates(location)\n",
    "    # Get coords from API:\n",
    "    if not coords:\n",
    "        # Get API key from file:\n",
    "        with open(\"auth.yml\", 'r') as ymlfile:\n",
    "            auth = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "        gmaps = googlemaps.Client(key=auth['apikeys']['googlemaps'])\n",
    "        #########################################################\n",
    "        ####### OVERRIDE API OBJECT TO PREVENT API CALLS: #######\n",
    "        geocode_result = gmaps.geocode(location)\n",
    "        #geocode_result = False\n",
    "        #print('WARNING -- API DISABLED')\n",
    "        #########################################################\n",
    "        #########################################################\n",
    "        if geocode_result:\n",
    "            lat = geocode_result[0]['geometry']['location']['lat']\n",
    "            lon = geocode_result[0]['geometry']['location']['lng']\n",
    "            coords = (lat, lon)\n",
    "    if coords:\n",
    "        return(is_in_bounding_box(coords, boxes))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding boxes used for Hurricane Harvey dataset:\n",
    "boxes = [[(29.1197,-99.9590682),(26.5486063,-97.5021)],\n",
    "        [(30.3893434,-97.5021),(26.5486063,-93.9790001)]]\n",
    "# Don't need to look these up (save on API requests)\n",
    "known_localities = ['houston', 'christi']\n",
    "\n",
    "# Get list of locations in profiles:\n",
    "users_df[\"location\"] = users_df[\"location\"].str.lower().str.strip()\n",
    "location_list = users_df.location.dropna().unique()\n",
    "\n",
    "# Create sublist of local/non-local locations (non-local only for manual verification)\n",
    "print(\"Running is_local() for {} strings...\".format(len(location_list)))\n",
    "local_location_list = [loc for loc in location_list if is_local(loc, boxes, known_localities)]\n",
    "non_local_location_list = [loc for loc in location_list if loc not in local_location_list]\n",
    "\n",
    "# Create column for users with local location listed in profile\n",
    "users_df['local_profile_location'] = users_df.location.str.lower().isin(local_location_list)\n",
    "\n",
    "# Write lists to file to save calling API on kernel restart:\n",
    "with open('data/harvey_user_location/location_list_from_api_local.txt', 'w') as f:\n",
    "    for item in local_location_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('data/harvey_user_location/location_list_from_api_non_local.txt', 'w') as f:\n",
    "    for item in non_local_location_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cached locations instead of querying API:\n",
    "#users_df[\"location\"] = users_df[\"location\"].str.lower().str.strip()\n",
    "#local_location_list_cached = []\n",
    "#with open('data/harvey_user_location/location_list_from_api_local.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        local_location_list_cached.append(line.rstrip('\\n'))\n",
    "#users_df['local_profile_location'] = users_df.location.str.lower().isin(local_location_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timezone Field\n",
    "Timezone data provided by Twitter when capturing the user objects is less specific than other methods, but may be useful as a supplementary source.\n",
    "As this data field has been deprecated by Twitter, it will not be available in new data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central Time (US & Canada)     341\n",
      "Eastern Time (US & Canada)     197\n",
      "Pacific Time (US & Canada)     177\n",
      "Mountain Time (US & Canada)     37\n",
      "Quito                           25\n",
      "Name: time_zone, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View most prevalent time zones:\n",
    "print(users_df['time_zone'].value_counts().head())\n",
    "\n",
    "# Create column for profiles in relevant time zone (chosen manually):\n",
    "relevant_timezone = 'Central Time (US & Canada)'\n",
    "users_df['local_timezone'] = users_df.time_zone == relevant_timezone\n",
    "users_df = users_df.drop(['time_zone', 'utc_offset'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Coding\n",
    "Accounts were manually coded as 'local' or 'non-local'.\n",
    "\n",
    "Coders were shown the user account details as well as the Twitter stream of the user. The coders were instructed to determine whether the user account was in an area affected by the hurricane at any point during the data collection period. Therefore, the term 'local' may be misleading to the reader, as the definition given to the coders will include anyone visiting the area as, for example, a responder or aid worker. This larger set of 'on the ground' users is a more useful target for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent manual coding:\n",
    "users_df['coded_as'] = \\\n",
    "    users_df['screen_name'].apply(lambda x: account_codings.get(user__screen_name = x).data_code.name)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "users_df['coded_as_witness'] = users_df['coded_as'] == 'Witness'\n",
    "users_df['coded_as_non_witness'] = users_df['coded_as'] == 'Non-Witness'\n",
    "\n",
    "# Remove original column:\n",
    "users_df = users_df.drop(['coded_as'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Unsure' code is represented as `False` values in both the `coded_as_witness` and `coded_as_non_witness` columns. If the 'Unsure' rows are removed, we can also remove the `coded_as_non_witness` column (which is now represented as `False` in the `coded_as_witness` column).\n",
    "<br /><br />\n",
    "\n",
    "### GPS from Tweet stream\n",
    "While the Tweets detected by the system may not contain GPS data, the author may have made other GPS-enabled Tweets during the collection period from which we can infer location. We create a column representing whether the user made any geolocated Tweets within the bounding box during the collection period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "User 10 of 1500: ChristiWilliams\n",
      "User 20 of 1500: OliviaFett\n",
      "User 30 of 1500: UHhousing\n",
      "User 40 of 1500: SmallTownDicks\n",
      "User 50 of 1500: WmBrockschmidt\n",
      "User 60 of 1500: marc_ahx\n",
      "User 70 of 1500: TamaraChanel\n",
      "User 80 of 1500: VotersDemand\n",
      "User 90 of 1500: MC_Halo687\n",
      "User 100 of 1500: TheBlazeKari\n",
      "User 110 of 1500: Ambersallin\n",
      "User 120 of 1500: Hamal\n",
      "User 130 of 1500: JMarkMcGinnis\n",
      "User 140 of 1500: deleon_sarita\n",
      "User 150 of 1500: meaneyreport\n",
      "User 160 of 1500: touchawe\n",
      "User 170 of 1500: Devil_dog_71\n",
      "User 180 of 1500: 2020Jobs\n",
      "User 190 of 1500: stevenmaislin\n",
      "User 200 of 1500: bquentin3\n",
      "Error with user:  TCAIS\n",
      "User 210 of 1500: Oscarluism_\n",
      "User 220 of 1500: hillarybeth\n",
      "Error with user:  SEFLCareers\n",
      "User 230 of 1500: homesteadraised\n",
      "User 240 of 1500: ArmyBtownAD\n",
      "User 250 of 1500: MagentaMelee\n",
      "User 260 of 1500: thepaigelewis\n",
      "User 270 of 1500: meVschristina\n",
      "User 280 of 1500: AvvBrosisky\n",
      "User 290 of 1500: JeremiahWheele1\n",
      "User 300 of 1500: KurtLKrieger\n",
      "User 310 of 1500: TrueDumbBlonde\n",
      "User 320 of 1500: MrAbdelLHS\n",
      "User 330 of 1500: LandlordLinks\n",
      "User 340 of 1500: gfbakery2014\n",
      "User 350 of 1500: JCP803\n",
      "User 360 of 1500: GinoMerlot\n",
      "User 370 of 1500: MeSSiaH_808\n",
      "User 380 of 1500: callinlexie\n",
      "User 390 of 1500: nikkinik528\n",
      "User 400 of 1500: JNLIII\n",
      "User 410 of 1500: IntentionallyKB\n",
      "User 420 of 1500: ezoptical\n",
      "User 430 of 1500: CabriLuigi\n",
      "User 440 of 1500: Bo_Wright\n",
      "User 450 of 1500: itzshelleybell\n",
      "User 460 of 1500: RWonMaui\n",
      "User 470 of 1500: Felixcalvince\n",
      "User 480 of 1500: PatMateluna\n",
      "User 490 of 1500: petermlotzemd1\n",
      "User 500 of 1500: IAmRobWu\n",
      "User 510 of 1500: TeviTroy\n",
      "User 520 of 1500: lovelyalica_\n",
      "User 530 of 1500: BizarroLuthor\n",
      "User 540 of 1500: austintindle\n",
      "User 550 of 1500: 2GrkGrls\n",
      "User 560 of 1500: weapon83\n",
      "User 570 of 1500: corgli\n",
      "User 580 of 1500: Heat975Action\n",
      "User 590 of 1500: SequoiaRE_EB\n",
      "User 600 of 1500: guss813\n",
      "User 610 of 1500: fatrat282\n",
      "User 620 of 1500: WendyFinneyWood\n",
      "User 630 of 1500: JohnnyKey_AR\n",
      "User 640 of 1500: SNSPP_PATS_FO_\n",
      "User 650 of 1500: NJS4EVER\n",
      "User 660 of 1500: ntebdwa\n",
      "User 670 of 1500: y81de\n",
      "User 680 of 1500: JackieSGolf\n",
      "User 690 of 1500: ThingsToShea\n",
      "User 700 of 1500: dallasreese\n",
      "User 710 of 1500: TurkWarfield\n",
      "User 720 of 1500: KaruniaAgung\n",
      "User 730 of 1500: catsterle\n",
      "User 740 of 1500: Jonathan4743\n",
      "User 750 of 1500: joal1969\n",
      "User 760 of 1500: youremysonshine\n",
      "User 770 of 1500: s_mellors\n",
      "User 780 of 1500: iamluvlady\n",
      "User 790 of 1500: MerrisBadcock\n",
      "User 800 of 1500: VictoriaBBurns\n",
      "User 810 of 1500: Margaret8OK\n",
      "User 820 of 1500: vicentearenastv\n",
      "User 830 of 1500: golivent1\n",
      "User 840 of 1500: ConmayS\n",
      "User 850 of 1500: EthanEmeryWX\n",
      "User 860 of 1500: LeahKWilliams\n",
      "User 870 of 1500: sameolg713\n",
      "User 880 of 1500: yanaincali\n",
      "User 890 of 1500: ChannelE2E\n",
      "User 900 of 1500: DJDLuxTx\n",
      "User 910 of 1500: SWELGL\n",
      "User 920 of 1500: abestesq\n",
      "User 930 of 1500: kyle_reidy\n",
      "User 940 of 1500: dontgruberme\n",
      "User 950 of 1500: BaneSaysTrump\n",
      "User 960 of 1500: dariameetsworld\n",
      "User 970 of 1500: SharpstownTX\n",
      "User 980 of 1500: RogueSquadronMC\n",
      "User 990 of 1500: shortmotivation\n",
      "User 1000 of 1500: DanielMoralesTV\n",
      "User 1010 of 1500: rcantu\n",
      "User 1020 of 1500: ZestyOrangePic\n",
      "User 1030 of 1500: ElenaRsv\n",
      "User 1040 of 1500: BeyondBlunt\n",
      "User 1050 of 1500: biominer86\n",
      "User 1060 of 1500: MikeImken\n",
      "User 1070 of 1500: Rosa_Sherrod\n",
      "User 1080 of 1500: wifeofJW\n",
      "User 1090 of 1500: AndreaROMANIN1\n",
      "User 1100 of 1500: TheBarkingPigTX\n",
      "User 1110 of 1500: lesleymesser\n",
      "User 1120 of 1500: Renate651\n",
      "User 1130 of 1500: BarryMyhawginya\n",
      "User 1140 of 1500: PepperBurns1\n",
      "User 1150 of 1500: RedPoliticalMan\n",
      "User 1160 of 1500: MrsBoothSays\n",
      "User 1170 of 1500: Foundry_HB\n",
      "User 1180 of 1500: VenatoreMedia\n",
      "User 1190 of 1500: KimiKentMusic\n",
      "User 1200 of 1500: SociologyofCC\n",
      "User 1210 of 1500: mlweldon5\n",
      "User 1220 of 1500: _ItEndsNow_\n",
      "User 1230 of 1500: LexeyJohnson\n",
      "User 1240 of 1500: not_DonnyTrump\n",
      "User 1250 of 1500: plsimmo\n",
      "User 1260 of 1500: lillamb1997\n",
      "User 1270 of 1500: JayleenHeft\n",
      "User 1280 of 1500: PrayerPictures\n",
      "User 1290 of 1500: HoustonISDGov\n",
      "User 1300 of 1500: BrownsteinHyatt\n",
      "User 1310 of 1500: olivesjoy\n",
      "User 1320 of 1500: bellinissima\n",
      "User 1330 of 1500: SUSANIRELAND4\n",
      "User 1340 of 1500: onlndtng_sucks\n",
      "User 1350 of 1500: DiscoverDior\n",
      "User 1360 of 1500: SMLaughna\n",
      "User 1370 of 1500: TweetsfromMsB\n",
      "User 1380 of 1500: ConveyerOfCool\n",
      "User 1390 of 1500: ocuellar10\n",
      "User 1400 of 1500: BusinessMoney5\n",
      "User 1410 of 1500: Ejwhite0\n",
      "User 1420 of 1500: Justin_Horne\n",
      "User 1430 of 1500: t3hasian\n",
      "User 1440 of 1500: palomaresjl\n",
      "User 1450 of 1500: SMUTexasMexico\n",
      "User 1460 of 1500: WeAreGoLocal\n",
      "User 1470 of 1500: ScoopALoop3\n",
      "User 1480 of 1500: JennyWCVB\n",
      "User 1490 of 1500: GalvHistory\n",
      "User 1500 of 1500: acman2k6\n"
     ]
    }
   ],
   "source": [
    "# Check whether any of a user's Tweets fall within the bounding box and update column:\n",
    "# This will take several minutes to run\n",
    "\n",
    "users_df['tweet_from_locality'] = False\n",
    "users = users_df.screen_name.tolist()\n",
    "\n",
    "for i in range(len(users)):\n",
    "    if i%100 == 0:\n",
    "        print('Progress: {} of {}: {}'.format(i, len(users)))\n",
    "    try:\n",
    "        geo_tweets = User.objects.get(screen_name=u).tweet.filter(coordinates_lat__isnull=False)\n",
    "    except:\n",
    "        print('Error with user: ', u)\n",
    "        continue\n",
    "    for tweet in geo_tweets:\n",
    "        coords = (tweet.coordinates_lat, tweet.coordinates_lon)\n",
    "        if is_in_bounding_box(coords, boxes):\n",
    "            users_df.loc[users_df['screen_name'] == u, 'tweet_from_locality'] = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Temporary Export\n",
    "The dataframe is exported to a csv file before further manipulation (and column dropping) to avoid repeating the expensive tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'data/harvey_user_location/df_users_temp.csv'\n",
    "\n",
    "# Sanitise description field:\n",
    "users_df[\"description\"] = users_df[\"description\"].str.replace(\"\\r\", \" \")\n",
    "users_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import and check rows match:\n",
    "orig_shape = users_df.shape\n",
    "users_df_temp = pd.read_csv(path, index_col=0)\n",
    "if users_df_temp.shape == orig_shape:\n",
    "    users_df = users_df_temp\n",
    "else:\n",
    "    print(\"Shape mis-match. Check string sanitisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment\n",
    "New features are synthesised from existing data.\n",
    "\n",
    "\n",
    "Columns are added which represent the age of the user account (at the time of original collection) and at which point during the event the account was first detected by the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to represent age of account at time of detection, and how soon\n",
    "# after the beginning of the event that the account was first detected.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate whole days between two dates:\n",
    "def get_age_in_days(date_str, anchor_date):\n",
    "    date_str = str(date_str) # In case date_str is already a datetime obj\n",
    "    if date_str[-3:-2] == \":\":\n",
    "        date_str = date_str[:-3] + date_str[-2:]\n",
    "    try:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    except:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S.%f%z')\n",
    "    return abs((anchor_date - datetime_object).days)\n",
    "\n",
    "    \n",
    "# Get dates of event:\n",
    "e = Event.objects.all()[0]\n",
    "end = max(e.time_end, e.kw_stream_end, e.gps_stream_end)\n",
    "start = min(e.time_start, e.kw_stream_start, e.gps_stream_start)\n",
    "\n",
    "\n",
    "# Create column for age of account at end of data collection period:\n",
    "users_df['account_age'] = users_df['created_at'].apply(get_age_in_days, args=(end,))\n",
    "\n",
    "# Create column for how early from beginning of event account was first detected:\n",
    "users_df['day_of_detection'] = users_df['added_at'].apply(get_age_in_days, args=(start,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error in data collection allowed some `in_degree` and `out_degree` values to become negative. These are adjusted to 0. \n",
    "\n",
    "Note: It is possible that other entries have values one lower than what they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix negative values in in_degree and out_degree: an error from data collection:\n",
    "users_df.loc[users_df['in_degree'] < 0, 'in_degree'] = 0\n",
    "users_df.loc[users_df['out_degree'] < 0, 'out_degree'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding\n",
    "Qualitative columns are converted into formats interpretable by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent length of profile description:\n",
    "users_df['description_length'] = users_df.description.str.len()\n",
    "#users_df = users_df.drop(['description_length'], axis=1)\n",
    "\n",
    "# Profile language is English:\n",
    "users_df['lang_is_en'] = users_df['lang'] == 'en'\n",
    "users_df = users_df.drop(['lang'], axis=1)\n",
    "\n",
    "# translator_type exists:\n",
    "users_df['has_translator_type'] = users_df['translator_type'] != 'none'\n",
    "users_df = users_df.drop(['translator_type'], axis=1)\n",
    "\n",
    "# Url in profile:\n",
    "users_df['has_url'] = users_df['url'].notnull()\n",
    "\n",
    "# User has changed screen_name during collection period:\n",
    "users_df['changed_screen_name'] = users_df['old_screen_name'].notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for columns which should be represented categorically by counting the unique values in each column (under the assumption that categorical variables will have fewer than 20 unique values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_source\n",
      "[1 3] \n",
      "\n",
      "day_of_detection\n",
      "[3 5 1 2 6 4 7 8] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check columns for categorical candidates:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 20:\n",
    "        if len(users_df[col].unique()) == 2 and 0 in users_df[col].unique() and 1 in users_df[col].unique():\n",
    "            continue # Already encoded as True/False\n",
    "        print(col)\n",
    "        print(users_df[col].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`day_of_detection` is an ordinal feature and is therefore untouched.\n",
    "\n",
    "`data_source` is a categorical feature, so we re-encode it as a binary value. Here, the values of 1 and 3 are arbitrary (the 2 value was not implemented in the collection process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns as one-hot:\n",
    "# data_source==1 is not encoded as we only need n-1 columns to represent n categories.\n",
    "users_df['is_data_source_3'] = users_df['data_source'] == 3.\n",
    "users_df = users_df.drop(['data_source'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True/False columns are converted to 1/0 values for model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns from boolean to binary:\n",
      "\n",
      "default_profile\n",
      "default_profile_image\n",
      "geo_enabled\n",
      "has_extended_profile\n",
      "is_translation_enabled\n",
      "verified\n",
      "local_profile_location\n",
      "local_timezone\n",
      "coded_as_witness\n",
      "coded_as_non_witness\n",
      "tweet_from_locality\n",
      "lang_is_en\n",
      "has_translator_type\n",
      "has_url\n",
      "changed_screen_name\n",
      "is_data_source_3\n"
     ]
    }
   ],
   "source": [
    "# Convert True/False columns to 1/0\n",
    "print('Converting columns from boolean to binary:\\n')\n",
    "for col in users_df.columns:\n",
    "    if (len(users_df[col].value_counts()) == 2 and \n",
    "            True in users_df[col].values and \n",
    "            False in users_df[col].values):\n",
    "        print(col)\n",
    "        users_df[col] = users_df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe exported to CSV.\n",
      "(1500, 45)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/harvey_user_location/df_users.csv'\n",
    "\n",
    "users_df.to_csv(path)\n",
    "\n",
    "# Re-import and check rows match:\n",
    "orig_shape = users_df.shape\n",
    "users_df_temp = pd.read_csv(path, index_col=0)\n",
    "if users_df_temp.shape == orig_shape:\n",
    "    users_df = users_df_temp\n",
    "    print('Dataframe exported to CSV.')\n",
    "    print(users_df.shape)\n",
    "else:\n",
    "    print(\"Shape mis-match. Check string sanitisation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
