{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Location Classification in Hurricane Harvey\n",
    "This is the first notebook in a series which are written primarily as a research logbook for the author. They are therefore not to be considered complete and do not represent the final analysis. For this -- see the final published papers and thesis, or contact the author directly.\n",
    "\n",
    "The goal of this analysis is to evaluate methods by which users Tweeting about Hurricane Harvey may be classified as in the area or otherwise.\n",
    "\n",
    "Data was collected with custom software which observed several Twitter streams and enhanced this information by querying the Twitter REST APIs for the network data (friends and followers) of each author. Stream volume which exceeded the capacity of the REST requests was discarded. \n",
    "* The keyword stream monitored the terms: [#harvey, #harveystorm, #hurricaneharvey, #corpuschristi]\n",
    "* The GPS stream used the bounding box: [-99.9590682, 26.5486063, -93.9790001, 30.3893434]\n",
    "* The collection period ran from 2017-08-26 01:32:18 until 2017-09-02 10:30:52 \n",
    "* 46,872 Tweets by 31,932 unique authors were recorded\n",
    "\n",
    "Data was coded using an interface built into the collection software by a primary coder. A secondary coder coded a sub-set of coded users for validation of the coding schema. User instances were coded by whether they 'appeared to be in the affected area'.\n",
    "\n",
    "These notebooks access the data directly from the database using standard Django query syntax.\n",
    "\n",
    "# Data Cleaning & Enrichment\n",
    "In this chapter, we will investigate the data which was collected by the custom software. It is assessed for suitability for machine learning approaches, and enriched with synthesised features. The final result is a dataframe which is ready for statistical techniques which are presented in later chapters.\n",
    "\n",
    "First we get all the coding instances made by the primary and secondary coders, and check the total codings of each class. There may be multiple coding dimensions (sets of coding schema), in which case the code requires adjustment to constrain to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialisation ###\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "\n",
    "# Location of data files\n",
    "DIR = './data/harvey_user_location/'\n",
    "\n",
    "EVENT_NAME = Event.objects.all()[0].name.replace(' ', '')\n",
    "DF_FILENAME = 'df_users.csv'\n",
    "\n",
    "# Confirm correct database is set in Django settings.py\n",
    "if 'Harvey' not in EVENT_NAME:\n",
    "    raise Exception('Event name mismatch -- check database set in Django')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  HurricaneHarvey\n",
      "Coding Dimension:  Local\n",
      "Subject:  user\n",
      "Class Totals (primary / secondary): \n",
      "\t Unsure: \t31 \t/ 8\n",
      "\t Non-Witness: \t1083 \t/ 112\n",
      "\t Witness: \t386 \t/ 80\n",
      "1500 Accounts coded by primary coder, 200 by secondary coder.\n"
     ]
    }
   ],
   "source": [
    "# Get coding instances of user objects:\n",
    "account_codings = (Coding.objects\n",
    "                    .filter(coding_id=1)\n",
    "                    .filter(user__isnull=False)\n",
    "                    .filter(data_code__data_code_id__gt=0)\n",
    "                  )\n",
    "account_codings_secondary = (Coding.objects\n",
    "                                 .filter(coding_id=2)\n",
    "                                 .filter(user__isnull=False)\n",
    "                            )\n",
    "\n",
    "# Check available coding schema:\n",
    "\n",
    "# Confirm correct database is set in Django settings.py\n",
    "print('Dataset: ', EVENT_NAME)\n",
    "dimensions = DataCodeDimension.objects.all()[1:]\n",
    "for d in dimensions:\n",
    "    print('Coding Dimension: ', d.name)\n",
    "    print('Subject: ', d.coding_subject)\n",
    "    print('Class Totals (primary / secondary): ')\n",
    "    for code in d.datacode.all():\n",
    "        print(\"\\t {}: \\t{} \\t/ {}\"\n",
    "                .format(code.name, \n",
    "                    account_codings.filter(data_code__id=code.id).count(), \n",
    "                    account_codings_secondary.filter(data_code__id=code.id).count())\n",
    "             )\n",
    "print(\"{} Accounts coded by primary coder, {} by secondary coder.\".format(account_codings.count(), account_codings_secondary.count()))\n",
    "if len(dimensions) > 1: \n",
    "    print('\\tNote: Totals represent sum of all codes from {} dimensions.'.format(len(dimensions)))\n",
    "    print('WARNING: Code in cells below assume one dimension -- adjust to constrain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a dataframe of all users which have been coded by the primary coder to create the initial dataset. The subjects of the secondary coder are a subset of this set by design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utc_offset                           920\n",
       "time_zone                            920\n",
       "centrality_closeness                 890\n",
       "centrality_degree                    890\n",
       "centrality_eigenvector               890\n",
       "centrality_load                      890\n",
       "centrality_undirected_eigenvector    890\n",
       "centrality_betweenness               890\n",
       "url                                  808\n",
       "old_screen_name                       98\n",
       "user_network_update_observed_at        0\n",
       "is_deleted_observed                    0\n",
       "is_deleted                             0\n",
       "user_followers                         0\n",
       "centrality_katz                        0\n",
       "user_followers_update                  0\n",
       "user_following                         0\n",
       "user_following_update                  0\n",
       "needs_phone_verification               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all Users coded by primary coder:\n",
    "# (exclude data_code_id=0 as this is the temporary 'to be coded' class)\n",
    "users = User.objects.filter(coding_for_user__coding_id=1, \n",
    "                            coding_for_user__data_code__data_code_id__gt=0)\n",
    "users_df = pd.DataFrame(list(users.values()))\n",
    "\n",
    "# Check for missing values by column:\n",
    "users_df.count()[users_df.count() != account_codings.count()].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The centrality measures have a common value. As these values are only calculated for the largest connected component of the graph, this consistency makes sense.\n",
    "* The fields time_zone, utc_offset, old_screen_name, and url are nullable.\n",
    "* Twitter has deprecated the field 'needs_phone_verification', so no values were returned.\n",
    "* Various 0 value fields had been added to the database schema but were not implemented at the time of collection. These can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping empty columns\n",
    "empty_cols = users_df.columns[users_df.isnull().all()]\n",
    "users_df.drop(empty_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop columns which have a single value, as they provide no differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns:  protected\n",
      "Dropping columns:  ratio_media\n",
      "Dropping columns:  user_class\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with only one unique value:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 1:\n",
    "        print('Dropping columns: ', col)\n",
    "        users_df = users_df.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Location Data\n",
    "There are a number of options which can represent the ground truth location of the user.\n",
    "* Location listed on a user's profile\n",
    "* User Timezone (deprecated)\n",
    "* Manual Coding\n",
    "* Location data derived from Tweet stream\n",
    "    * GPS tagged Tweets\n",
    "    * Mention of location in Tweet body\n",
    "    \n",
    "### Parsing user-set location in profile field\n",
    "The location the user sets in their profile as a string is evaluated and a locality decision made. In this instance, a location is considered 'local' if its coordinates (supplied by the Google geolocation API or parsed directly from the location string) fall within the bounding box used for geographic Twitter data collection, or if it contains the string 'houston' or 'christi' (representing the town Corpus Christi). Both of these locations fall within the bounding box, and are used here as a time-saving operation.\n",
    "\n",
    "Note that as this field can be set manually, it is unverifiable and therefore not a perfect representation of location, even where it exists. Users may neglect to update their location after moving, and some observations were made of users setting their location to that of a disaster event as a 'show of solidarity'.\n",
    "\n",
    "NOTE: Is using this method for other locations, be careful of instances such as: the single-point coordinates for the location 'United States' may be within a given bounding box despite the area represented encompassing a much larger area (and therefore unlikely to actually be from within the box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This block supports manual coding of locations as local or non-local.\n",
    "## It has been superceded by the next block which uses the Googlemaps API\n",
    "\n",
    "#location_list = users_df.location.unique()\n",
    "#with open('data/harvey_user_location/location_list_all_profile_locations.txt', 'w') as f:\n",
    "#    for item in location_list:\n",
    "#        f.write(\"%s\\n\" % item)\n",
    "\n",
    "################\n",
    "## This list then manually sorted and non-local locations removed.\n",
    "## List of local locations then re-imported.\n",
    "## Note this list excludes any locations containing 'Christi' or 'Houston'\n",
    "## Note: if more users are coded, this list needs to be re-examined. Raise alert:\n",
    "#if users_df.shape[0] != 931:\n",
    "#    print('ALERT: New codings detected. Consider updating manual locality selection')\n",
    "################\n",
    "\n",
    "#with open('data/harvey_user_location/local_profile_locations_manual_check.txt', 'r') as f:\n",
    "#    local_locations_list = f.read().splitlines()\n",
    "    \n",
    "## Create column for users with local location listed in profile\n",
    "#users_df['local_profile_location_manual'] = \\\n",
    "#    (users_df.location.str.contains('houston|christi', case=False, regex=True) |\n",
    "#    users_df.location.isin(local_locations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def parse_coordinates(string):\n",
    "    '''Parse a string for coordinates'''\n",
    "    reg = '[nsewNSEW]?\\s?-?\\d+[\\.°]\\s?\\d+°?\\s?[nsewNSEW]?'\n",
    "    result = re.findall(reg, string)\n",
    "    # Check if format is degrees minutes rather than degrees decimal (discard seconds)\n",
    "    reg_has_minutes = '\\d+\\s?°\\s*\\d+.?\\d+\\s?\\''\n",
    "    has_minutes = re.findall(reg_has_minutes, string)\n",
    "    if len(result) == 2: # Coordinates detected\n",
    "        for i in range(len(result)):\n",
    "            # Replace middle degree symbol with decimal:\n",
    "            reg_middle_degree = '(\\d+)°\\s?(\\d+)'\n",
    "            result[i] = re.sub(reg_middle_degree, r'\\1.\\2', result[i])\n",
    "            # Remove trailing degree symbol, N and E marks:\n",
    "            reg_strip = '[°neNE\\s]'\n",
    "            result[i] = re.sub(reg_strip, '', result[i])\n",
    "            # Replace south/west with negative sign:\n",
    "            reg_replace_sw = '[swSW](\\d+\\.\\d+)|(\\d+\\.\\d+)[swSW]'\n",
    "            result[i] = re.sub(reg_replace_sw, r'-\\1\\2', result[i])\n",
    "            # Remove double negative (where string contained eg. '-99.10w')\n",
    "            result[i] = re.sub('--', '-', result[i])\n",
    "            result[i] = float(result[i])\n",
    "            # Convert minutes to decimal\n",
    "            if len(has_minutes) == 2:\n",
    "                result[i] = math.modf(result[i])[1] + math.modf(result[i])[0] / 60 * 100\n",
    "                result[i] = round(result[i], 5)\n",
    "        return (result[0], result[1])\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import googlemaps\n",
    "\n",
    "\n",
    "def is_in_bounding_box(coords, boxes):\n",
    "    '''\n",
    "    Check whether coordinates fall within defined bounding box:\n",
    "    Boxes are defined as their NW and SE points.\n",
    "    '''\n",
    "    for box in boxes:\n",
    "        if coords[0] < box[0][0] and coords[0] > box[1][0]:\n",
    "            if coords[1] > box[0][1] and coords[1] < box[1][1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_local(location, boxes, known_localities=[]):\n",
    "    '''\n",
    "    Check whether a location string falls within a set of \n",
    "    bounding boxes using Googlemaps API.\n",
    "    \n",
    "    <<<< WARNING >>>>\n",
    "    If a broad location is given (e.g. 'Texas', 'USA'), the \n",
    "    returned coordinates may fall within the bounding box by\n",
    "    chance and give false positives.\n",
    "    '''\n",
    "    if not location:\n",
    "        return\n",
    "    # Check known localities first to save on API requests:\n",
    "    for x in known_localities:\n",
    "        if x in location:\n",
    "            return True\n",
    "    # Try and parse coordinates from string rather than API query:\n",
    "    coords = parse_coordinates(location)\n",
    "    # Get coords from API:\n",
    "    if not coords:\n",
    "        # Get API key from file:\n",
    "        with open(\"auth.yml\", 'r') as ymlfile:\n",
    "            auth = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "        key = auth['apikeys']['googlemaps']\n",
    "        gmaps = googlemaps.Client(key=key)\n",
    "        \n",
    "        #########################################################\n",
    "        ####### OVERRIDE API OBJECT TO PREVENT API CALLS: #######\n",
    "        geocode_result = gmaps.geocode(location)\n",
    "        #geocode_result = False\n",
    "        #print('WARNING -- API DISABLED')\n",
    "        #########################################################\n",
    "        #########################################################\n",
    "        if geocode_result:\n",
    "            lat = geocode_result[0]['geometry']['location']['lat']\n",
    "            lon = geocode_result[0]['geometry']['location']['lng']\n",
    "            coords = (lat, lon)\n",
    "    if coords:\n",
    "        return(is_in_bounding_box(coords, boxes))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local locations from file\n"
     ]
    }
   ],
   "source": [
    "# Bounding boxes used for Hurricane Harvey dataset:\n",
    "boxes = [[(29.1197,-99.9590682),(26.5486063,-97.5021)],\n",
    "        [(30.3893434,-97.5021),(26.5486063,-93.9790001)]]\n",
    "\n",
    "# Don't need to look these up (save on API requests)\n",
    "known_localities = ['houston', 'christi']\n",
    "\n",
    "users_df[\"location\"] = users_df[\"location\"].str.lower().str.strip()\n",
    "\n",
    "# Use Google maps API to determine locality of a string and export to file.\n",
    "# Import from existing file if available to avoid API calls.\n",
    "local_file = 'location_list_from_api_local.txt'\n",
    "non_local_file = 'location_list_from_api_non_local.txt'\n",
    "non_location_file = 'location_list_from_api_non_location.txt'\n",
    "\n",
    "try:\n",
    "    local_location_list = []\n",
    "    with open(DIR + local_file, 'r') as f:\n",
    "        for line in f:\n",
    "            local_location_list.append(line.rstrip('\\n'))\n",
    "    non_local_location_list = []\n",
    "    with open(DIR + non_local_file, 'r') as f:\n",
    "        for line in f:\n",
    "            non_local_location_list.append(line.rstrip('\\n'))\n",
    "    non_location_list = []\n",
    "    with open(DIR + non_location_file, 'r') as f:\n",
    "        for line in f:\n",
    "            non_location_list.append(line.rstrip('\\n'))\n",
    "    print(\"Loaded local locations from file\")\n",
    "    \n",
    "except:\n",
    "    # Create sublist of local/non-local locations (non-local only for manual verification)\n",
    "    location_list = users_df.location.dropna().unique()\n",
    "    print(\"Running is_local() for {} strings...\".format(len(location_list)))\n",
    "    \n",
    "    loc_dict = {}\n",
    "    for loc in locs:\n",
    "        loc_dict[loc] = is_location(loc, boxes)\n",
    "    local_location_list = [x for x in loc_dict.keys() if loc_dict[x] == True]\n",
    "    non_local_location_list = [x for x in loc_dict.keys() if loc_dict[x] == False]\n",
    "    non_location_list = [x for x in loc_dict.keys() if loc_dict[x] == None]    \n",
    "#     local_location_list = [loc for loc in location_list if is_local(loc, boxes, known_localities)]\n",
    "#     non_local_location_list = [loc for loc in location_list if loc not in local_location_list]\n",
    "\n",
    "    # Write lists to file to save calling API on kernel restart:\n",
    "    with open(DIR + local_file, 'w') as f:\n",
    "        for item in local_location_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with open(DIR + non_local_file, 'w') as f:\n",
    "        for item in non_local_location_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with open(DIR + non_location_file, 'w') as f:\n",
    "        for item in non_location_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Create column for users with local location listed in profile\n",
    "# Currently setting non-location, non-null profile locations as null vals.\n",
    "users_df['is_local_profile_location'] = users_df.location.str.lower().isin(local_location_list)\n",
    "users_df['is_non_local_profile_location'] = users_df.location.str.lower().isin(non_local_location_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local: 397, 26.5%\n",
      "Non-Local: 765, 51.0%\n",
      "No location listed: 338, 22.5%\n"
     ]
    }
   ],
   "source": [
    "# Check the proportions. Note that location strings which existed but\n",
    "# could not be parsed as locations were grouped with the null values\n",
    "\n",
    "tot = len(users_df)\n",
    "loc = sum(users_df.is_local_profile_location)\n",
    "non_loc = sum(users_df.is_non_local_profile_location)\n",
    "undef = len(users_df) - sum((users_df.is_non_local_profile_location | users_df.is_local_profile_location))\n",
    "\n",
    "print('Local: {}, {:.3}%'.format(loc, loc/tot*100))\n",
    "print('Non-Local: {}, {:.3}%'.format(non_loc, non_loc/tot*100))\n",
    "print('No location listed: {}, {:.3}%'.format(undef, undef/tot*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timezone Field\n",
    "Timezone data provided by Twitter when capturing the user objects is less specific than other methods, but may be useful as a supplementary source.\n",
    "As this data field has been deprecated by Twitter, it will not be available in new data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central Time (US & Canada)     341\n",
      "Eastern Time (US & Canada)     197\n",
      "Pacific Time (US & Canada)     177\n",
      "Mountain Time (US & Canada)     37\n",
      "Quito                           25\n",
      "Name: time_zone, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View most prevalent time zones:\n",
    "print(users_df['time_zone'].value_counts().head())\n",
    "\n",
    "# Create column for profiles in relevant time zone (chosen manually based on the observed event):\n",
    "RELEVANT_TIMEZONE = 'Central Time (US & Canada)'\n",
    "users_df['is_local_timezone'] = users_df.time_zone == RELEVANT_TIMEZONE\n",
    "users_df = users_df.drop(['time_zone', 'utc_offset'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Coding\n",
    "Accounts were manually coded as 'local' or 'non-local'.\n",
    "\n",
    "Coders were shown the user account details as well as the Twitter stream of the user. The coders were instructed to determine whether the user account was in an area affected by the hurricane at any point during the data collection period. Therefore, the term 'local' may be misleading to the reader, as the definition given to the coders will include anyone visiting the area as, for example, a responder or aid worker. This larger set of 'on the ground' users is a more useful target for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent manual coding:\n",
    "users_df['coded_as'] = \\\n",
    "    users_df['screen_name'].apply(lambda x: account_codings.get(user__screen_name = x).data_code.name)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "users_df['is_coded_as_witness'] = users_df['coded_as'] == 'Witness'\n",
    "users_df['is_coded_as_non_witness'] = users_df['coded_as'] == 'Non-Witness'\n",
    "\n",
    "# Remove original column:\n",
    "users_df = users_df.drop(['coded_as'], axis=1)\n",
    "\n",
    "# Note: Two columns are used as there is a third value of 'unsure'\n",
    "# This could be merged into one of the columns if preferred.\n",
    "# print('Third code count: ', sum(~users_df['coded_as_witness'] & ~users_df['coded_as_non_witness']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Unsure' code is represented as `False` values in both the `coded_as_witness` and `coded_as_non_witness` columns. If the 'Unsure' rows are removed, we can also remove the `coded_as_non_witness` column (which is now represented as `False` in the `coded_as_witness` column).\n",
    "<br /><br />\n",
    "\n",
    "### GPS from Tweet stream\n",
    "While the Tweets detected by the system may not contain GPS data, the author may have made other GPS-enabled Tweets during the collection period from which we can infer location. We create a column representing whether the user made any geolocated Tweets within the bounding box during the collection period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0 of 1500: LiveHappySA\n",
      "Progress: 100 of 1500: thehalfwayj\n",
      "Progress: 200 of 1500: polvogt\n",
      "Progress: 300 of 1500: charmgirl13\n",
      "Progress: 400 of 1500: eodpenguin\n",
      "Progress: 500 of 1500: lrixford\n",
      "Progress: 600 of 1500: SWJOCOCERT\n",
      "Progress: 700 of 1500: thereal4speed\n",
      "Progress: 800 of 1500: dschexnaydre\n",
      "Progress: 900 of 1500: YPEatingwStyle\n",
      "Progress: 1000 of 1500: KyleColby\n",
      "Progress: 1100 of 1500: catchtheteaTV\n",
      "Progress: 1200 of 1500: FranaldoCurl\n",
      "Progress: 1300 of 1500: KosmosEnergy\n",
      "Progress: 1400 of 1500: MsDimplez2u\n"
     ]
    }
   ],
   "source": [
    "# Check whether any of a user's Tweets fall within the bounding box and update column.\n",
    "# has_tweet_from_locality == True if ANY of a user's Tweets fall within the box.\n",
    "# Note: This will take several minutes to run\n",
    "\n",
    "users_df['has_tweet_from_locality'] = False\n",
    "users_list = users_df.screen_name.tolist()\n",
    "\n",
    "for i in range(len(users_list)):\n",
    "    u = users_list[i]\n",
    "    if i%100 == 0:\n",
    "        print('Progress: {} of {}: {}'.format(i, len(users_list), u))\n",
    "    try:\n",
    "        geo_tweets = User.objects.get(screen_name=u).tweet.filter(coordinates_lat__isnull=False)\n",
    "    except:\n",
    "        print('Error with user: ', u)\n",
    "        continue\n",
    "    for tweet in geo_tweets:\n",
    "        coords = (tweet.coordinates_lat, tweet.coordinates_lon)\n",
    "        if is_in_bounding_box(coords, boxes):\n",
    "            users_df.loc[users_df['screen_name'] == u, 'has_tweet_from_locality'] = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Temporary Export\n",
    "The dataframe is exported to a csv file before further manipulation (and column dropping) to avoid repeating the expensive tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'df_users_interim.csv'\n",
    "path = DIR + filename\n",
    "\n",
    "# Sanitise description field:\n",
    "users_df[\"description\"] = users_df[\"description\"].str.replace(\"\\r\", \" \")\n",
    "users_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import and check rows match:\n",
    "orig_shape = users_df.shape\n",
    "users_df_temp = pd.read_csv(path, index_col=0)\n",
    "if users_df_temp.shape == orig_shape:\n",
    "    users_df = users_df_temp\n",
    "else:\n",
    "    print(\"Shape mis-match. Check string sanitisation\")\n",
    "path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichment\n",
    "New features are synthesised from existing data.\n",
    "\n",
    "\n",
    "Columns are added which represent the age of the user account (at the time of original collection) and at which point during the event the account was first detected by the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns to represent age of account at time of detection, and how soon\n",
    "# after the beginning of the event that the account was first detected.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate whole days between two dates:\n",
    "def get_age_in_days(date_str, anchor_date):\n",
    "    date_str = str(date_str) # In case date_str is already a datetime obj\n",
    "    if date_str[-3:-2] == \":\":\n",
    "        date_str = date_str[:-3] + date_str[-2:]\n",
    "    try:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    except:\n",
    "        datetime_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S.%f%z')\n",
    "    return abs((anchor_date - datetime_object).days)\n",
    "\n",
    "    \n",
    "# Get dates of event:\n",
    "e = Event.objects.all()[0]\n",
    "end = max(e.time_end, e.kw_stream_end, e.gps_stream_end)\n",
    "start = min(e.time_start, e.kw_stream_start, e.gps_stream_start)\n",
    "\n",
    "\n",
    "# Create column for age of account at end of data collection period:\n",
    "users_df['account_age'] = users_df['created_at'].apply(get_age_in_days, args=(end,))\n",
    "\n",
    "# Create column for how early from beginning of event account was first detected:\n",
    "users_df['day_of_detection'] = users_df['added_at'].apply(get_age_in_days, args=(start,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error in data collection allowed some `in_degree` and `out_degree` values to become negative. These are adjusted to 0. \n",
    "\n",
    "Note: It is possible that other entries have values one lower than what they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix negative values in in_degree and out_degree: an error from data collection:\n",
    "users_df.loc[users_df['in_degree'] < 0, 'in_degree'] = 0\n",
    "users_df.loc[users_df['out_degree'] < 0, 'out_degree'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding\n",
    "Qualitative columns are converted into formats interpretable by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to represent length of profile description:\n",
    "users_df['description_length'] = users_df.description.str.len()\n",
    "#users_df = users_df.drop(['description_length'], axis=1)\n",
    "\n",
    "# Profile language is English:\n",
    "users_df['is_lang_en'] = users_df['lang'] == 'en'\n",
    "users_df = users_df.drop(['lang'], axis=1)\n",
    "\n",
    "# translator_type exists:\n",
    "users_df['has_translator_type'] = users_df['translator_type'] != 'none'\n",
    "users_df = users_df.drop(['translator_type'], axis=1)\n",
    "\n",
    "# Url in profile:\n",
    "users_df['has_url'] = users_df['url'].notnull()\n",
    "\n",
    "# User has changed screen_name during collection period:\n",
    "users_df['has_changed_screen_name'] = users_df['old_screen_name'].notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for columns which should be represented categorically by counting the unique values in each column (under the assumption that categorical variables will have fewer than 20 unique values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_source\n",
      "[1 3] \n",
      "\n",
      "day_of_detection\n",
      "[3 5 1 2 6 4 7 8] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check columns for categorical candidates:\n",
    "for col in users_df.columns:\n",
    "    if len(users_df[col].value_counts()) <= 20:\n",
    "        if len(users_df[col].unique()) == 2 and 0 in users_df[col].unique() and 1 in users_df[col].unique():\n",
    "            continue # Already encoded as True/False\n",
    "        print(col)\n",
    "        print(users_df[col].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`day_of_detection` is an ordinal feature and is therefore untouched.\n",
    "\n",
    "`data_source` is a categorical feature, so we re-encode it as a binary value. Here, the values of 1 and 3 are arbitrary (the 2 value was not implemented in the collection process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns as one-hot:\n",
    "# data_source==1 is not encoded as we only need n-1 columns to represent n categories.\n",
    "users_df['is_data_source_3'] = users_df['data_source'] == 3.\n",
    "users_df = users_df.drop(['data_source'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True/False columns are converted to 1/0 values for model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns from boolean to binary:\n",
      "\n",
      "default_profile\n",
      "default_profile_image\n",
      "geo_enabled\n",
      "has_extended_profile\n",
      "is_translation_enabled\n",
      "verified\n",
      "is_local_profile_location\n",
      "is_local_timezone\n",
      "is_coded_as_witness\n",
      "is_coded_as_non_witness\n",
      "has_tweet_from_locality\n",
      "is_lang_en\n",
      "has_translator_type\n",
      "has_url\n",
      "has_changed_screen_name\n",
      "is_data_source_3\n"
     ]
    }
   ],
   "source": [
    "# Convert True/False columns to 1/0\n",
    "print('Converting columns from boolean to binary:\\n')\n",
    "for col in users_df.columns:\n",
    "    if (len(users_df[col].value_counts()) == 2 and \n",
    "            True in users_df[col].values and \n",
    "            False in users_df[col].values):\n",
    "        print(col)\n",
    "        users_df[col] = users_df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>added_at</th>\n",
       "      <th>centrality_betweenness</th>\n",
       "      <th>centrality_closeness</th>\n",
       "      <th>centrality_degree</th>\n",
       "      <th>centrality_eigenvector</th>\n",
       "      <th>centrality_load</th>\n",
       "      <th>centrality_undirected_eigenvector</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>...</th>\n",
       "      <th>account_age</th>\n",
       "      <th>day_of_detection</th>\n",
       "      <th>description_length</th>\n",
       "      <th>is_lang_en</th>\n",
       "      <th>has_translator_type</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_changed_screen_name</th>\n",
       "      <th>is_data_source_3</th>\n",
       "      <th>is_coded_as_witness</th>\n",
       "      <th>is_coded_as_non_witness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-28 20:42:59.273657+00:00</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.135798</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>3.905631e-07</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>5.377061e-05</td>\n",
       "      <td>2013-03-01 19:23:11+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1645</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-30 13:58:20.296918+00:00</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.122066</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>1.785776e-07</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.210768e-06</td>\n",
       "      <td>2014-01-20 00:34:57+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-26 19:51:45.107222+00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077120</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>8.518251e-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.589479e-11</td>\n",
       "      <td>2012-07-24 13:47:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1865</td>\n",
       "      <td>1</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-26 11:13:05.769123+00:00</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.167070</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>4.315565e-05</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>3.327919e-04</td>\n",
       "      <td>2010-12-16 17:30:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2451</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-26 14:19:23.604361+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-04-24 12:08:14+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3052</td>\n",
       "      <td>1</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           added_at  centrality_betweenness  \\\n",
       "0  2017-08-28 20:42:59.273657+00:00                0.000043   \n",
       "1  2017-08-30 13:58:20.296918+00:00                0.000015   \n",
       "2  2017-08-26 19:51:45.107222+00:00                0.000000   \n",
       "3  2017-08-26 11:13:05.769123+00:00                0.000383   \n",
       "4  2017-08-26 14:19:23.604361+00:00                     NaN   \n",
       "\n",
       "   centrality_closeness  centrality_degree  centrality_eigenvector  \\\n",
       "0              0.135798           0.000304            3.905631e-07   \n",
       "1              0.122066           0.000243            1.785776e-07   \n",
       "2              0.077120           0.000061            8.518251e-14   \n",
       "3              0.167070           0.000668            4.315565e-05   \n",
       "4                   NaN                NaN                     NaN   \n",
       "\n",
       "   centrality_load  centrality_undirected_eigenvector  \\\n",
       "0         0.000048                       5.377061e-05   \n",
       "1         0.000019                       2.210768e-06   \n",
       "2         0.000000                       7.589479e-11   \n",
       "3         0.000388                       3.327919e-04   \n",
       "4              NaN                                NaN   \n",
       "\n",
       "                  created_at  default_profile  default_profile_image  ...  \\\n",
       "0  2013-03-01 19:23:11+00:00                0                      0  ...   \n",
       "1  2014-01-20 00:34:57+00:00                1                      0  ...   \n",
       "2  2012-07-24 13:47:47+00:00                0                      0  ...   \n",
       "3  2010-12-16 17:30:04+00:00                0                      0  ...   \n",
       "4  2009-04-24 12:08:14+00:00                0                      0  ...   \n",
       "\n",
       "  account_age  day_of_detection  description_length  is_lang_en  \\\n",
       "0        1645                 3                96.0           1   \n",
       "1        1321                 5               124.0           1   \n",
       "2        1865                 1               134.0           1   \n",
       "3        2451                 1               128.0           1   \n",
       "4        3052                 1               136.0           1   \n",
       "\n",
       "   has_translator_type  has_url  has_changed_screen_name  is_data_source_3  \\\n",
       "0                    1        0                        0                 0   \n",
       "1                    0        1                        0                 0   \n",
       "2                    0        1                        0                 1   \n",
       "3                    0        1                        0                 0   \n",
       "4                    0        0                        0                 0   \n",
       "\n",
       "   is_coded_as_witness  is_coded_as_non_witness  \n",
       "0                    0                        1  \n",
       "1                    0                        1  \n",
       "2                    0                        1  \n",
       "3                    0                        1  \n",
       "4                    0                        1  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move target columns to end of dataframe:\n",
    "target_cols = ['is_coded_as_witness', 'is_coded_as_non_witness']\n",
    "\n",
    "cols = list(users_df.columns.values)\n",
    "for col in target_cols:\n",
    "    cols.remove(col)\n",
    "users_df = users_df[cols + target_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe exported to CSV.\n",
      "(1500, 46)\n"
     ]
    }
   ],
   "source": [
    "path = DIR + DF_FILENAME\n",
    "\n",
    "users_df.to_csv(path)\n",
    "\n",
    "# Re-import and check rows match:\n",
    "orig_shape = users_df.shape\n",
    "users_df_temp = pd.read_csv(path, index_col=0)\n",
    "if users_df_temp.shape == orig_shape:\n",
    "    users_df = users_df_temp\n",
    "    print('Dataframe exported to CSV.')\n",
    "    print(users_df.shape)\n",
    "else:\n",
    "    print(\"Shape mis-match. Check string sanitisation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
