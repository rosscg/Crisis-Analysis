{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis of Hurricane Harvey Data\n",
    "\n",
    "This notebook contains an initial exploration of the data collected from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:07:23.232958Z",
     "start_time": "2019-07-10T18:07:23.229469Z"
    }
   },
   "outputs": [],
   "source": [
    "from streamcollect.models import Tweet, User, Event, Hashtag, Url, Mention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Event details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Event.objects.all()[0]\n",
    "print(e.name)\n",
    "print(min(e.kw_stream_start, e.gps_stream_start))\n",
    "td = e.kw_stream_end - e.kw_stream_start\n",
    "print('Capture window (kw): {} days, {} hours, {} minutes'.format(td.days, td.seconds//3600, td.seconds//60%60))\n",
    "td = e.gps_stream_end - e.gps_stream_start\n",
    "print('Capture window (geo): {} days, {} hours, {} minutes'.format(td.days, td.seconds//3600, td.seconds//60%60))\n",
    "tracked_kws = Event.objects.all()[0].keyword.all().values_list('keyword', flat=True)\n",
    "print('Tracked keywords: {}'.format(tracked_kws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print fields for the Tweet model for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:07:23.272225Z",
     "start_time": "2019-07-10T18:07:23.235050Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[f.name for f in Tweet._meta.get_fields()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the Tweets by `data_source` value (i.e. whether they were detected by the stream or added).\n",
    "\n",
    "* -1 = Identified as spam/irrelevant\n",
    "* 0 = Sourced from rest API, or quotes/replied_to Tweets\n",
    "* 1 = Low-priority keyword stream\n",
    "* 2 = High-priority keyword stream\n",
    "* 3 = Geo stream (contains coordinates)\n",
    "* 4 = Geo stream (does not contain coordinates, but Place object falls within bounding box)\n",
    "\n",
    "Note that some older datasets may not adhere strictly to the above. E.g. for the Hurricane Harvey dataset, all keyword streamed Tweets are `data_source = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:07:25.724926Z",
     "start_time": "2019-07-10T18:07:23.274338Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total Tweets: {}\".format(Tweet.objects.all().count()))\n",
    "for n in range(-1,5):\n",
    "    print(\"data_source = {}: {}\".format(n, Tweet.objects.filter(data_source=n).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Content\n",
    "First, we check the proportion of Tweets (by stream) which contain URLs, mentions, and hashtags. As media was not stored during the Harvey data collection, we cannot test this proportion.\n",
    "\n",
    "Also consider testing hashtags excluding those already tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_proportions(queryset):\n",
    "    total = queryset.count()\n",
    "    print('Contains hashtag: {}'.format(queryset.filter(hashtags__isnull=False).distinct().count()))\n",
    "    print('{0:.2%}'.format(queryset.filter(hashtags__isnull=False).distinct().count()/total))\n",
    "    print('No hashtag: {}'.format(queryset.filter(hashtags__isnull=True).count()))\n",
    "    print('Contains URL: {}'.format(queryset.filter(urls__isnull=False).distinct().count()))\n",
    "    print('{0:.2%}'.format(queryset.filter(urls__isnull=False).distinct().count()/total))\n",
    "    print('No URL: {}'.format(queryset.filter(urls__isnull=True).count()))\n",
    "    print('Contains mention: {}'.format(queryset.filter(mentions__isnull=False).distinct().count()))\n",
    "    print('{0:.2%}'.format(queryset.filter(mentions__isnull=False).distinct().count()/total))\n",
    "    print('No mention: {}'.format(queryset.filter(mentions__isnull=True).count()))\n",
    "    print('Total: {}'.format(total))\n",
    "\n",
    "for data_source in [1,3]:\n",
    "    print('\\nChecking source = {}'.format(data_source))\n",
    "    queryset = Tweet.objects.filter(data_source=data_source)\n",
    "    print_proportions(queryset)\n",
    "\n",
    "print('\\nChecking Overall...')\n",
    "queryset = Tweet.objects.filter(data_source__gte=1)\n",
    "print_proportions(queryset)\n",
    "\n",
    "print('\\nSource = 1, excluding Instagram...')\n",
    "queryset = Tweet.objects.filter(data_source=1).exclude(source='Instagram')\n",
    "print_proportions(queryset)\n",
    "print('\\nSource = 3, excluding Instagram...')\n",
    "queryset = Tweet.objects.filter(data_source=3).exclude(source='Instagram')\n",
    "print_proportions(queryset)\n",
    "print('\\nOverall, excluding Instagram...')\n",
    "queryset = Tweet.objects.filter(data_source__gte=1).exclude(source='Instagram')\n",
    "print_proportions(queryset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue where some Tweets from source=1 do not contain hashtag objects, which may have been an error during storage to database, or in the way the Tweet is formatted.\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Tweet.objects.filter(data_source=1).filter(hashtags__isnull=True)[:5]:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check proportion of Tweets which include coordinates. It is expected that this is 100% for the data_source=3 stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_coordinates(queryset):\n",
    "    print('Geo-tag count: {}'.format(queryset.filter(coordinates_type__isnull=False).count()))\n",
    "    print('{0:.2%}'.format(queryset.filter(coordinates_type__isnull=False).count()/queryset.count()))\n",
    "\n",
    "print('Number of Tweets including coordinates:')\n",
    "print('\\nData source = 1:')\n",
    "queryset = Tweet.objects.filter(data_source=1)\n",
    "gps_coordinates(queryset)\n",
    "print('\\nData source = 3:')\n",
    "queryset = Tweet.objects.filter(data_source=3)\n",
    "gps_coordinates(queryset)\n",
    "print('\\nOverall:')\n",
    "queryset = Tweet.objects.filter(data_source__gte=1)\n",
    "gps_coordinates(queryset)\n",
    "\n",
    "print('\\nData source = 3, exlcuding source = instagram:')\n",
    "queryset = Tweet.objects.filter(data_source=3).exclude(source='Instagram')\n",
    "gps_coordinates(queryset)\n",
    "print('\\nData source = 1, exlcuding source = instagram:')\n",
    "queryset = Tweet.objects.filter(data_source=1).exclude(source='Instagram')\n",
    "gps_coordinates(queryset)\n",
    "print('\\nOverall, exlcuding source = instagram:')\n",
    "queryset = Tweet.objects.filter(data_source__gte=1).exclude(source='Instagram')\n",
    "gps_coordinates(queryset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which mentions, urls and hashtags are most common amongst the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from django.db.models import Case, IntegerField, Sum, When\n",
    "\n",
    "max_items = 20\n",
    "\n",
    "# Get keywords which were tracked to exlude them from count.\n",
    "keywords = Keyword.objects.all().values_list('keyword', flat=True)\n",
    "print(keywords)\n",
    "# Remove preceding hash, as hashes are excluded from hashtag objects.\n",
    "keywords = [k[1:] for k in keywords]\n",
    "\n",
    "# Count all Tweets:\n",
    "hashtags_with_counts = Hashtag.objects.exclude(hashtag__in=keywords)\\\n",
    "    .annotate(tweet_count=Count('tweets__id')).order_by('-tweet_count')[:max_items]\n",
    "# Count for filtered Tweets:\n",
    "hashtags_with_counts_kw = Hashtag.objects.exclude(hashtag__in=keywords)\\\n",
    "    .annotate(tweet_count=Sum(Case(When(\\\n",
    "        tweets__data_source=1, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items]\n",
    "# Count for filtered Tweets:\n",
    "hashtags_with_counts_geo = Hashtag.objects.exclude(hashtag__in=keywords)\\\n",
    "    .annotate(tweet_count=Sum(Case(When(\\\n",
    "        tweets__data_source=3, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items]\n",
    "\n",
    "print('\\nKeyword Hashtag counts:')\n",
    "for v in hashtags_with_counts_kw:\n",
    "    print('{}: {}'.format(v.tweet_count, v))\n",
    "print('\\nGeo Hashtag counts:')\n",
    "for v in hashtags_with_counts_geo:\n",
    "    print('{}: {}'.format(v.tweet_count, v))\n",
    "#print('\\nOverall Hashtag counts:')\n",
    "#for v in hashtags_with_counts:\n",
    "#    print('{}: {}'.format(v.tweet_count, v))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the hashtags above, most can be classified as 'regional / referring to the event'. There are a number of unrelated tags from the automated job adverts picked up by the geo stream. #trump and #climatechange come from discussion about the event.\n",
    "\n",
    "#veterans and #repost are not obvious in their meaning. We can check some of these Tweets to understand the discussion around these tags.\n",
    "\n",
    "The below output shows that #veterans is used by job advert platforms and #repost appears to be a convention on Instagram to represent what, on Twitter, would be called a Retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veteran_ts = Hashtag.objects.filter(hashtag='veterans')[0].tweets.all()\n",
    "repost_ts = Hashtag.objects.filter(hashtag='repost')[0].tweets.all()\n",
    "\n",
    "print('\\n===========\\n#Veterans:\\n===========')\n",
    "for t in veteran_ts[:5]:\n",
    "    print(t.text)\n",
    "    #print('https://twitter.com/{}/status/{}\\n'.format(t.author.screen_name, t.tweet_id))\n",
    "\n",
    "print('\\n===========\\n#Repost:\\n===========')\n",
    "for t in repost_ts[:5]:\n",
    "    print(t.text)\n",
    "    print('https://twitter.com/{}/status/{}'.format(t.author.screen_name, t.tweet_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregating URLs\n",
    "\n",
    "# Attempt to unwind URLs before printing.\n",
    "import requests\n",
    "\n",
    "# Remove arguments from URL (exluded as arguments contain identifying information)\n",
    "#import re\n",
    "#prog = re.compile('(.*)(?=\\?)')\n",
    "\n",
    "# Count all Tweets:\n",
    "#urls_with_counts = Url.objects.all()\\\n",
    "#    .annotate(tweet_count=Count('tweets__id')).order_by('-tweet_count')[:max_items]\n",
    "# Count for filtered Tweets:\n",
    "urls_with_counts_kw = Url.objects.all()\\\n",
    "    .annotate(tweet_count=Sum(Case(When(\\\n",
    "        tweets__data_source=1, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items+10]\n",
    "# Count for filtered Tweets:\n",
    "urls_with_counts_geo = Url.objects.all()\\\n",
    "    .annotate(tweet_count=Sum(Case(When(\\\n",
    "        tweets__data_source=3, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items]\n",
    "\n",
    "print('\\nKeyword URL counts:')\n",
    "for v in urls_with_counts_kw:\n",
    "    r = requests.get('http://' + str(v))\n",
    "    url = r.url\n",
    "    #result = prog.match(r.url)\n",
    "    if not url:\n",
    "        url = 'http://' + str(v)\n",
    "    #url = result.group(0)\n",
    "    print('{}: {} \\t{}'.format(v.tweet_count, url, v))\n",
    "    \n",
    "print('\\n Geo URL counts:')\n",
    "for v in urls_with_counts_geo:\n",
    "    r = requests.get('http://' + str(v))\n",
    "    url = r.url\n",
    "    if not url:\n",
    "        url = 'http://' + str(v)\n",
    "    print('{}: {} \\t{}'.format(v.tweet_count, url, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Tweets including a URL manually:\n",
    "url = 'redcross.org'\n",
    "ts = Url.objects.filter(url=url)[0].tweets.all()\n",
    "for t in ts:\n",
    "    print('\\nAuthor: {}'.format(t.author))\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregating Mentions\n",
    "\n",
    "# Count all Tweets:\n",
    "mentions_with_counts = Mention.objects.all() \\\n",
    "    .annotate(tweet_count=Count('tweets__id')).order_by('-tweet_count')[:max_items]\n",
    "# Count for filtered Tweets:\n",
    "mentions_with_counts_kw = Mention.objects.all().annotate(tweet_count=Sum(Case(When(\n",
    "        tweets__data_source=1, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items]\n",
    "# Count for filtered Tweets:\n",
    "mentions_with_counts_geo = Mention.objects.all().annotate(tweet_count=Sum(Case(When(\n",
    "        tweets__data_source=3, \n",
    "    then=1), default=0, output_field=IntegerField()))).order_by('-tweet_count')[:max_items]\n",
    "\n",
    "print('\\nKeyword URL counts:')\n",
    "for v in mentions_with_counts_kw:\n",
    "    print('{0}: {1:.0%} {2} \\t\\thttps://twitter.com/{3}'.format(v.tweet_count, v.tweets.filter(data_source=1).values(\"author\").distinct().count()/v.tweet_count, v, v))\n",
    "print('\\n Geo URL counts:')\n",
    "for v in mentions_with_counts_geo:\n",
    "    print('{0}: {1:.0%} {2} \\t\\thttps://twitter.com/{3}'.format(v.tweet_count, v.tweets.filter(data_source=3).values(\"author\").distinct().count()/v.tweet_count, v, v))\n",
    "#print('\\nOverall URL counts:')\n",
    "#for v in mentions_with_counts:\n",
    "#    print('{}: {} \\t\\thttps://twitter.com/{}'.format(v.tweet_count, v, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking Tweets including a Mention manually:\n",
    "mention = 'texastoyzz'\n",
    "ts = Mention.objects.filter(mention=mention)[0].tweets.all()\n",
    "for t in ts:\n",
    "    print('\\nAuthor: {}'.format(t.author))\n",
    "    print(t)\n",
    "\n",
    "#Mention.objects.filter(mention=mention)[0].tweets.all().values(\"author\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Source\n",
    "The source of a Tweet may be a suitable proxy for some level of user classification, as some businesses may use automated platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there are null values for source field, as these would have to be counted seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet.objects.filter(data_source__gt=0).filter(source__isnull=True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportions by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:07:26.023592Z",
     "start_time": "2019-07-10T18:07:25.726885Z"
    }
   },
   "outputs": [],
   "source": [
    "from django.db.models import Count\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:07:28.196137Z",
     "start_time": "2019-07-10T18:07:26.025828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all streamed Tweets\n",
    "tweets = Tweet.objects.filter(data_source__gt=0)\n",
    "\n",
    "# Group tweets by 'source' and count totals\n",
    "source_query = list(tweets.values('source').annotate(total_count=Count('source')).order_by('-total_count'))\n",
    "\n",
    "# Turn query into dictionary and create dataframe\n",
    "source_list = [x[\"source\"] for x in source_query]\n",
    "source_counts = [x[\"total_count\"] for x in source_query]\n",
    "source_dictionary = {\"source\" : source_list, \"count\" : source_counts}\n",
    "df = pd.DataFrame.from_dict(source_dictionary)\n",
    "df['proportion_of_total'] = df['count'] / tweets.count()\n",
    "\n",
    "# Split tweets into Keyword and Geo streams and add similar columns:\n",
    "geo_tweets = tweets.filter(data_source__gt=2)\n",
    "geo_source_query = list(geo_tweets.values('source').annotate(total_count=Count('source')).order_by('-total_count'))\n",
    "geo_dictionary = {k[\"source\"]:k[\"total_count\"] for k in geo_source_query}\n",
    "geo_count_col = pd.Series([geo_dictionary.get(df[\"source\"][i],0) for i in range(0, len(df))]) \n",
    "df.insert(2, \"geo_count\", geo_count_col) \n",
    "df['proportion_of_geo_stream'] = df['geo_count'] / geo_tweets.count()\n",
    "\n",
    "kw_tweets = tweets.filter(data_source__lte=2)\n",
    "kw_source_query = list(kw_tweets.values('source').annotate(total_count=Count('source')).order_by('-total_count'))\n",
    "kw_dictionary = {k[\"source\"]:k[\"total_count\"] for k in kw_source_query}\n",
    "kw_count_col = pd.Series([kw_dictionary.get(df[\"source\"][i],0) for i in range(0, len(df))]) \n",
    "df.insert(2, \"kw_count\", kw_count_col) \n",
    "df['proportion_of_kw_stream'] = df['kw_count'] / kw_tweets.count()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `proportion_of_geo_stream` represents the proportion of the geo streamed tweets that has come from the given source, not the proportion of the source that is from the geo stream (and similarly for `proportion_of_kw_stream`.\n",
    "\n",
    "For example, the above table clearly shows that the majority (76.6%) of Tweets returned by the geo stream came from Instagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:09:08.601056Z",
     "start_time": "2019-07-10T18:09:06.995379Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[:20].plot.bar(x='source', y='proportion_of_total', rot=85)\n",
    "df.sort_values(by=['proportion_of_kw_stream'], ascending=False)[:20].plot.bar(x='source', y='proportion_of_kw_stream', rot=85)\n",
    "df.sort_values(by=['proportion_of_geo_stream'], ascending=False)[:20].plot.bar(x='source', y='proportion_of_geo_stream', rot=85)\n",
    "\n",
    "df[:20].plot.bar(x='source', y=['proportion_of_total', 'proportion_of_kw_stream', 'proportion_of_geo_stream'], rot=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instagram makes up a large proportion of the geo-tagged Tweets. This appears to be a built-in function of the Instagram app when an Instagram user has auto-crossposting to Twitter enabled.\n",
    "\n",
    "We can replot the geo chart excluding the Instagram content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:09:35.844238Z",
     "start_time": "2019-07-10T18:09:35.491837Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['proportion_of_geo_stream'], ascending=False)[1:21].plot.bar(x='source', y='proportion_of_geo_stream', rot=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check what proportion of Instagram content is geo tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:09:39.949780Z",
     "start_time": "2019-07-10T18:09:39.217752Z"
    }
   },
   "outputs": [],
   "source": [
    "#query = Tweet.objects.filter(data_source=1)\n",
    "#query = Tweet.objects.filter(data_source=1).exclude(source='Instagram')\n",
    "query = Tweet.objects.filter(data_source=1).filter(source='Instagram')\n",
    "total = query.count()\n",
    "no_geo = query.filter(coordinates_type__isnull=True).count()\n",
    "print(\"{0} of {1} Instagram posts from kw_stream do not include coordinates ({2:.2f}%).\".format(no_geo, total, (no_geo / total * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of Content by Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually check the collection of each source value for common behaviours (for example, whether a source platforms is automatically generating marketing Tweets).\n",
    "\n",
    "First, we collect a list of sources which appear in the dataset over 100 times. \n",
    "\n",
    "We then take a look at some content from each source for a brief qualitative analysis. We also check whether a source is dominated by a small set of users by checking the proportional distribution by user. This will allow us to check whether certain sources are comprised of (e.g.) automated marketing or spam accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:09:43.393589Z",
     "start_time": "2019-07-10T18:09:43.387042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sources for qualitative analysis:\n",
    "sources_for_analysis = df['source'].loc[df['count'] > 100]\n",
    "sources_for_analysis\n",
    "#list(sources_for_analysis.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:10:06.168481Z",
     "start_time": "2019-07-10T18:09:44.444173Z"
    }
   },
   "outputs": [],
   "source": [
    "for source_name in list(df['source'].loc[df['count'] > 100].values):\n",
    "    print('\\n\\n--------------------------------------')\n",
    "    print(\"Tweets for source: {}\".format(source_name))\n",
    "    print('--------------------------------------\\n')\n",
    "    query = tweets.filter(source=source_name)\n",
    "    for t in query[:min(query.count(), 10)]:\n",
    "        print(t.text)\n",
    "    #text_df = pd.DataFrame(list(tweets.filter(source=source_name).values('text')))\n",
    "    #text_df[:10]\n",
    "\n",
    "    author_count = list(query.values('author__screen_name').annotate(total_count=Count('author__screen_name')).order_by('-total_count'))\n",
    "\n",
    "    # Turn query into dictionary and create dataframe\n",
    "    source_list = [x[\"author__screen_name\"] for x in author_count]\n",
    "    source_counts = [x[\"total_count\"] for x in author_count]\n",
    "    source_dictionary = {\"author__screen_name\" : source_list, \"count\" : source_counts}\n",
    "    df2 = pd.DataFrame.from_dict(source_dictionary)\n",
    "    df2['total_proportion'] = df2['count'] / query.count()\n",
    "\n",
    "    print('\\n')\n",
    "    print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we are primarily interested in categorising a source as automated or otherwise irrelevant to our application.\n",
    "\n",
    "* Typical Usage:\n",
    "  * Instagram (cross-posting), Twitter for iPhone, Twitter Web Client, Twitter for Android, TweetDeck, Twitter for iPad, Twitter Lite, Facebook (cross-posting)\n",
    "* Automated posting from other network (used in a different way to normal usage):\n",
    "  * Foursquare, Untappd, (Instagram)\n",
    "* Advertising/Spam:\n",
    "  * Paper.li\n",
    "* Automated Job Postings:\n",
    "  * TweetMyJOBS, SafeTweet by TweetMyJOBS\n",
    "* Media Outlets / content managers: \n",
    "  * SocialNewsDesk, Sprout Social, IFTTT, Hootsuite, Buffer\n",
    "* Private Network (News Outlet): \n",
    "  * BubbleLife\n",
    "* Private app: \n",
    "  * Error-log\n",
    "  \n",
    "Of the 'typical usage' sources, it may be worth considering the platform -- ie. 'Instagram', 'Twitter for iPhone' etc are likely to be from mobile devices, whereas 'Twitter Web Client' and 'TweetDeck' are more likely to be generated on computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:10:06.936725Z",
     "start_time": "2019-07-10T18:10:06.934660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting distribution of authors\n",
    "#df2.plot(y='total_proportion', x='author__screen_name', rot=85)\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:31:59.476664Z",
     "start_time": "2019-07-10T17:31:59.459692Z"
    }
   },
   "source": [
    "The proportion of Tweets per-source that contain URLS, media, mentions and hashtags are calculated and added to the dataframe.\n",
    "\n",
    "Note that the media column is excluded as this wasn't captured for the Harvey dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:10:40.627887Z",
     "start_time": "2019-07-10T18:10:07.356630Z"
    }
   },
   "outputs": [],
   "source": [
    "url_prop = []\n",
    "hashtag_prop = []\n",
    "mention_prop = []\n",
    "media_prop = []\n",
    "\n",
    "for source_value in sources_for_analysis:\n",
    "    source_tweets = tweets.filter(source=source_value)\n",
    "    total = source_tweets.count()\n",
    "    \n",
    "    url_count = source_tweets.filter(urls__isnull=False).distinct().count()\n",
    "    url_prop.append(url_count / total)\n",
    "    hashtag_count = source_tweets.filter(hashtags__isnull=False).distinct().count()\n",
    "    hashtag_prop.append(hashtag_count / total)\n",
    "    mention_count = source_tweets.filter(mentions__isnull=False).distinct().count()\n",
    "    mention_prop.append(mention_count / total)\n",
    "    #media_count = source_tweets.filter(media_files__isnull=False).count()\n",
    "    #media_prop.append(media_count / total)\n",
    "\n",
    "col_index = len(df.columns)\n",
    "df.insert(col_index, 'url_prop', pd.Series(url_prop))\n",
    "df.insert(col_index, 'hashtag_prop', pd.Series(hashtag_prop))\n",
    "df.insert(col_index, 'mention_prop', pd.Series(mention_prop))\n",
    "#df.insert(col_index, 'media_prop', pd.Series(media_prop))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:10:42.211119Z",
     "start_time": "2019-07-10T18:10:41.130166Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['url_prop'], ascending=False)[0:21].plot.bar(x='source', y='url_prop', rot=85)\n",
    "df.sort_values(by=['hashtag_prop'], ascending=False)[0:21].plot.bar(x='source', y='hashtag_prop', rot=85)\n",
    "df.sort_values(by=['mention_prop'], ascending=False)[0:21].plot.bar(x='source', y='mention_prop', rot=85)\n",
    "#df.sort_values(by=['media_prop'], ascending=False)[0:21].plot.bar(x='source', y='media_prop', rot=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unsurprising to see that the sources identified as spam/automated are more likely to include URLs and Mentions. \n",
    "\n",
    "It is interesting to note that there is a significant difference in URL rates between the sources expected to be computer-based vs mobile, which supports the idea that the former are computer users and therefore more likely to be sharing content they are reading, rather than posting live thoughts while 'on-the-go'.\n",
    "\n",
    "Of the above categories, we can safely disregard all but those classified as 'Typical Usage'. The content generated by users of the Foursquare and Untappd apps does contain some interesting content, but as most interactions are based on the use of the apps, it's generally unrelated to the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['url_prop'], ascending=False)[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T18:10:42.771997Z",
     "start_time": "2019-07-10T18:10:42.769723Z"
    }
   },
   "outputs": [],
   "source": [
    "#eliminated_sources = ['Foursquare', 'Untappd', 'Paper.li', 'TweetMyJOBS', 'SafeTweet by TweetMyJOBS', 'SocialNewsDesk', 'Sprout Social', 'IFTTT', 'BubbleLife', 'Error-log']\n",
    "#original_tweet_count = tweets.count()\n",
    "\n",
    "#tweets = Tweet.objects.filter(data_source__gt=0).exclude(source__in=eliminated_sources)\n",
    "#print('{0} Tweets eliminated from dataset of {1} ({2:.2f}%).'.format(original_tweet_count - tweets.count(), original_tweet_count, ((original_tweet_count - tweets.count()) / original_tweet_count * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Exploration of data which has been hand-coded.\n",
    "\n",
    "Tweets and accounts have been coded, with a secondary coder doing a proportion of objects which were coded by the main coder.\n",
    "\n",
    "Dimensions refer to different coding schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2180 Tweets coded by primary coder, 225 by secondary coder.\n",
      "931 Accounts coded by primary coder, 151 by secondary coder.\n",
      "\n",
      "\n",
      "Classes by Dimension:\n",
      "\n",
      "'Information Type', type: tweet\n",
      "['Aid Request', 'Ground Truth', 'Info for Affected', 'Info for Non-Affected', 'Emotion - Affected', 'Emotion - Unaffected', 'Unrelated', 'Unclassified']\n",
      "\n",
      "'Local', type: user\n",
      "['Unsure', 'Non-Witness', 'Witness']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Codings for Tweets:\n",
    "# Note: we exclude data_code__data_code_id=0 as this is a 'to be coded' class, which is an artefact of the software.\n",
    "tweet_codings = Coding.objects.filter(coding_id=1).filter(tweet__isnull=False).filter(data_code__data_code_id__gt=0)\n",
    "tweet_codings_secondary = Coding.objects.filter(coding_id=2).filter(tweet__isnull=False)\n",
    "# Codings for accounts:\n",
    "account_codings = Coding.objects.filter(coding_id=1).filter(user__isnull=False).filter(data_code__data_code_id__gt=0)\n",
    "account_codings_secondary = Coding.objects.filter(coding_id=2).filter(user__isnull=False)\n",
    "\n",
    "# Note these totals combine all user or Tweet codes, so can be misleading if more than one dimension is used.\n",
    "print(\"{} Tweets coded by primary coder, {} by secondary coder.\".format(tweet_codings.count(), tweet_codings_secondary.count()))\n",
    "print(\"{} Accounts coded by primary coder, {} by secondary coder.\".format(account_codings.count(), account_codings_secondary.count()))\n",
    "print('\\n')\n",
    "\n",
    "# Check available coding schema:\n",
    "dimensions = DataCodeDimension.objects.all()\n",
    "print('Classes by Dimension:\\n')\n",
    "for d in dimensions:\n",
    "    print('\\'{}\\', type: {}\\n{}\\n'.format(d.name, d.coding_subject, list(d.datacode.values_list('name', flat=True))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of coder agreement\n",
    "First, we check the agreement between the two coders to assess the choice and definition of codes.\n",
    "\n",
    "The primary coder's choices are displayed as rows, the secondary coder's choices are columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aid Request</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Info for Affected</th>\n",
       "      <th>Info for Non-Affected</th>\n",
       "      <th>Emotion - Affected</th>\n",
       "      <th>Emotion - Unaffected</th>\n",
       "      <th>Unrelated</th>\n",
       "      <th>Unclassified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aid Request</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ground Truth</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Info for Affected</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Info for Non-Affected</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion - Affected</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion - Unaffected</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unrelated</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Aid Request  Ground Truth  Info for Affected  \\\n",
       "Aid Request                      5             0                  1   \n",
       "Ground Truth                     0            33                  0   \n",
       "Info for Affected                0             0                 14   \n",
       "Info for Non-Affected            0             7                  7   \n",
       "Emotion - Affected               0             3                  0   \n",
       "Emotion - Unaffected             1             6                  2   \n",
       "Unrelated                        0             2                  2   \n",
       "Unclassified                     0             0                  0   \n",
       "\n",
       "                       Info for Non-Affected  Emotion - Affected  \\\n",
       "Aid Request                                0                   0   \n",
       "Ground Truth                               0                   0   \n",
       "Info for Affected                          0                   0   \n",
       "Info for Non-Affected                     33                   1   \n",
       "Emotion - Affected                         1                   6   \n",
       "Emotion - Unaffected                       7                   8   \n",
       "Unrelated                                  2                   3   \n",
       "Unclassified                               0                   0   \n",
       "\n",
       "                       Emotion - Unaffected  Unrelated  Unclassified  \n",
       "Aid Request                               0          0             0  \n",
       "Ground Truth                              2          1             0  \n",
       "Info for Affected                         0          0             0  \n",
       "Info for Non-Affected                     1          1             0  \n",
       "Emotion - Affected                        2          1             0  \n",
       "Emotion - Unaffected                     40          0             0  \n",
       "Unrelated                                 2         31             0  \n",
       "Unclassified                              0          0             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the following code will need to be updated if there is more than one dimension for \n",
    "# Tweet or User coding.\n",
    "\n",
    "# Create Matrix as dataframe\n",
    "classes = DataCodeDimension.objects.get(coding_subject='tweet').datacode.values_list('name', flat=True)\n",
    "class_df = pd.DataFrame(index=classes, columns=classes)\n",
    "class_df = class_df.fillna(0)\n",
    "\n",
    "# Get all Tweets that have been coded by both users:\n",
    "double_coded_tweets = Tweet.objects.filter(coding_for_tweet__coding_id=1).filter(coding_for_tweet__coding_id=2)\n",
    "\n",
    "for t in double_coded_tweets:\n",
    "    coding1 = t.coding_for_tweet.filter(coding_id=1)[0]\n",
    "    coding2 = t.coding_for_tweet.filter(coding_id=2)[0]\n",
    "    class_df.loc[coding1.data_code.name, coding2.data_code.name] += 1\n",
    "\n",
    "class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate disagreement percentages for rows and columns.\n",
    "\n",
    "The 'error' represents the error of the subject coder if the alternate coder represents the correct class. It is better interpreted as a count of disagreements, per class chosen by the subject coder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Primary's Errors     %  Second's Errors     %\n",
      "Aid Request                           1  16.7                1  16.7\n",
      "Ground Truth                          3   8.3               18  35.3\n",
      "Info for Affected                     0   0.0               12  46.2\n",
      "Info for Non-Affected                17  34.0               10  23.3\n",
      "Emotion - Affected                    7  53.8               12  66.7\n",
      "Emotion - Unaffected                 24  37.5                7  14.9\n",
      "Unrelated                            11  26.2                3   8.8\n",
      "Unclassified                          0   NaN                0   NaN\n"
     ]
    }
   ],
   "source": [
    "agreement_counts = [row[index] for index, row in class_df.iterrows()]\n",
    "\n",
    "col_count = class_df.sum(axis=0).subtract(agreement_counts)\n",
    "col_proportion = col_count.divide(class_df.sum(axis=0)/100).round(1)\n",
    "\n",
    "row_count = class_df.sum(axis=1).subtract(agreement_counts)\n",
    "row_proportion = row_count.divide(class_df.sum(axis=1)/100).round(1)\n",
    "\n",
    "prop_df = pd.concat([row_count, row_proportion, col_count, col_proportion], axis=1)\n",
    "prop_df.columns = ['Primary\\'s Errors', '%', 'Second\\'s Errors', '%']\n",
    "print(prop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories for which there is a large imbalance in the error rate are interesting as they may represent categories for which each coder has a fundamentally different interpretation. For example, the 'Info for Affected' class: every Tweet coded in this class by the primary coder was also coded as such by the secondary, however the secondary also categorised a further 12 Tweets in this class that the primary did not. This suggests the primary coder had a much more narrow definition of this class.\n",
    "\n",
    "These values must be considered with respect to their proportion of the total set. For example, there may be a high error rate in a code that appears rarely in the dataset while the majority of the data falls in one class with a high agreement rating. \n",
    "\n",
    "We may also be interested in weighting particular classes when calculating accuracy, if they are more important than others to correctly classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cohen's Kappa\n",
    "Instead of using a basic accuracy measurement, we calculate the Cohen's Kappa, which accounts for the probability of the users agreeing on a code by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.659000697635257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cohen's Kappa\n",
    "def calc_cohen(df):\n",
    "    total = class_df.sum().sum()\n",
    "\n",
    "    pr_list = class_df.sum(axis=1).divide(total)\n",
    "    sec_list = class_df.sum(axis=0).divide(total)\n",
    "    pe = sum(pr_list*sec_list)\n",
    "    po = sum(agreement_counts)/total\n",
    "\n",
    "    kappa = (po - pe) / (1 - pe)\n",
    "\n",
    "    return kappa\n",
    "\n",
    "calc_cohen(class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kappa is .66\n",
    "\n",
    "A score between .61-80 suggests 'substantial agreement' (Landis, J.R.; Koch, G.G. (1977). \"The measurement of observer agreement for categorical data\").\n",
    "\n",
    "However, as number of classes increases, the Kappa should also naturally increase (as pe will naturally become smaller)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unsure</th>\n",
       "      <th>Non-Witness</th>\n",
       "      <th>Witness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unsure</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Witness</th>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Witness</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unsure  Non-Witness  Witness\n",
       "Unsure            0            0        3\n",
       "Non-Witness       5           89        8\n",
       "Witness           0            5       41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the following code will need to be updated if there is more than one dimension for \n",
    "# Tweet or User coding.\n",
    "\n",
    "# Create Matrix as dataframe\n",
    "classes = DataCodeDimension.objects.get(coding_subject='user') \\\n",
    "            .datacode.values_list('name', flat=True)\n",
    "class_df = pd.DataFrame(index=classes, columns=classes)\n",
    "class_df = class_df.fillna(0)\n",
    "\n",
    "# Get all Users that have been coded by both users:\n",
    "double_coded_users = User.objects.filter(coding_for_user__coding_id=1).filter(coding_for_user__coding_id=2)\n",
    "\n",
    "for u in double_coded_users:\n",
    "    coding1 = u.coding_for_user.filter(coding_id=1)[0]\n",
    "    coding2 = u.coding_for_user.filter(coding_id=2)[0]\n",
    "    class_df.loc[coding1.data_code.name, coding2.data_code.name] += 1\n",
    "\n",
    "class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Primary's Errors      %  Second's Errors      %\n",
      "Unsure                      3  100.0                5  100.0\n",
      "Non-Witness                13   12.7                5    5.3\n",
      "Witness                     5   10.9               11   21.2\n"
     ]
    }
   ],
   "source": [
    "agreement_counts = [row[index] for index, row in class_df.iterrows()]\n",
    "\n",
    "col_count = class_df.sum(axis=0).subtract(agreement_counts)\n",
    "col_proportion = col_count.divide(class_df.sum(axis=0)/100).round(1)\n",
    "\n",
    "row_count = class_df.sum(axis=1).subtract(agreement_counts)\n",
    "row_proportion = row_count.divide(class_df.sum(axis=1)/100).round(1)\n",
    "\n",
    "prop_df = pd.concat([row_count, row_proportion, col_count, col_proportion], axis=1)\n",
    "prop_df.columns = ['Primary\\'s Errors', '%', 'Second\\'s Errors', '%']\n",
    "print(prop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706551915602443"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cohen(class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both Kappas indicate 'substantial agreement', we may proceed with using them in supervised learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data Set\n",
    "\n",
    "We will create a dataset each for the coded Tweets and user accounts.\n",
    "\n",
    "### Tweet Dataframe\n",
    "\n",
    "We start by adding all coded Tweets to a dataframe, then checking column names for those which should be encoded or dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>coordinates_lat</th>\n",
       "      <th>coordinates_lon</th>\n",
       "      <th>coordinates_type</th>\n",
       "      <th>created_at</th>\n",
       "      <th>data_source</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>media_files</th>\n",
       "      <th>media_files_type</th>\n",
       "      <th>place_id</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_int</th>\n",
       "      <th>replied_to_status_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1338848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-08-26 12:57:50+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Great reporting on #Harvey @FOXNews @SHarrigan...</td>\n",
       "      <td>901443573475946496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5827896</td>\n",
       "      <td>29.7629</td>\n",
       "      <td>-95.3832</td>\n",
       "      <td>Point</td>\n",
       "      <td>2017-08-31 16:49:46+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>I distinctly remember packing for Houston in a...</td>\n",
       "      <td>903313880188948481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8965229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-08-29 23:55:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Lite</td>\n",
       "      <td>starting the school year off with a flooded ho...</td>\n",
       "      <td>902696231780196352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7507026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-08-29 14:02:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Pardot</td>\n",
       "      <td>As #HurricaneHarvey continues, we hope these t...</td>\n",
       "      <td>902546996799520768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3432737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-08-31 01:05:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Please help my Aunt Grace get back on her feet...</td>\n",
       "      <td>903076349149106176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id  coordinates_lat  coordinates_lon coordinates_type  \\\n",
       "0    1338848              NaN              NaN             None   \n",
       "1    5827896          29.7629         -95.3832            Point   \n",
       "2    8965229              NaN              NaN             None   \n",
       "3    7507026              NaN              NaN             None   \n",
       "4    3432737              NaN              NaN             None   \n",
       "\n",
       "                 created_at  data_source  favorite_count     id  \\\n",
       "0 2017-08-26 12:57:50+00:00            1               0   4852   \n",
       "1 2017-08-31 16:49:46+00:00            3               0  45580   \n",
       "2 2017-08-29 23:55:27+00:00            1               0  32393   \n",
       "3 2017-08-29 14:02:27+00:00            1               0  26193   \n",
       "4 2017-08-31 01:05:54+00:00            1               0  41262   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_user_id  ... media_files  \\\n",
       "0                    NaN                  NaN  ...        None   \n",
       "1                    NaN                  NaN  ...        None   \n",
       "2                    NaN                  NaN  ...        None   \n",
       "3                    NaN                  NaN  ...        None   \n",
       "4                    NaN                  NaN  ...        None   \n",
       "\n",
       "  media_files_type place_id quoted_status_id quoted_status_id_int  \\\n",
       "0             None     None             None                  NaN   \n",
       "1             None     None             None                  NaN   \n",
       "2             None     None             None                  NaN   \n",
       "3             None     None             None                  NaN   \n",
       "4             None     None             None                  NaN   \n",
       "\n",
       "  replied_to_status_id retweet_count              source  \\\n",
       "0                 None             0  Twitter for iPhone   \n",
       "1                 None             0           Instagram   \n",
       "2                 None             0        Twitter Lite   \n",
       "3                 None             0              Pardot   \n",
       "4                 None             0  Twitter for iPhone   \n",
       "\n",
       "                                                text            tweet_id  \n",
       "0  Great reporting on #Harvey @FOXNews @SHarrigan...  901443573475946496  \n",
       "1  I distinctly remember packing for Houston in a...  903313880188948481  \n",
       "2  starting the school year off with a flooded ho...  902696231780196352  \n",
       "3  As #HurricaneHarvey continues, we hope these t...  902546996799520768  \n",
       "4  Please help my Aunt Grace get back on her feet...  903076349149106176  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all Tweets coded by primary coder:\n",
    "tweets = Tweet.objects.filter(coding_for_tweet__coding_id=1, coding_for_tweet__data_code__data_code_id__gt=0)\n",
    "tweet_df = pd.DataFrame(list(tweets.values()))\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author_id',\n",
       " 'coordinates_lat',\n",
       " 'coordinates_lon',\n",
       " 'coordinates_type',\n",
       " 'created_at',\n",
       " 'data_source',\n",
       " 'favorite_count',\n",
       " 'id',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_user_id',\n",
       " 'is_deleted',\n",
       " 'is_deleted_observed',\n",
       " 'lang',\n",
       " 'media_files',\n",
       " 'media_files_type',\n",
       " 'place_id',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_int',\n",
       " 'replied_to_status_id',\n",
       " 'retweet_count',\n",
       " 'source',\n",
       " 'text',\n",
       " 'tweet_id']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweet_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain columns are converted into a numeric or binary representation and others are removed as irrelevant to classification.\n",
    "\n",
    "Language is encoded as binary column for english / non-english. English represents over 90% of the data.\n",
    "\n",
    "One-hot encoding uses values 0 and 1 instead of True and False to enable ML integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change author_id, which is the SQL user id, to the Twitter user id of the author:\n",
    "tweet_df['author_id'] = tweet_df['author_id'].apply(lambda x: User.objects.get(id=x).user_id)\n",
    "\n",
    "\n",
    "# 'Has Coordinates' column:\n",
    "d = {'Point': 1, None: 0}\n",
    "tweet_df['has_coords'] = tweet_df['coordinates_type'].replace(d)\n",
    "# Alternate methods:\n",
    "#x = tweet_df['coordinates_type'].map(d)\n",
    "#tweet_df['coordinates_type'].replace(d, inplace=True)\n",
    "\n",
    "# 'Is a Reply' column:\n",
    "tweet_df['is_reply'] = 0\n",
    "tweet_df.loc[tweet_df['in_reply_to_user_id'].isnull() == False, 'is_reply'] = 1\n",
    "\n",
    "# 'Is Quoting' column:\n",
    "tweet_df['is_quoting'] = 0\n",
    "tweet_df.loc[tweet_df['quoted_status_id_int'].isnull() == False, 'is_quoting'] = 1\n",
    "\n",
    "#tweet_df[tweet_df.quoted_status_id_int.notnull()].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     0.906422\n",
       "und    0.047706\n",
       "es     0.025229\n",
       "fr     0.005505\n",
       "pt     0.002752\n",
       "ar     0.001835\n",
       "de     0.001835\n",
       "da     0.001376\n",
       "nl     0.001376\n",
       "ja     0.000917\n",
       "it     0.000917\n",
       "ht     0.000459\n",
       "et     0.000459\n",
       "pl     0.000459\n",
       "cy     0.000459\n",
       "fi     0.000459\n",
       "tl     0.000459\n",
       "ru     0.000459\n",
       "vi     0.000459\n",
       "hi     0.000459\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode languages as integers\n",
    "\n",
    "# Encode each language as a seperate integer (not appropriate for ML):\n",
    "#langs = tweet_df.lang.unique()\n",
    "#lang_encoding = list(range(len(langs)))\n",
    "#lang_encoding_dict = dict(zip(langs, lang_encoding))\n",
    "#tweet_df['lang'].replace(lang_encoding_dict, inplace=True)\n",
    "#lang_encoding_dict\n",
    "\n",
    "# Single language encoding:\n",
    "tweet_df['lang_en'] = 0\n",
    "tweet_df.loc[tweet_df['lang'] == 'en', 'lang_en'] = 1\n",
    "\n",
    "# Check proportion of each language in dataset to justify choice of only english:\n",
    "tweet_df['lang'].value_counts()/tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode sources as one-hot columns for those that made up at least 1% of the overall dataset identified in the exploratory analysis:\n",
    "\n",
    "Note: TweetMyJOBS and SafeTweet by TweetMyJOBS are combined into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = ['Instagram', 'Twitter for iPhone', 'Twitter Web Client', 'Twitter for Android', 'Paper.li', \n",
    " 'Hootsuite', 'TweetMyJOBS', 'SafeTweet by TweetMyJOBS', 'IFTTT', 'Facebook', 'TweetDeck', \n",
    " 'Twitter for iPad', 'BubbleLife', 'Twitter Lite']\n",
    "\n",
    "tweet_df['source_other'] = 1\n",
    "for s in source_list:\n",
    "    col_name = 'source_' + s.replace(\" \", \"\")\n",
    "    tweet_df[col_name] = 0\n",
    "    tweet_df.loc[tweet_df['source'] == s, col_name] = 1\n",
    "    tweet_df.loc[tweet_df['source'] == s, 'source_other'] = 0\n",
    "    \n",
    "# Merge columns:\n",
    "tweet_df.loc[tweet_df['source_SafeTweetbyTweetMyJOBS'] == 1, 'source_TweetMyJOBS'] = 1\n",
    "tweet_df.drop(columns=['source_SafeTweetbyTweetMyJOBS'], inplace=True)\n",
    "    \n",
    "#tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>data_source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>has_coords</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_quoting</th>\n",
       "      <th>lang_en</th>\n",
       "      <th>source_other</th>\n",
       "      <th>...</th>\n",
       "      <th>source_TwitterforAndroid</th>\n",
       "      <th>source_Paper.li</th>\n",
       "      <th>source_Hootsuite</th>\n",
       "      <th>source_TweetMyJOBS</th>\n",
       "      <th>source_IFTTT</th>\n",
       "      <th>source_Facebook</th>\n",
       "      <th>source_TweetDeck</th>\n",
       "      <th>source_TwitterforiPad</th>\n",
       "      <th>source_BubbleLife</th>\n",
       "      <th>source_TwitterLite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>877507343826604034</td>\n",
       "      <td>2017-08-26 12:57:50+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Great reporting on #Harvey @FOXNews @SHarrigan...</td>\n",
       "      <td>901443573475946496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126695871</td>\n",
       "      <td>2017-08-31 16:49:46+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>I distinctly remember packing for Houston in a...</td>\n",
       "      <td>903313880188948481</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3872916612</td>\n",
       "      <td>2017-08-29 23:55:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>starting the school year off with a flooded ho...</td>\n",
       "      <td>902696231780196352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289365657</td>\n",
       "      <td>2017-08-29 14:02:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>As #HurricaneHarvey continues, we hope these t...</td>\n",
       "      <td>902546996799520768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1228146372</td>\n",
       "      <td>2017-08-31 01:05:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Please help my Aunt Grace get back on her feet...</td>\n",
       "      <td>903076349149106176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id                created_at  data_source  \\\n",
       "0  877507343826604034 2017-08-26 12:57:50+00:00            1   \n",
       "1           126695871 2017-08-31 16:49:46+00:00            3   \n",
       "2          3872916612 2017-08-29 23:55:27+00:00            1   \n",
       "3           289365657 2017-08-29 14:02:27+00:00            1   \n",
       "4          1228146372 2017-08-31 01:05:54+00:00            1   \n",
       "\n",
       "                                                text            tweet_id  \\\n",
       "0  Great reporting on #Harvey @FOXNews @SHarrigan...  901443573475946496   \n",
       "1  I distinctly remember packing for Houston in a...  903313880188948481   \n",
       "2  starting the school year off with a flooded ho...  902696231780196352   \n",
       "3  As #HurricaneHarvey continues, we hope these t...  902546996799520768   \n",
       "4  Please help my Aunt Grace get back on her feet...  903076349149106176   \n",
       "\n",
       "   has_coords  is_reply  is_quoting  lang_en  source_other  ...  \\\n",
       "0           0         0           0        1             0  ...   \n",
       "1           1         0           0        1             0  ...   \n",
       "2           0         0           0        1             0  ...   \n",
       "3           0         0           0        1             1  ...   \n",
       "4           0         0           0        1             0  ...   \n",
       "\n",
       "   source_TwitterforAndroid  source_Paper.li  source_Hootsuite  \\\n",
       "0                         0                0                 0   \n",
       "1                         0                0                 0   \n",
       "2                         0                0                 0   \n",
       "3                         0                0                 0   \n",
       "4                         0                0                 0   \n",
       "\n",
       "   source_TweetMyJOBS  source_IFTTT  source_Facebook  source_TweetDeck  \\\n",
       "0                   0             0                0                 0   \n",
       "1                   0             0                0                 0   \n",
       "2                   0             0                0                 0   \n",
       "3                   0             0                0                 0   \n",
       "4                   0             0                0                 0   \n",
       "\n",
       "   source_TwitterforiPad  source_BubbleLife  source_TwitterLite  \n",
       "0                      0                  0                   0  \n",
       "1                      0                  0                   0  \n",
       "2                      0                  0                   1  \n",
       "3                      0                  0                   0  \n",
       "4                      0                  0                   0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unecessary columns:\n",
    "drop_columns = ['coordinates_lat', 'coordinates_lon', 'coordinates_type', \n",
    "                'favorite_count', 'id', 'in_reply_to_status_id', 'in_reply_to_user_id', \n",
    "                'is_deleted_observed', 'media_files', 'media_files_type', 'place_id', \n",
    "                'quoted_status_id', 'quoted_status_id_int', 'replied_to_status_id', \n",
    "                'retweet_count', 'is_deleted', 'source', 'lang']\n",
    "\n",
    "tweet_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich the data by adding columns from linked data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_count(tweet_id):\n",
    "    return tweets.get(tweet_id=tweet_id).hashtags.count()\n",
    "def get_url_count(tweet_id):\n",
    "    return tweets.get(tweet_id=tweet_id).urls.count()\n",
    "def get_mention_count(tweet_id):\n",
    "    return tweets.get(tweet_id=tweet_id).mentions.count()\n",
    "\n",
    "tweet_df['hashtag_count'] = tweet_df['tweet_id'].map(get_hashtag_count)\n",
    "tweet_df['url_count'] = tweet_df['tweet_id'].map(get_url_count)\n",
    "tweet_df['mention_count'] = tweet_df['tweet_id'].map(get_mention_count)\n",
    "\n",
    "#list(tweet_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the code, which is the target outcome, as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>data_source</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>has_coords</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>is_quoting</th>\n",
       "      <th>lang_en</th>\n",
       "      <th>source_other</th>\n",
       "      <th>...</th>\n",
       "      <th>source_IFTTT</th>\n",
       "      <th>source_Facebook</th>\n",
       "      <th>source_TweetDeck</th>\n",
       "      <th>source_TwitterforiPad</th>\n",
       "      <th>source_BubbleLife</th>\n",
       "      <th>source_TwitterLite</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>data_code_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>877507343826604034</td>\n",
       "      <td>2017-08-26 12:57:50+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Great reporting on #Harvey @FOXNews @SHarrigan...</td>\n",
       "      <td>901443573475946496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126695871</td>\n",
       "      <td>2017-08-31 16:49:46+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>I distinctly remember packing for Houston in a...</td>\n",
       "      <td>903313880188948481</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3872916612</td>\n",
       "      <td>2017-08-29 23:55:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>starting the school year off with a flooded ho...</td>\n",
       "      <td>902696231780196352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289365657</td>\n",
       "      <td>2017-08-29 14:02:27+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>As #HurricaneHarvey continues, we hope these t...</td>\n",
       "      <td>902546996799520768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1228146372</td>\n",
       "      <td>2017-08-31 01:05:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Please help my Aunt Grace get back on her feet...</td>\n",
       "      <td>903076349149106176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            author_id                created_at  data_source  \\\n",
       "0  877507343826604034 2017-08-26 12:57:50+00:00            1   \n",
       "1           126695871 2017-08-31 16:49:46+00:00            3   \n",
       "2          3872916612 2017-08-29 23:55:27+00:00            1   \n",
       "3           289365657 2017-08-29 14:02:27+00:00            1   \n",
       "4          1228146372 2017-08-31 01:05:54+00:00            1   \n",
       "\n",
       "                                                text            tweet_id  \\\n",
       "0  Great reporting on #Harvey @FOXNews @SHarrigan...  901443573475946496   \n",
       "1  I distinctly remember packing for Houston in a...  903313880188948481   \n",
       "2  starting the school year off with a flooded ho...  902696231780196352   \n",
       "3  As #HurricaneHarvey continues, we hope these t...  902546996799520768   \n",
       "4  Please help my Aunt Grace get back on her feet...  903076349149106176   \n",
       "\n",
       "   has_coords  is_reply  is_quoting  lang_en  source_other  ...  source_IFTTT  \\\n",
       "0           0         0           0        1             0  ...             0   \n",
       "1           1         0           0        1             0  ...             0   \n",
       "2           0         0           0        1             0  ...             0   \n",
       "3           0         0           0        1             1  ...             0   \n",
       "4           0         0           0        1             0  ...             0   \n",
       "\n",
       "   source_Facebook  source_TweetDeck  source_TwitterforiPad  \\\n",
       "0                0                 0                      0   \n",
       "1                0                 0                      0   \n",
       "2                0                 0                      0   \n",
       "3                0                 0                      0   \n",
       "4                0                 0                      0   \n",
       "\n",
       "   source_BubbleLife  source_TwitterLite  hashtag_count  url_count  \\\n",
       "0                  0                   0              2          0   \n",
       "1                  0                   0              0          1   \n",
       "2                  0                   1              1          0   \n",
       "3                  0                   0              1          1   \n",
       "4                  0                   0              1          1   \n",
       "\n",
       "   mention_count  data_code_id  \n",
       "0              4             6  \n",
       "1              0             5  \n",
       "2              0             5  \n",
       "3              0             3  \n",
       "4              1             4  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tweet_code(tweet_id):\n",
    "    return Coding.objects.filter(coding_id=1).get(tweet__tweet_id=tweet_id) \\\n",
    "        .data_code.data_code_id\n",
    "\n",
    "tweet_df['data_code_id'] = tweet_df['tweet_id'].map(get_tweet_code)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_df.to_csv(r'tweet_df.csv', index = None, header=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet_df is now ready to be used in classification algorithms, however we can also add data from the Tweets' authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['added_at', 'betweenness_centrality', 'closeness_centrality', 'created_at', 'data_source', 'default_profile', 'default_profile_image', 'degree_centrality', 'description', 'eigenvector_centrality', 'favourites_count', 'followers_count', 'friends_count', 'geo_enabled', 'has_extended_profile', 'id', 'in_degree', 'is_deleted', 'is_deleted_observed', 'is_translation_enabled', 'katz_centrality', 'lang', 'listed_count', 'load_centrality', 'location', 'name', 'needs_phone_verification', 'old_screen_name', 'out_degree', 'protected', 'ratio_detected', 'ratio_media', 'ratio_original', 'screen_name', 'statuses_count', 'suspended', 'time_zone', 'translator_type', 'tweets_per_hour', 'undirected_eigenvector_centrality', 'url', 'user_class', 'user_followers', 'user_followers_update', 'user_following', 'user_following_update', 'user_id', 'user_network_update_observed_at', 'utc_offset', 'verified']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>added_at</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>created_at</th>\n",
       "      <th>data_source</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>description</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>user_class</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_followers_update</th>\n",
       "      <th>user_following</th>\n",
       "      <th>user_following_update</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_network_update_observed_at</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-26 00:37:31.831480+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-02-18 00:37:53+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>495469195</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-27 20:58:57.505146+00:00</td>\n",
       "      <td>4.006556e-08</td>\n",
       "      <td>0.131911</td>\n",
       "      <td>2011-03-03 07:24:09+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>Ginger guy in Houston, Texas. I might talk pol...</td>\n",
       "      <td>3.298253e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>https://t.co/Uzn7bzMxoE</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>260126165</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-31 15:53:00.939210+00:00</td>\n",
       "      <td>4.457059e-04</td>\n",
       "      <td>0.185294</td>\n",
       "      <td>2010-06-16 15:40:52+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>WELCOME to Student Housing and Residential Lif...</td>\n",
       "      <td>2.660015e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>http://t.co/Pg6DkCwrZA</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>156321797</td>\n",
       "      <td>None</td>\n",
       "      <td>-18000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-26 16:56:17.089665+00:00</td>\n",
       "      <td>2.123291e-04</td>\n",
       "      <td>0.185398</td>\n",
       "      <td>2009-04-08 22:15:18+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>-- Extreme Storm Chaser \"Michael Koch\" -- @sky...</td>\n",
       "      <td>3.014993e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>https://t.co/tJTP1PVqi6</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29851333</td>\n",
       "      <td>None</td>\n",
       "      <td>-14400</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-28 23:17:23.168775+00:00</td>\n",
       "      <td>1.840374e-04</td>\n",
       "      <td>0.117098</td>\n",
       "      <td>2012-02-16 17:57:20+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>Toystore with a lot more!  Pop Culture fanatic...</td>\n",
       "      <td>7.147106e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>http://t.co/kw5DR7prpB</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>494251206</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          added_at  betweenness_centrality  \\\n",
       "0 2017-08-26 00:37:31.831480+00:00                     NaN   \n",
       "1 2017-08-27 20:58:57.505146+00:00            4.006556e-08   \n",
       "2 2017-08-31 15:53:00.939210+00:00            4.457059e-04   \n",
       "3 2017-08-26 16:56:17.089665+00:00            2.123291e-04   \n",
       "4 2017-08-28 23:17:23.168775+00:00            1.840374e-04   \n",
       "\n",
       "   closeness_centrality                created_at  data_source  \\\n",
       "0                   NaN 2012-02-18 00:37:53+00:00            3   \n",
       "1              0.131911 2011-03-03 07:24:09+00:00            3   \n",
       "2              0.185294 2010-06-16 15:40:52+00:00            1   \n",
       "3              0.185398 2009-04-08 22:15:18+00:00            1   \n",
       "4              0.117098 2012-02-16 17:57:20+00:00            3   \n",
       "\n",
       "   default_profile  default_profile_image  degree_centrality  \\\n",
       "0             True                  False                NaN   \n",
       "1            False                  False           0.000182   \n",
       "2            False                  False           0.001762   \n",
       "3            False                  False           0.003037   \n",
       "4             True                  False           0.000607   \n",
       "\n",
       "                                         description  eigenvector_centrality  \\\n",
       "0                                                                        NaN   \n",
       "1  Ginger guy in Houston, Texas. I might talk pol...            3.298253e-07   \n",
       "2  WELCOME to Student Housing and Residential Lif...            2.660015e-04   \n",
       "3  -- Extreme Storm Chaser \"Michael Koch\" -- @sky...            3.014993e-02   \n",
       "4  Toystore with a lot more!  Pop Culture fanatic...            7.147106e-08   \n",
       "\n",
       "   ...                      url  user_class  user_followers  \\\n",
       "0  ...                     None           2            None   \n",
       "1  ...  https://t.co/Uzn7bzMxoE           2            None   \n",
       "2  ...   http://t.co/Pg6DkCwrZA           2            None   \n",
       "3  ...  https://t.co/tJTP1PVqi6           2            None   \n",
       "4  ...   http://t.co/kw5DR7prpB           2            None   \n",
       "\n",
       "   user_followers_update  user_following  user_following_update    user_id  \\\n",
       "0                   None            None                   None  495469195   \n",
       "1                   None            None                   None  260126165   \n",
       "2                   None            None                   None  156321797   \n",
       "3                   None            None                   None   29851333   \n",
       "4                   None            None                   None  494251206   \n",
       "\n",
       "  user_network_update_observed_at utc_offset  verified  \n",
       "0                            None       None     False  \n",
       "1                            None       None     False  \n",
       "2                            None     -18000     False  \n",
       "3                            None     -14400     False  \n",
       "4                            None       None     False  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all authors from tweet_df:\n",
    "authors = User.objects.filter(tweet__in=tweets).distinct()\n",
    "author_df = pd.DataFrame(list(authors.values()))\n",
    "\n",
    "# Check available fields:\n",
    "print(list(author_df.columns))\n",
    "\n",
    "author_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fields from User object:\n",
    "# 'user_id',\n",
    "# 'favourites_count', 'followers_count', 'friends_count', 'statuses_count',\n",
    "# 'listed_count',\n",
    "# 'default_profile',\n",
    "# 'default_profile_image',\n",
    "# 'has_extended_profile',\n",
    "# 'geo_enabled',\n",
    "# 'verified',\n",
    "#### Fields added/calculated during data collection:\n",
    "# 'user_class',\n",
    "# 'data_source',\n",
    "# 'in_degree','out_degree',\n",
    "# 'betweenness_centrality', 'closeness_centrality','degree_centrality', \n",
    "# 'eigenvector_centrality', 'katz_centrality', 'load_centrality'\n",
    "# 'undirected_eigenvector_centrality'\n",
    "# 'ratio_detected','ratio_media','ratio_original' 'tweets_per_hour', \n",
    "\n",
    "#### Columns to encode:\n",
    "# 'lang',\n",
    "# 'time_zone',    # TODO\n",
    "# 'utc_offset',   # TODO\n",
    "# 'url', \n",
    "# 'created_at',   # TODO\n",
    "# 'location',\n",
    "\n",
    "# Columns not relevant to classification:\n",
    "drop_columns =  ['added_at', 'id', 'old_screen_name', 'is_deleted_observed', \n",
    "                 'is_deleted', 'user_followers', 'user_followers_update', \n",
    "                 'user_following', 'user_following_update', \n",
    "                 'user_network_update_observed_at', 'needs_phone_verification',\n",
    "                 'suspended', 'protected', 'translator_type', 'is_translation_enabled',\n",
    "                 'description', 'name', 'screen_name']\n",
    "author_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# Rename conflict with Tweet dataframe column name:\n",
    "author_df.rename(columns={'data_source':'user_data_source'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en       0.9460\n",
       "es       0.0215\n",
       "fr       0.0065\n",
       "de       0.0045\n",
       "pt       0.0035\n",
       "it       0.0035\n",
       "ar       0.0030\n",
       "en-gb    0.0030\n",
       "nl       0.0015\n",
       "fi       0.0015\n",
       "tr       0.0010\n",
       "sv       0.0010\n",
       "xx-lc    0.0005\n",
       "el       0.0005\n",
       "en-GB    0.0005\n",
       "ca       0.0005\n",
       "sk       0.0005\n",
       "ru       0.0005\n",
       "ja       0.0005\n",
       "Name: lang, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'has_url' column:\n",
    "author_df['profile_has_url'] = 0\n",
    "author_df.loc[author_df['url'].isnull() == False, 'profile_has_url'] = 1\n",
    "\n",
    "# Profile has a local location listed, as matched with the list:\n",
    "local_locations = ['tx', 'houston', 'texas']\n",
    "author_df['profile_has_local_location'] = 0\n",
    "author_df.loc[author_df['location'].str.contains(\n",
    "    '|'.join(local_locations), case=False), 'profile_has_local_location'] = 1\n",
    "\n",
    "# Convert True/False to 1/0\n",
    "d = {True: 1, False: 0}\n",
    "author_df['default_profile'] = author_df['default_profile'].replace(d)\n",
    "author_df['default_profile_image'] = author_df['default_profile_image'].replace(d)\n",
    "author_df['geo_enabled'] = author_df['geo_enabled'].replace(d)\n",
    "author_df['has_extended_profile'] = author_df['has_extended_profile'].replace(d)\n",
    "author_df['verified'] = author_df['verified'].replace(d)\n",
    "\n",
    "\n",
    "# Encode languages as boolean: is 'en':\n",
    "author_df['user_lang_en'] = 0\n",
    "author_df.loc[author_df['lang'] == 'en', 'user_lang_en'] = 1\n",
    "# Check proportion of each language in dataset (to justify choice of only encoding 'en'):\n",
    "author_df['lang'].value_counts()/authors.count()\n",
    "\n",
    "# TODO: categorise and encode the following three\n",
    "# 'utc_offset', \n",
    "# 'time_zone',\n",
    "# 'created_at',\n",
    "##author_df['time_zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['lang', 'url', 'location', 'utc_offset', 'time_zone', 'created_at']\n",
    "author_df.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with authors at correct locations (and duplicated where necessary) \n",
    "## Replace with pd.dataframe.merge\n",
    "row_list = []\n",
    "for x in tweet_df['author_id']:\n",
    "    row_list.append(author_df.loc[author_df['user_id'] == x].iloc[0])\n",
    "author_arranged_df = pd.DataFrame(row_list)\n",
    "author_arranged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Combine dataframes\n",
    "if author_arranged_df.shape[0] == tweet_df.shape[0]:\n",
    "    tweet_df = pd.concat([tweet_df, author_arranged_df], axis=1)\n",
    "else:\n",
    "    print('Error joining frames.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.drop(columns=['text'], inplace=True) # Dropping to avoid unsanitised data breaking csv.\n",
    "\n",
    "tweet_df.to_csv(r'data/harvey_tweet_df.csv', index = None, header = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all users coded by primary coder:\n",
    "#users = User.objects.filter(coding_for_user__coding_id=1, coding_for_user__data_code__data_code_id__gt=0)\n",
    "#user_df = pd.DataFrame(list(users.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 668,
   "position": {
    "height": "690px",
    "left": "1135px",
    "right": "20px",
    "top": "123px",
    "width": "685px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
